{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Kartoza Handbook","text":"<p>This site comprises the organisational and technical documentation for Kartoza.</p> <p>This is where we highlight the procedures, principles, and processes related to Development, DevOps, and GIS, in line with the organisations best practices.</p> <p>This is open content, available on GitHub and freely licensed as public domain content under the terms of CC0 1.0 Universal.</p> <p>This content is delivered without any warranty, express or implied. Use at own risk.</p>"},{"location":"#purpose","title":"Purpose","text":"<p>The purpose and function of this collection of documents is to perform the following:</p> <ul> <li>Improve consistency in processes and products</li> <li>Improve efficiency and innovation</li> <li>Increase transparency and accountability</li> <li>Improve value for clients</li> <li>Provide a space for the dissemination and proliferation of ideas</li> <li>Promote a culture of openness and collaboration</li> <li>Provide a single source of truth for resources</li> <li>Promote personal growth and development</li> <li>Add value to the community</li> </ul>"},{"location":"#scope","title":"Scope","text":"<p>Kartoza is a company that specializes in Open Source Geospatial solutions. As a result, topics covered by this documentation will be limited to categories relevant to the operations of Kartoza. These categories are outlined as follows:</p> <ul> <li>Company: General practices and procedures for Kartoza staff</li> <li>GIS: Resources for Geographic Information Systems and data</li> <li>Development: Software development processes, tools, and conventions</li> <li>DevOps: Developer operations and system administration</li> <li>Resources: Cheatsheets, links, media, and other resources</li> </ul> <p>This repository is limited to these categories, with some slight overlap in domain verticals.</p> <p>In the majority of instances, where overlaps with adjacent fields of interest, such as Data Science, \"Big Data\", or to some extent even earth observation and remote sensing, these elements should be primarily remanded to external references in the resources section.</p> <p>Whilst some resources (such as tutorials on Semi-Automated Classification with QGIS) may be considered valuable additions, the priority of this content is to remain a concise collection of resources directly related to the operations and key competencies of Kartoza staff.</p>"},{"location":"#limitations","title":"Limitations","text":"<p>This collection of documents does not constitute a replacement for Standard Operating Procedures (SOPs) and company policy. In some cases, our SOPs may point to sections of this handbook, but the SOP itself is canonical as to where the procude content lies. Due to the rate at which modern technology develops, opinions change, project needs or priorities are adjusted, and the operational requirements of the organisation evolve, this collection is somewhat ephemeral and should be considered a dynamic \"living document\" which is subject to constant change and iteration.</p> <p>Processes and documentation from this collection are developed in conjunction with the broader community, independant contractors, temporary staff, juniors, and interns. As such they are not guaranteed to reflect the views of Kartoza, and are not intended to be a substitute for official policy.</p> <p>As resources and processes mature, they may be incorporated into official SOPs as required.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Although Kartoza is a privately held Open Source development and consulting company, the organisation deeply values transparency, delivery of value to the broader community, and continuous engagement with all stakeholders.</p> <p>Community contributions to this documentation site and associated resources are welcome. Contributions are expected to adhere to the QGIS.org Diversity Statement and Code of Conduct.</p> <p>If you have any queries or feedback, please contact us at info@kartoza.com</p>"},{"location":"contributing/#conventions","title":"Conventions","text":"<p>The following conventions outline expectations for contributions to this documentation project:</p> <ul> <li>Use grammar checking tools where available, such as Grammarly or spell checking extensions for your IDE</li> <li>Request a review for internal changes before they are merged into the main repository</li> <li>Default to British English spellings rather than American English</li> <li>Do not commit sensitive information or links to non-public resources. This includes internal unlisted youtube channels, cloud storage repositories such as nextcloud, or personal details</li> <li>Due to the nature of the contents in this repository, when making large edits that do not create new content, communicate with team members to prevent collisions</li> <li>When producing assets such as images, ensure they are the minimum viable size and do not commit large resources to git</li> <li>Assets and media elements such as images are best left out of the source control where possible. Use an external storage system (e.g. minio/ s3), or [paste an image into an image]??? to get a GitHub reference to the media item rather than committing to git. This includes screenshots etc. that are likely to change or be updated over time. Use the assets directory to store assets that are not likely to change such as logos</li> <li>When using assets, upload them to a suitable file path according to their primary usage location, e.g. <code>assets/images/resources/cheatsheets/postgresql/joins.png</code></li> <li>Due to the depth and breadth of these resources, it is necessary to manually index new pages in various subcategories to ensure access and discoverability</li> <li>It makes sense to use a consistent legend of emoji for tagging project and documentation items. Although it may have a steep initial learning curve or implementation strategy, using emoji and unicode symbols to tag elements is a fun and intuitive way to attach metadata to elements which makes visually scanning over documents and commit histories much more effective in multiple languages. Please see the polyglot document for more information.</li> <li>Build and check your changes locally to catch any errors before committing them to the main repository</li> <li>TODO: come up with some formatting guideline (e.g. max line length etc)</li> </ul>"},{"location":"contributing/#tags-badges-and-shields","title":"Tags, Badges, and Shields","text":"<p>Tagging elements with emoji is useful for visual identification and search of various elements in broader categories, but sometimes more explicit metadata is required to be attached to something to indicate whether it constitutes a general resource, opinion, community standard, or whether something is a known reference item connected to an official SOP. One method of identifying such features may be using shields.io, for example:</p> <ul> <li> <code>![Best Practice](https://img.shields.io/badge/kartoza-best--practice-blue)</code></li> <li> <code>![Community](https://img.shields.io/badge/community-standard-brightgreen)</code></li> <li> <code>![Industry](https://img.shields.io/badge/industry-standard-yellowgreen)</code></li> </ul>"},{"location":"contributing/#translations","title":"Translations","text":"<p>Due to the scope and intention for frequent updates to this documentation, additional languages will not be supported at this time.</p> <p>Translations and i18n are handled by the documentation framework, as outlined in the mkdocs and mkdocs-material documentation.</p>"},{"location":"contributing/#framework","title":"Framework","text":"<p>This documentation uses the mkdocs-material framework, and site configuration is specified in the mkdocs.yaml file. Various extensions are supported to improve usability, such as pymdown, which may be enabled via pull requests. Please note that only extensions which provide relevant value will be considered for integration, and extensions with significant learning curves or duplication should be avoided. Extensions which provide accessibility improvements are welcome.</p>"},{"location":"contributing/#building","title":"Building","text":"<p>The online documentation is built using github actions and published to the gh-pages branch.</p> <p>To build the documentation locally, use the docker command <code>docker run --rm -it -v ${PWD}:/docs squidfunk/mkdocs-material build</code> to populate the site directory with the static content. To serve the data for testing, a simple solution is to use a python webserver to serve the data at <code>127.0.0.1:9101</code> using the command <code>cd site &amp;&amp; python -m http.server --bind 127.0.0.1 9101</code>.</p> <p>Note that the generated site data and assets are explicitly excluded from git.</p>"},{"location":"polyglot/","title":"Polyglot: The Emoji Map","text":"<p>Although it may have a steep initial learning curve or implementation strategy, using emoji and unicode symbols to tag elements is a fun and intuitive way to attach metadata to elements which makes visually scanning over documents and commit histories much more effective in multiple languages.</p> <p>In order for this to be effective, a consistent method of referencing the emoji meanings is required. This is challenging because the utilisation of emoji are typically context specific, which requires a mapping of emoji meanings for various contexts.</p> <p>Kartoza values inclusion and diversity. Please contribute to ensure that the items represented here remain inclusive and fair wherever possible.</p>"},{"location":"polyglot/#gitmoji","title":"gitmoji","text":"<p>Tagging commit messages in git is a useful tool for visually assessing the issues addressed by a particular commit.</p> <p>A community standard has already been developed, available at https://gitmoji.dev/</p>"},{"location":"polyglot/#kitmoji-gitmoji-","title":"kitmoji (gitmoji-)","text":"<p>gitmoji is fairly comprehensive collection, and used quite widely in the tech community (relative to similar projects). The downside to this is that it is rather verbose and becomes cumbersome to learn and use.</p> <p>The simple solution is to select a subset (kit) of the gitmoji icons and use them as broader higher level categories. This keeps things a bit more consistent and allows gradual adoption of the wider collection.</p> id Icon Reference Function 1 \u2728 <code>:sparkles:</code> New features 2 \ud83d\udc1b <code>:bug:</code> Bugfix 3 \u267b\ufe0f <code>:recycle:</code> Refactoring/ Comments 4 \ud83d\udcdd <code>:memo:</code> Documentation 5 \ud83d\udc84 <code>:lipstick:</code> UI 6 \u26a1\ufe0f <code>:zap:</code> Performance 7 \ud83d\ude80 <code>:rocket:</code> CI/ CD/ Deployment 8 \u2705 <code>:white_check_mark:</code> Testing 9 \ud83d\udd12\ufe0f <code>:lock:</code> Security 10 \ud83d\udd25 <code>:fire:</code> Remove data 11 \u23ea\ufe0f <code>:rewind:</code> Revert changes 12 \u2697\ufe0f / \ud83d\udca9 <code>:alembic:</code> / <code>:poop:</code> Experiments/ PoC/ bad code <p>Not every commit has to have a gitmoji, but it's useful for common cases.</p>"},{"location":"polyglot/#geomoji","title":"GeoMoji","text":"<p>As with gitmoji, we need a consistent way to reference Geographic Information Elements. This could be related to data sources, licenses, tools, or standard metadata categories. Some may confuse the concept of geomoji with generic symbology and signage, but in this instance the reference is to a series of common Emoji characters which can be used as concise, visually effective hashtags that can be used across across applications, search tools, git messages, documentation, or social media. It would be great to extend this concept to a collection of map symbols (e.g. using emoji instead of a font library).</p> <p>A alpha-state concept project is in development at https://github.com/zacharlie/geomoji.</p>"},{"location":"company/kartoza/","title":"About Kartoza","text":"<p>In this section we describe the company, our ethos and general expectations of our team members and their use of technology and systems within the company.</p> <p>All staff members are expected to read and comply with (where applicable) the content laid out in this handbook.</p> <p>Kartoza is a South Africa-based Free and Open Source GIS (FOSSGIS) service provider. We use GIS software to solve complex location-related problems for individuals, businesses and governments around the world.</p> <p>Kartoza was formed as a merger between Linfiniti and Afrispatial.</p> <p></p> <p>Learn more at our company website https://kartoza.com</p> <ul> <li>Strategic Objective</li> </ul> <p>Working at Kartoza:</p> <ul> <li>Your Kartoza Computer</li> <li>Essential Software</li> <li>Scrum at Kartoza</li> <li>Daily Workflow</li> <li>Communication</li> </ul>"},{"location":"company/kartoza/communication/","title":"Communication","text":"<p>In Kartoza, communcation is probably the most important skill that we want to engender in our staff. Whether it is communicating your work in standups and issues, talking with clients, checking in with project leads, writing emails and reports, communication is probably the most important skill needed for your work.</p> <p>Good communication keeps the company running smoothly, keeps happiness levels high, instills a sense of professionalism in customers and the outside world that interact with us, and prevents wasting time and effort.</p> <p>As a purely virtual company, communication skills are particularly important because we do not work together in a physical space and we need to make extra effort to ensure that we communicate well.</p>"},{"location":"company/kartoza/communication/#communication-systems","title":"Communication Systems","text":""},{"location":"company/kartoza/communication/#slack","title":"Slack","text":"<p>Slack is our primary communication mechanism. You should have slack running and available at all times during your work day. If you run mission critical systems (e.g. maintaining client web sites, server infrastructure), you should have slack on your phone so that we can contact you after hours in emergencies.</p> <p>In some cases our clients have slack channels, which you should participate and monitor as needed to carry out your client related tasks.</p> <p>All staff members are at minumum members of the '1-kartoza'  and '4-random' channels. You should use 1-kartoza for all your general communications.</p> <p>You might be tempted to use DMs (Direct Messages) to ask team mates for technical information, plan work etc. Please don't do this! Use the 1-kartoza channel for all communications unless it is something of a personal nature that you need to discuss with management.</p> <p>Why do we ask you not to use DMs? Because when you communicate in the 1-kartoza channel that is public to the company, we realise a number of benefits:</p> <ol> <li>We can see what things you are working on.</li> <li>We can see you are actively working (which is often difficult in a virtual company).</li> <li>If you are experience a technical issue, the whole company can be aware of it and potentially help you, rather than you being stuck waiting for a single person to help you.</li> <li>If you are stuck on something, project leads and management can be aware of which projects are having issues and plan accordingly.</li> <li>It creates a sense of 'togetherness' that is otherwise lost in a virtual company.</li> </ol> <p>For internal company communications, we have our lowest bar in Slack in terms of how much attention you need to pay to grammar and crafting beautiful prose.</p>"},{"location":"company/kartoza/communication/#email","title":"Email","text":"<p>All Kartoza employees are issued with an email address ending in @kartoza.com. When you set up your email account, make sure to include our corporate branding in your signature as per the image below.</p> <p></p> <p>Here is the signature in text form:</p> <p>\u200b </p> <p>Tim Sutton Kartoza Co-Founder</p> <p>Visit http://kartoza.com to find out about open source:</p> <ul> <li>Desktop GIS programming services</li> <li>Geospatial web development</li> <li>GIS Training</li> <li>Consulting Services</li> </ul> <p>Tim is a member of the QGIS Project Steering Committee</p> <p>Replace the line under your name with your job description e.g. Senior Developer.</p> <p>When writing emails there are a few things we want you to ensure which we list below:</p> <ul> <li>Correctness: Check the details in your email are factually correct.</li> <li>Completeness: Make sure you address all the questions raised if it is a reply to a colleague or client.</li> <li>Professionalism: Make sure that you have spell-checked your email (use grammarly or similar if needed and especially if you are not a native english speaker).</li> <li>Responsiveness: Reply to emails within a working day, even if the reply is just to confirm receipt and inform the other person when they will get a complete reply.</li> <li>Politeness: Emails should be written in a polite way - never be rude to the person you are communicating with even if they are being rude to you. Politeness includes taking the trouble to spell names correctly, thank the person for their email and greet them at the start and end of your mail.</li> </ul>"},{"location":"company/kartoza/communication/#meet--video-call","title":"Meet / Video Call","text":"<p>Often you will need to meet with clients on Google Meet or similar video conferencing call.</p> <p>When attending a video / voice call, follow these guidelines:</p> <ul> <li>Be on time: Join the call a few minutes before it starts so that you are 'ready to go' when the call kicks off.</li> <li>Check your environment: Most of us work from home and that means sometimes we might have pets or family or other things going on. Try to sit in a place where you can work without distracting the call participants of these thngs. Make sure the room (or at least the part of the room visible in the camera frame) is tidy and neat.</li> <li>Sound: It is critical you can be heard during the call. Ensure you have a good microphone and camera and that your environment is noise free - it will be a distraction for the other call participants.</li> <li>Bandwidth: As a virtual / remote worker, you are responsible for ensuring that you have a reliable internet connection that is fast enough to handle voice calls with video / screenshares. This is a condition of your employment and if you do not have an adequate connection, we may not be able to keep you under our employment.</li> <li>Power: If you live in a power challenged society (e.g. South Africa), you are expected to ensure you have alternatives in place to ensure you have power during meetings - charged laptop batteries, battery packs for your internet connection etc.</li> <li>Avoid being overly familiar: Be friendly and open but keep in mind we are typically billing clients for our time during a call, or they are assessing the possibility of contracting us or collaborating with us. For this reason keep the focus of the call on work.</li> </ul>"},{"location":"company/kartoza/communication/#escalation","title":"Escalation","text":"<p>When you are communicating with colleagues, especially if it is something urgent you should follow this escalation procedure if you do not get the response you need:</p> <ul> <li>Ask them in the original medium (email / slack / video call)</li> <li>Follow up with a second medium (e.g. a nudge in slack to check email) if you do not get a timely response</li> <li>Send a calendar invitation for a short meeting if you get no response.</li> <li>Send the person a whatsapp / telegram etc.</li> <li>Phone the person (mobile/whatsapp/telegram etc.)</li> <li>Ask their line manager or a colleague to reach out to them</li> </ul> <p>Continue escalating the frequency and form of communication until you get a response.</p>"},{"location":"company/kartoza/communication/#responding-to-client-emails","title":"Responding to Client Emails","text":"<p>If a client or potential client writes to you, our expectation is that you should respond to them by next business day. If you reply, please CC in the original recipients  so they know the email has been dealt with. We ask you all to focus on your communication skills as it is a recurring issue that we have in the company and really creates a bad experience for our clients. </p> <p>It is quite ok to give just a short reply giving a timeline of when a detailed reply will be forthcoming. If answering an email enquiry requires a substantial amount of time, you can also chat with the client (or ask your project or general manager to do so) and raise a quotation for the work involved in the client request.</p>"},{"location":"company/kartoza/daily_workflow/","title":"Your daily workflow at Kartoza","text":""},{"location":"company/kartoza/daily_workflow/#starting-your-day","title":"Starting your day","text":"<p>The activities for your day are determined by:</p> <ol> <li>Which project you are working on, as listed in the company-wide project Gantt chart / sheet. Normally we try to have you work on a single project per day to reduce cognitive friction as you context switch from one project to another.</li> <li>The scrum board / project board for that day's project. The list of sized tickets should be implemented sequentially, from the top of the 'This sprint' (or Ready or 'To-do') column, down to the bottom.</li> </ol>"},{"location":"company/kartoza/daily_workflow/#story-points","title":"Story points","text":"<p>Your issues closed in GitHub (GH) in a day should equate to a day's worth or more (assuming some tasks will be quicker than estimated) of GH issue sizes. Before working on any issue it should be sized. Any issue deemed to be larger than one day of work (size 8) should be split into smaller issues.</p>"},{"location":"company/kartoza/daily_workflow/#standups","title":"Standups","text":"<p>Your daily stand ups should be written like this:</p>"},{"location":"company/kartoza/daily_workflow/#yesterday","title":"Yesterday","text":"<p>Project: Flux Capacitor Enlargement</p> <ul> <li>[Size 4] Closed https://github.com/fcap/foobar/issues/123 - Add transparency to map</li> <li>[Size 4] Closed https://github.com/fcap/foobar/issues/124 - Add transparency to legend</li> <li>[Size 2] WIP https://github.com/fcap/foobar/issues/125 - Add transparency to widgets</li> </ul> <p>Project: Kartoza</p> <ul> <li>[Size 1] Water cooler - outed @Rudolf as a serial imposter</li> </ul>"},{"location":"company/kartoza/daily_workflow/#today","title":"Today","text":"<p>Project: Flux Capacitor Enlargement</p> <ul> <li>[Size 8] https://github.com/fcap/foobar/issues/128 - Render flux capacitor on map</li> </ul>"},{"location":"company/kartoza/daily_workflow/#blockers","title":"Blockers","text":"<ul> <li>Waiting for client to respond with clarification request on https://github.com/fcap/foobar/issues/130</li> </ul> <p>Here is an example from our Slack of a beautifully presented standup - yours should look like this too!</p> <p></p>"},{"location":"company/kartoza/daily_workflow/#running-your-timesheets","title":"Running your timesheets","text":"<p>We run our own timesheet application (thanks Dimas!) at timesheets.kartoza.com - and in our ERP Next platform. The timesheets.kartoza.com is preferred since it has many niceties to make your time tracking more efficient. We bill our customers in 15-minute increments, rounded down or up as appropriate. This is aggregated across the day. That means if you e.g. log 2h12 minutes on one cost center for a client and 1h00 minutes on another cost center for the same client, the client will be billed 3h15min for the day's work.</p> <p>The descriptions in your timesheets should be defensible, terse descriptions of work with links to GitHub issues. Defensible means that if a client were ever to ask you to justify how you spent your time in an invoiced line item, we can refer back to the description and see references to tickets worked on and the story points associated with the work.</p> <p>Your timesheets should match the projects planned on the planner sheet.</p>"},{"location":"company/kartoza/daily_workflow/#setting-up-the-timesheet-account","title":"Setting up the timesheet account","text":"<p>Step 1: Go to https://timesheets.kartoza.com</p> <p>Step 2: Choose register to create your account:</p> <p></p> <p>Step 3: Complete the form to create your account:</p> <p></p> <p>Step 4: Use the cog menu to get to the settings panel</p> <p></p> <p>Go to ERP Next to get your API keys by looking in your user profile e.g.</p> <p>https://kartoza.erpnext.com/app/user/YOURUSERNAME%40kartoza.com</p> <p></p> <p>Back on the timesheets app, in the form that appeared from the cog menu, enter your API key and secret:</p> <p></p> <p>Now use button (1) to pull data from erpnext then button (2) to return to your timesheet.</p> <p></p>"},{"location":"company/kartoza/daily_workflow/#completing-your-timesheet","title":"Completing your timesheet","text":"<p>Once you are logged in, you will see this interface: </p> <p>The Activity dropdown has a list of activities, such as meetings, applied GIS work, coding, etc. This is a mandatory field and an option needs to be selected there to describe the type of activity being recorded. </p> <p>The Project dropdown has a list of ongoing projects at Kartoza. If the activity being recorded is related to Kartoza, such as Admin tasks, or internal meetings such as All Hands meetings then this can be left blank and the system will automatically record it as Kartoza. However, if the task is related to a project, select a project from the dropdown.</p> <p>The Task dropdown has a list of tasks pertaining to specific projects, this differs from project to project since the tasks will be broken down by high-level requirements so if the project requires development there would be a dropdown value 'Development/Implementation' but other projects might not need implementation. If you can't find the specific task you are doing, either look for the list and see what fits it (for example if the task you are recording is to discuss the data model with the client then you can select 'Modelling' from the list instead). However, if a task is crucial and missing from the list, contact Marina.</p>"},{"location":"company/kartoza/daily_workflow/#view-the-planning-sheet","title":"View the Planning sheet","text":"<p>The planning sheet shows the schedule for the list of Kartoza members and the projects they are assigned to. This can  be accessed by clicking on the cog menu and selecting 'Planning': </p> <p>Using this, you can see what projects are assigned to you for a specific period. Since projects will include various staff members, you can also find out who else is simultaneously assigned to the project in case you have queries or need to work on a task with someone and need their help.</p>"},{"location":"company/kartoza/daily_workflow/#view-the-burndown-chart","title":"View the Burndown chart","text":"<p>The burndown chart provides information on how much time we have left to complete the project, how much time was allocated to a specific project and how much we have exceeded the allocated time if the project is not concluded on the specified deadline. This can  be accessed by clicking on the cog menu and selecting 'Burndown Chart'. </p>"},{"location":"company/kartoza/daily_workflow/#additional-guidelines","title":"Additional Guidelines","text":"<p>Developing a project can be a lot of fun and enjoyable to work together to make something awesome, but we need to do it within the guide rails which we have laid down:</p> <ol> <li>Work only on the project allocated to you in your planning sheet. If you    need to work on something else, you need to communicate with your project manager and ask    her to change your planning allocation. Do this ahead of time, not on the day    of working.</li> <li>You need to follow the scrum board - if the ticket you have chosen in your    standup is not on the scrum board, that is a complete No-No.</li> <li>You need to work only on tickets you have been assigned - if someone asked    you to work on #46, you need to ask them to allocate you as the    developer.</li> <li>Before working on any ticket, it needs to be sized. You listed [5] as the    effort for your ticket in the standup, but your ticket lists [1] as its    size. If you disagree on the sizing you cannot start to work on the ticket    before reaching consensus with the technical lead.</li> <li>You need to work only on tickets that are in the 'this sprint' (or Ready or 'To-do') column - you    currently have only one ticket #57  which is eligible to be worked on. If    you feel another ticket needs to happen first (e.g. it is a logical dependency)    then again, you need to have a conversation with the technical lead and ask    them to update the board to accommodate your requested changes to the plan.</li> <li>When you write that you will work on a ticket of 8 points or less, it needs    to be done at the end of your workday with a neat PR referencing the    ticket, providing testing (both backend and UI), providing a screen grab or    better, an animated GIF illustrating the work done, passing all coding    standard checks (pylint / black /pep8 etc.). If at the end of the day you have    not finished what you committed to then you have taken a wrong turn in your    workflow - you probably would have realised that you would not finish the task    at 50% in and you should have raised the issue with your team along with an    estimate of how much time extra will be needed to resolve. Then you stop    working on that ticket and work down the sprint board onto the next issue.</li> <li>Basically when you go over your estimate, you are spending either the    client's money or Kartoza's profits for the job, neither of which should    be done without conscious agreement of your team mates.</li> </ol> <p>We really need you to step up and introduce this rigour into your work.  As you can see in the list above, in no case do we ask you to do anything unreasonable - we don't ask that you 'grind out' 18 hour days coding and going quietly insane with stress and pressure - we just want to see work delivered in a steady, orderly flow so we can predict timelines, make profit, please our clients and be awesome! So please keep it tight and communicate with your project manager and technical team lead using the framework above, let us see a PR at the end of the day (or more if your queue has multiple issues), and take on the tasks in a respectful, communicative, methodical and conscious manner.  We promise if you do that, your enjoyment of your work is going to sky rocket and your rapport and respect of your colleagues will too!</p>"},{"location":"company/kartoza/daily_workflow/#creating-issues-and-linking-them-to-projects","title":"Creating issues and linking them to projects","text":"<ol> <li>Create an issue on the Issues page and populate it properly (referenced elsewhere)</li> <li>Link the Issue to the project. This puts it in the funnel for triage</li> <li>Link it to a milestone. </li> <li>Tag it if you like.</li> <li>Do NOT assign it to anyone</li> </ol>"},{"location":"company/kartoza/daily_workflow/#team-lead--senior-dev-sprint-planning-roles","title":"Team lead / Senior dev sprint planning roles","text":"<ol> <li>Review and curate the kanban board well before a sprint planning meeting</li> <li>Check that issues are moving along the board. A cluttered board is a sign of poor progress or poor management or both!</li> <li>Sprint planning meeting should be short and focus on allocating work for the next sprint</li> <li>Set up and manage milestones in GitHub. These should match ERPNext tasks.</li> <li>Based on the project plan and other inputs decide wich issues should join the backlog. Add them from the Triage list to the backlog and order them, then direct them to be sized.</li> <li>In the sprint planning meeting, based on discussion and sizes, move issues into the Ready column and assign them. </li> <li>The Ready column must have only as many issues as the team can do in one sprint, no more!</li> </ol>"},{"location":"company/kartoza/daily_workflow/#the-scrum-board","title":"The scrum board","text":"<p>Our scrum boards are organised like this project below:  </p> <p>The columns of the scrum board have the following purposes:</p> <p>\u26d4\ufe0fNo Status (aka Backlog): There are all issues that have been raised by anyone. There is currently no plan to work on these, but they are topically connected to the project board.</p> <ul> <li>\ud83d\udd75\ufe0fWho can place tickets in this column?:</li> <li>\ud83d\udc68Any stakeholder!</li> </ul> <p>\u23f3\ufe0f To be estimated: Tickets that have to be estimated for an upcoming sprint. The size label has to be assigned based on the amount of hours it will take to complete the ticket.</p> <p>The following sizing scheme should be used:</p> <ul> <li>1\ufe0f\u20e3  It's quick I will have this done in 1 hour</li> <li>2\ufe0f\u20e3 Give me 2 hours and I will have it for you</li> <li>3\ufe0f\u20e3 It will take me between 2 hours to half a day</li> <li>5\ufe0f\u20e3 It will take between half a day and a full day</li> <li>8\ufe0f\u20e3 This is a full day job</li> <li>1\ufe0f\u20e33\ufe0f\u20e3 I'm going to need about a day and a half to do the job</li> <li>2\ufe0f\u20e30\ufe0f\u20e3 It will take at least two to three days</li> <li>4\ufe0f\u20e30\ufe0f\u20e3 This will take a full week</li> </ul> <p>For any ticket that has been assigned a value larger than 8, the ticket must be broken out into smaller issues so that the units of work are 1 day or less.</p> <ul> <li>\ud83d\udd75\ufe0fWho can place tickets in this column?:</li> <li>\ud83c\udfc9 Only the project planner ('scrum master') in collaboration with the client where appropriate.</li> </ul> <p>\ud83d\uded2TODO: These are sized tickets, that are ready to be worked on and go into the sprint for the week. The should be listed from top to bottom in priority order. During the sprint planning call, items for the upcoming sprint are identified from this column (see below).</p> <ul> <li>\ud83d\udd75\ufe0fWho can place tickets in this column?:</li> <li>\ud83c\udfc9 Only the project planner ('scrum master') in collaboration with the client where appropriate.</li> </ul> <p>\ud83c\udfc3Next Sprint: These are tickets that will be worked on in the next sprint. They are placed here during the sprint planning call (which happens the week before a sprint starts) so that they do not cause confusion with the items in the current sprint plan. Only sized tickets should be in this column. The cumulative total of the sizes of issues in the next sprint column should be a little more than the number of hours that have been allocated to the team members. This is because sizing should be conservative and if the team finish their tasks more quickly than expected, they should have enough other tasks to keep busy.</p> <ul> <li>\ud83d\udd75\ufe0fWho can place tickets in this column?:</li> <li>\ud83c\udfc9 Only the project planner ('scrum master') in collaboration with the client where appropriate.</li> </ul> <p>\ud83c\udfc3This Sprint: These are tickets that will be worked on in the this sprint. They are placed here at the start of the sprint in the order (vertically) in which they should be completed, highest priority at the top. Only sized tickets should be in this column. As per above, the cumulative total of the sizes of issues in the sprint column should be a little more than the number of hours that have been allocated to the team members. - \ud83d\udd75\ufe0fWho can place tickets in this column?: - \ud83c\udfc9 Only the project planner ('scrum master') in collaboration with the client where appropriate.</p> <p>\ud83d\udc69In Progress: When the team members are ready for a new task, they move the task they will work on from the 'This Sprint' Column to the 'In Progress' column to indicate they are working on that issue. In most cases, there should only be one issue per team member in the 'In Progress' column at any time.</p> <ul> <li>\ud83d\udd75\ufe0fWho can place tickets in this column?:</li> <li>\ud83d\udc69Only the developer / team member working on the ticket.</li> </ul> <p>\ud83d\udc68Needs Review: When a team member completes their work on an issue, they place it in the needs review column. At this point our testing team members will go and verify they feature works as expected.</p> <ul> <li>Situations in which a ticket may fail review:</li> <li>Functionality does not match requirements in ticket.</li> <li>Changes result in a regression elsewhere.</li> <li>Changes result in errors to be logged in javascript console.</li> <li>Changes result in errors to be logged in sentry.</li> <li>Changes result in errors to be logged in console.</li> <li> <p>If a ticket fails review, it should be moved back to the 'This Sprint' column.</p> </li> <li> <p>\ud83d\udd75\ufe0fWho can place tickets in this column?:</p> </li> <li>\ud83d\udc69Only the developer / team member working on the ticket.</li> </ul> <p>\ud83c\udfcb\ufe0fClient to Test: Whenever a feature has been completed and tested internally, it should be signed off by the client. Placing the issue in this column indicates to the client that this item is ready for testing. In some cases, changes may not be client facing and the project planner may choose to skip this step.</p> <ul> <li>Situations in which a ticket may fail review:</li> <li>Functionality does not match requirements in ticket.</li> <li>Customer raises an issue with the functionality not working as expected.</li> <li> <p>Customer encounters a bug or error using the new functionality.</p> </li> <li> <p>\ud83d\udd75\ufe0fWho can place tickets in this column?:</p> </li> <li>\ud83d\udc69The developer / team member working on the ticket.</li> <li>\ud83d\udc68The tester who just completed working on the ticket.</li> <li>\ud83c\udfc9The project planner / scrum master</li> </ul> <p>\u270d\ufe0fNeeds Docs: Documentation should be a standard part of the sizing of tickets. This should include code documentation ('docstrings'), API docs, user manual entries etc. If, however, the ticket results in changes to workflows (e.g. screenshots need updating, workflows change)</p> <ul> <li>\ud83d\udd75\ufe0fWho can place tickets in this column?:</li> <li>\ud83c\udfc9The project planner / scrum master</li> </ul> <p>\ud83c\udf04 Done: Everyone is happy! Yay! The ticket can be marked as done.</p> <ul> <li>\ud83d\udd75\ufe0fWho can place tickets in this column?:</li> <li>\ud83d\udc69The customer.</li> <li>\ud83c\udfc9The project planner / scrum master</li> </ul>"},{"location":"company/kartoza/essential_software/","title":"Essential Software","text":"<p>In this section we will enumerate all of the software packages you should have installed on your computer. There are some items that can be considered optional, but you should be aware of all of these tools and install them when and as the need arises.</p> <p>In addition to essential software, there are many useful packages that you should be aware of that can be occassionally used to improve your productivity. Tim maintains a list of these declaratively in his Nix Config Repo.</p>"},{"location":"company/kartoza/essential_software/#graphical-applications","title":"Graphical Applications","text":""},{"location":"company/kartoza/essential_software/#qgis","title":"QGIS","text":"<p>URL:  Purpose: Notes:</p>"},{"location":"company/kartoza/essential_software/#_1","title":"Essential Software","text":"<p>URL: https://flameshot.org/ Purpose: Taking annotated screenshots efficiently Notes: I map Ctrl-4 to the tool in gnome like this:</p> <p></p>"},{"location":"company/kartoza/essential_software/#libreoffice","title":"LibreOffice","text":"<p>URL: libreoffice.org/ Purpose: Office productivity suite. Notes:</p>"},{"location":"company/kartoza/essential_software/#services","title":"Services","text":""},{"location":"company/kartoza/essential_software/#postgresql","title":"Postgresql","text":"<p>URL: www.postgresql.org Purpose: Relational database management system with GIS support. Notes:</p>"},{"location":"company/kartoza/essential_software/#console-applications","title":"Console applications","text":"<p>These are applications used from the command line.</p>"},{"location":"company/kartoza/essential_software/#asciinema-","title":"Asciinema \ud83c\udf9e\ufe0f","text":"<p>URL: https://asciinema.org/ Purpose: Terminal session screen recorder Notes:</p> <p>Nixos packages: * asciinema * asciinema-agg * asciinema-scenario</p>"},{"location":"company/kartoza/essential_software/#bat-","title":"bat \ud83d\udd27","text":"<p>bat is a modern alternative to 'cat'. It lists the contents of a file on stdout.</p> <p></p> <p>Nixos packages: bat</p>"},{"location":"company/kartoza/essential_software/#btop","title":"btop","text":"<p>btop : A modern alternative to top - list the running processes on your system |\ud83d\udd27 </p> <p></p> <p>Nixos packages: btop</p>"},{"location":"company/kartoza/essential_software/#byobu","title":"byobu","text":"<p> byobu | Terminal multiplexer |\ud83d\udd27  comma # handy \"nix-shell -p\" shortcut - just do \", programmename\" and it does rather \"nix-shell -p programmename\" cowsay | Useful tool to say stuff with the persona of a cow |\ud83d\udd27  exa |Modern alternative to ls - lists files |\ud83d\udd27  fd | Modern alternative to find - finds files |\ud83d\udd27  ffmpeg_5-full | Free MPEG encoder |\ud83c\udf9e\ufe0f figlet | Make ASCII text titles |\ud83d\udd27  git | Version control management tools |\ud83d\udd27  gotop | go based top - list the running processes on your system |\ud83e\ude7a iftop | list the network traffic on a network interface|\ud83e\ude7a imagemagickBig | command line based image manipulation  |\u270f\ufe0f lazydocker | helpful docker command line docker ui |\ud83d\udd27  lazygit | helful git command line ui|\ud83d\udd27  mc | console based file manager |\ud83d\udcc1 ncdu | disk usage reporting too | \ud83d\uddb4 neofetch | show useful system info (I have it show whenever I open a terminal) |\ud83d\udd27  nethogs | show which processes are consuming bandwidth on your machine |\ud83d\udd27  nix-direnv | Automatically set up your dev env when you enter a directory. |\ud83d\udd27  pgcli | More modern postgres terminal client |\ud83d\udd27  wget | Fetch files from the internet in your console |\ud83d\udd27  dua | better du command |\ud83d\udd27 exa | better ls command |\ud83d\udd27 fish | fish shell like bash but with lots of goodies |\ud83d\udd27 gping | a better ping implementation |\ud83d\udd27 kitty | nicer terminal emulator with a lot of cool features |\ud83d\udd27 lftp | for remote backups |\ud83d\udd27 ranger | console file manager |\ud83d\udcc1 powertop | swee what apps use the most power on your machine |\ud83d\udd27 restic | for local backups |\ud83d\udd27 unzip | unzip stuff |\ud83d\udd27 usbutils | lsusb etc |\ud83d\udd27</p>"},{"location":"company/kartoza/essential_software/#git--github-account","title":"Git &amp; GitHub Account","text":"<p>URL: github.org Purpose: Version control of digital assets Notes: Although GitHub is not Git, you need to be set up to work on github.</p>"},{"location":"company/kartoza/essential_software/#auxilliary-applications","title":"Auxilliary applications:","text":""},{"location":"company/kartoza/essential_software/#gui-apps","title":"Gui apps","text":"Name Description Type audacity Sound file editor \ud83c\udfa4 blender 3D Graphical Scene Modelling \ud83d\udce6\ufe0f dbeaver Database IDE and diagram generator \ud83d\udee2\ufe0f deja-dup Backup Application \ud83d\uddb4 emote Emoji picker (invoke with ctrl-alt-e) \ud83d\ude00 drawio Diagraming tool \u270f\ufe0f flameshot Screenshot tool \ud83d\udda5\ufe0f gimp Raster painting / image creation tool \u270f\ufe0f gnome.gnome-sound-recorder Simple voice / recorder \ud83c\udfa4 gnome.gnome-terminal Terminal application for command line work firefox Web Browser \ud83c\udf10 google-chrome Google's ubiquitous web browser \ud83c\udf10 gpick Colour picker \u270f\ufe0f inkscape Vector art / graphics application \u270f\ufe0f kdenlive Timeline based video editor \ud83c\udf9e\ufe0f keepassxc Offline password vault \ud83d\udd11 nextcloud-client Sync client for NextCloud \ud83c\udf10 obs-studio Screencasting application \ud83c\udf9e\ufe0f paperwork Document manager \ud83d\uddce qtcreator C++ IDE for Qt5/6 \ud83d\udd27 slack Chat tool for company communications \ud83d\udcac synfigstudio 2D animation studio \u270f\ufe0f tdesktop Telegram chat client \ud83d\udcac vscode Multi-language IDE \ud83d\udd27 xournalpp PDF annotation / editing tool \u270f\ufe0f citations Citation manager \ud83d\uddce emblem Modern emjoi browser (alternative to emote) - needs Gnome 44 \ud83d\ude00 eyedropper Colour picker (alternative to gpick) - needs Gnome 44 \u270f\ufe0f gaphor lorem Lorem ipsum text generato - needs Gnome 44 \ud83d\uddce solanum - needs Gnome 44 zap Sound effects board  - needs Gnome 44 \ud83c\udfa4"},{"location":"company/kartoza/operating_principles/","title":"Kartoza's Thirty Principles","text":"<ol> <li>We are innovative. This means we are not set in our ways. We have a diverse toolset and skills and while we retain combinations that work well, we readily change and adapt these and discard those that don't work.</li> <li>Company decisions conform to the Strategic Objective, these Thirty Principles and Working Procedures documents.</li> <li>We are a lean and agile company and follow SAFe principles.</li> <li>We promote open and easy access to information.</li> <li>We provide opportunities for community involvement.</li> <li>We provide opportunities for education and training in FOSS tools and GISc.</li> <li>Our internal systems and client-facing solutions are scalable, resilient and reliable.</li> <li>We are the highest-quality spatial IT company in South Africa and equal to the best globally. We do whatever it takes to ensure the quality of service to our clients, employees and suppliers is impeccable.</li> <li>We draw solid lines, thus providing an exact status of where things stand. Documented procedures are the main defence against grey-area problems.</li> <li>\"Get the job done.\" Can the employee do his or her job, or is there always a complication of one kind or another? This ability to \"get the job done quickly and accurately without excuses or complications\" is the most valuable trait an employee can possess.</li> <li>Employees come first. We employ people who have an innate desire to perform at a hundred percent. We reward them accordingly. The natural outcome is that we serve our clients well.</li> <li>We are not fire killers. We are fire prevention specialists. We don't manage problems; we work on system enhancement and system maintenance to prevent problems from happening in the first place.</li> <li>Problems are gifts that inspire us to action. A problem prompts the act of creating or improving a system or procedure. We don't want setbacks, but when one occurs we think, \"thank you for this wake-up call,\" and take assertive system-improvement action to prevent the setback from happening again.</li> <li>We focus on just [[a few manageable markets|Kartoza Portfolio Offerings]]. Although we watch for new opportunities, in the end we provide \"just a few products and services implemented in superb fashion,\" rather than a complex array of average-quality offerings.</li> <li>We find the simplest solution. Ockham's Law, also called the Law of Economy, states, \"Entities are not to be multiplied beyond necessity . . . the simplest solution is invariably the correct solution.\"</li> <li>The money we save or waste is not Monopoly money! We are careful not to devalue the worth of a Rand (or Dollar or Rupiah or Euro) just because it has to do with the business.</li> <li>We operate the company via documented procedures and systems. \"Any recurring problem can be solved with a system.\" We take the necessary time to create and implement systems and procedures and in the end, it is well worth it. If there is a recurring problem, a written procedure is created to prevent the problem from happening again. On the other hand, we don't bog down the organisation with processes and procedures that target once-in-a-while situations. Sometimes we elect to not create a procedure.</li> <li>\"Just don't do it.\" Eliminate the unnecessary. Many times, elimination of a system, protocol, or potential project is a very good thing. Think simplicity. Automate. Refine to the smallest number of steps or discard altogether. Would a simple \"no\" save time, energy or money?</li> <li>Our documented systems, procedures, and functions are \"off-the-street.\" This means anyone with appropriate training can perform procedures unassisted. The real-world evidence of this is we can hire an individual \"off-the-street\" who has good GIS or programming skills and have him or her doing QGIS or development work within a week. For this result, protocols have to be efficient, simple, and thoroughly documented.</li> <li>Do it NOW. All actions build on \"point-of-sale\" theory. We don't delay an action if it can be done immediately. Just like any major retail outlet, we \"update inventories and databases at the exact time the transaction takes place.\" There is no paperwork floating around the office after a physical transaction. We ask, \"How can we perform the task NOW without creating lingering details that we must clean up later?\"</li> <li>We glean the Kartoza mindset from Stephen Covey's books, including The 7 Habits of Highly Successful People, First Things First, and The 8th Habit. As well, we consider Good to Great by Jim Collins; The E-Myth Revisited by Michael Gerber; Awaken the Giant Within by Anthony Robbins; Maverick by Ricardo Semler; Work the System by Sam Carpenter.</li> <li>We pattern individual organisation upon Franklin-Covey theory. We use organising mechanisms that are always at hand. We prioritise, schedule, and document. The system is always up-to-date and we use it all the time. For Kartoza, these are Github, Google Suite (Mail, Drive, etc.), SyncThing, Sage, OpenProject and others (subject to change as of August 2020)</li> <li>Sequence and priority are critical. We work on the most important tasks first. We spend maximum time on \"non-urgent/important\" tasks via Stephen Covey's time-matrix philosophy.</li> <li>We double-check everything before release. If a penchant for double-checking is not an innate personal habit, then it must be cultivated. Double-checking is a conscious step in every task, performed either by the individual managing the task or someone else.</li> <li>Our environment is spotless: clean and ordered, simple, efficient, functional. No \"rat's nests\", literally or figuratively.</li> <li>Employee training is structured, scheduled, and thorough. Assertive client contact is also structured, scheduled, and thorough.</li> <li>We are deadline-obsessed. If someone in the organisation says they will be finished with a task or project by a certain date and time, then he or she commits to finishing by that deadline (or, if legitimate delays intrude, advise coworkers well in advance that the deadline is impossible to meet).</li> <li>We maintain equipment and keep it a hundred percent functional at all times. If something is not working as it should, fix it now even if it's not necessary to fix it now. It's a matter of good housekeeping and of maintaining good habits. This is just the way we do things.</li> <li>Mastery of the English language is critical. We are aware of how we sound and what we write. We do whatever we can to improve. We are patient as a coworker corrects us.</li> <li>We study to increase our skills. A steady diet of reading and contemplation is vital to personal development. It is a matter of self-discipline.</li> <li>We avoid multitasking activities. When communicating with someone else, we are a hundred percent present. We give full attention to the person in front of us (or to the task at hand). We focus on listening and understanding. Read the classic 'Treating Type A Behaviour and Your Heart' by Meyer Friedman. \"Mindfulness\" is paying complete attention to one thing at a time: read 'Full Catastrophe Living' by Jon Kabat-Zinn.</li> <li>When in the office we work hard on Kartoza business. We keep our heads down; we focus, and in turn the company pays well. That's \"the deal\". The work week rarely exceeds forty hours.</li> <li>Complete means \"complete.\" Almost or tomorrow is not \"complete.\" In particular, this is germane to GitHub issues.</li> <li>We strive for a social climate that is serious and quiet yet pleasant, serene, light, and friendly. Kartoza is a nice place to work.</li> <li>As opposed to \"doing the work,\" an \"operations manager's\" job is to create, monitor, and document systems (which consist of people, equipment, procedures, and maintenance schedules).</li> <li>The Managing Director(s) oversee department heads and overall systems. It is the MD's job to direct, coordinate, and monitor the company.</li> </ol>"},{"location":"company/kartoza/scrum_at_kartoza/","title":"Scrum at Kartoza","text":"<p>The Kartoza Scrum Approach</p> <p>Our Scrum approach is intended to provide an excellent experience for our customers. </p> <p>Our handbook provides three sections that explain our approach to Scrum.</p> <ol> <li>This document, which is a high level explanation of our approach to Scrum and how we run our sprints.  </li> <li>The daily workflow document provides a step be step guide on how to run your work day. - https://kartoza.github.io/TheKartozaHandbook/company/kartoza/daily_workflow/ </li> <li>The scrum workshop is a practical exercise intended to be used in a role-play situation during staff induction, to train you in following our scrum methodology. - https://kartoza.github.io/TheKartozaHandbook/company/kartoza/working_in_sprints/ </li> </ol> <p>What do we do differently?</p> <p>One key philosophical difference in our approach to Scrum is that we treat the company as the project and the projects as subprojects of the company. This way, we can still execute Scrum in a classical way with daily standups etc., but we work at the company level\u2014our company as the main project. </p> <p>Scrum and Agile methodologies are designed around teams of eight and usually have a few larger projects with multiple teams working on them. Kartoza is a small company with many small projects where we typically have only one or two developers per project. Therefore, we have adapted Scrum to create our own custom implementation, which is well-suited to our company.</p> <p>Company wide stand-up meetings</p> <p>The key difference in our approach is that every day when we have our Scrum stand-up meeting, it will be company-wide for all people involved in production work. This approach has the following advantages:</p> <ol> <li>We reduce the impact of \u2018digital isolation\u2019 - the feeling of being disconnected from your colleagues and being \u2018lonely\u2019 in your work. By joining a company wide standup, you get to interact with all of your colleagues at least once a day.  </li> <li>We increase accountability. Having to present your work in front of the whole company every day provides an incentive to have something to show for your work the previous day.   </li> <li>We increase motivation. When you falter in your work, your colleagues will notice and be there to boost you. When you are excelling in your work, the whole team is going to be there celebrating your success with you.</li> </ol> <p>Calibration Points - Effectiveness</p> <p>The daily standup calls are intended to check in on key calibration points that allow us to measure the effectiveness of team members and to catch delivery problems early. The calibration points need to be considered together as a whole, as each adds an element to the narrative of a staff member\u2019s productivity patterns. Each role in the company has different calibration points:</p> <ol> <li>Developer / Technical Staff: </li> <li>Story points  </li> <li>PRs (if relevant)  </li> <li>Lines of Code / Features Digitised / Lessons Created  </li> <li>Hours logged  </li> <li>Senior Developer </li> <li>Everything a developer needs to do as above  </li> <li>Developers stay within working calibration boundaries  </li> <li>Deliverable / UR completion in time budget  </li> <li>Daily comms with customer (Communication about progress)  </li> <li>Review and approve PRs  </li> <li>Team Lead / Scrum Masters </li> <li>Did we meet all the requirements of the contract?  </li> <li>Did we make invoicing / payment milestones?  </li> <li>Did we complete the project within budget?  </li> <li>Did we complete the project within timelines?  </li> <li>Reporting to Management  </li> <li>Review and Approve Time Sheets  </li> <li>Act as \u2018spirit guides\u2019 for the teams doing implementation work  </li> <li>Project Manager </li> <li>Takes ultimate responsibility  </li> <li>Project managers are the linchpin in the company, ensuring a good relationship with customers, providing follow-up business opportunities, and pursuing those opportunities when they arise. </li> </ol> <p>Consequences of miscalibration</p> <p>During the course of a project, miscalibration will lead to client disappointment, missed deadlines, poorly implemented solutions, lost future business, staff stress and burn out etc. So it is very important that we all operate within the ranges specified. We want to take a moment to consider the financial model of a consulting company like Kartoza and where the weight of responsibility falls when a team member\u2019s work is not within the calibration guidelines. There are three sources that can potentially bear the cost of a team member\u2019s outputs:</p> <p>Customer-funded work is the first prize! When a customer is funding our work we have a sustainable company and we are meeting our company\u2019s value proposition:  we share our team member\u2019s skills and expertise in order to solve our customer\u2019s problems, the customer pays us as compensation for our effort.</p> <p>Kartoza-funded work. This can often be a precursor or warning that a project is going outside of its calibration envelope. If Kartoza is funding work that is being delivered to a customer, we are essentially paying to build someone else\u2019s solution. In some cases we may choose to do this because:</p> <ol> <li>We have underestimated the effort needed to provide a solution for the customer.  </li> <li>We want to do something \u2018extra\u2019 for the customer as a good-will gesture  </li> <li>We want to contribute time for a \u2018good cause\u2019 like an open source project or a social initiative.</li> </ol> <p>The decision to invest Kartoza funds into the creation of a project for a customer needs to be taken by Kartoza management. Technical staff should never make this call, and they should defer to management if they see a need for this.</p> <p>Staff Funded Work: Staff are paid by Kartoza to carry our assignments needed to meet our obligations to our clients. When staff work within their calibrated norms, they receive compensation from Kartoza. When staff work outside of their calibrated norms, we do not accept their timesheets. Let\u2019s give some practical examples:</p> <p>Example 1: Jane has spent 14 hours across the last two days on a ticket that is sized as 4 hours. Our norms dictate that as soon as it becomes evident that you will not complete a ticket within its allotted time, you need to raise this as an issue to the PM. The PM can decide one of four actions:</p> <ol> <li>Resize the ticket to give you more time  </li> <li>Halt work on the ticket  </li> <li>Allocate the ticket to a different developer  </li> <li>Technical call with senior to give guidance required</li> </ol> <p>Jane\u2019s time logged in excess of the 4 hours allocated for the ticket will be rejected and she will be responsible for catching up her monthly hour quota.</p> <p>Example 2: Fred has logged an 8 hour day and has closed 8 story points. When reviewing his PR\u2019s the technical lead discovers that the 8 hours of time logged equate to a single PR with 10 lines of extremely trivial code changes. The amount of productivity demonstrated is inconsistent with the hours logged and the technical team raises this as an issue to the PM.</p> <p>Example 3: Lee has logged only 5 hours on their timesheet yesterday and only closed 3 of the 5 story points they were allocated. The work they did do is of good quality, but the project is now behind schedule. The issue gets raised by the PM who requests that Lee puts in the missing time in order to bring the project back in line with the schedule. </p> <p>Protecting our time</p> <p>Because our work calibration process outlined above requires rigorous and careful time management, we need to \u201cprotect our time\u201d. This means that we manage our company and our projects in a way that time is not squandered and that every hour that we log on a client project delivers the most possible value for that hour. Some key elements to protecting our time are:</p> <ol> <li>When holding a meeting e.g. a technical break out call, only the relevant people needed to solve the problem should be present.  </li> <li>Critically review all meetings, especially recurring meetings and ruthlessly remove meetings that do not bring value to the company.  </li> <li>Scrum meetings need to be strictly time boxed and run according to a tightly controlled schedule.</li> </ol> <p>In the section that follows, we are going to run through the daily sequence of our two week sprints and highlight other aspects of how we do Scrum the \u2018Kartoza Way\u2019.</p> <p>Terminology</p> <ol> <li>Deliverable / User Requirement - a key feature in the project  </li> <li>Scrum master - Project manager who leads the project and mentors the staff to follow the Kartoza Scrum methodology  </li> <li>Milestone - A significant point in a project timeline that marks progress and helps to track the project's status. Milestones are used to represent key achievements. These are often linked to payments and the tasks shown on the timesheets</li> </ol> <p>Roles and Responsibilities </p> <ul> <li>The Scrum master maintains the project vision and aligns it with client needs. They understand the timelines and project constraints. The Scrum master is responsible for understanding the client's needs, and ensuring that we execute according to the client's needs and the contract. The Scrum master will only maintain the epics and the big-picture items on the scrum board, the \u2018detailed\u2019 issues are managed by the rest of the team.  </li> <li>Our senior developers act as technical team leads, manage tickets and ensure they are clearly described, so developers know exactly what\u2019s required. Senior developers dedicate time daily to reviewing pull requests, ensuring code quality and adherence to standards. This role is responsible for maintaining the tickets that will be worked on and ensuring that all work planned aligns with the architecture set out by the senior developer.  </li> <li>Developers and technical staff work to implement the solution for our client. They work through the issues they have been assigned on the scrum board. The tickets should be written such that the developer who is assigned the work is completely clear about what the requirements are and what the technical approach should be for completing that task.</li> </ul> <p>Our Scrum flow</p> <p>First, let\u2019s take a look at our calendar of events through one sprint cycle:</p> <p>\ud83d\udfe2 - Team Lead, \ud83d\udd35 - Senior developer, \ud83d\udfe3 - Developer/GIS</p> Day Communication Action 1 Sprint Meeting (with client) \u000b Daily standup (internal) Client feedback Client update email with meeting notes Prep:\u000b\ud83d\udd35\ud83d\udfe3 Breakdown parent tickets into smaller tickets. \ud83d\udd35\ud83d\udfe3 Ensure tickets are sized  \ud83d\udfe2\ud83d\udd35 Ensure any new tickets are added to the board\u000b\ud83d\udfe2\ud83d\udd35 Assign tickets to resources (if not already done)\u000b\ud83d\udfe2 Ensure board is up to date\u000b\u000b\ud83d\udfe2\ud83d\udd35 Prioritise the tickets for \u201cThis sprint\u201d and \u201cNext sprint\u201d\u000b\ud83d\udfe2\ud83d\udd35Assign tickets to resources (if not already done) \ud83d\udd35Breakdown parent tickets into smaller tickets. \ud83d\udd35Ensure tickets are sized \ud83d\udfe2Ensure any new tickets are added to the board Prep: \ud83d\udfe2 Go through the PR list. \ud83d\udd35Review PRs What tickets were closed/worked on yesterday? What tickets are being worked on today? Any blockers? \ud83d\udd35Senior dev to send client daily update with new changes highlighted. \ud83d\udfe2To be sent by PMO 2 Daily standup (internal) Client feedback Prep: \ud83d\udfe2 Go through the PR list. \ud83d\udd35Review PRs What tickets were closed/worked on yesterday? What tickets are being worked on today? Any blockers? \ud83d\udd35Senior dev to send client daily update with new changes highlighted. 3 Daily standup (internal) Client feedback Prep: \ud83d\udfe2 Go through the PR list. \ud83d\udd35Review PRs What tickets were closed/worked on yesterday? What tickets are being worked on today? Any blockers? \ud83d\udd35Senior dev to send client daily update with new changes highlighted. 4 Daily standup (internal) Client feedback Prep: \ud83d\udfe2 Go through the PR list. \ud83d\udd35Review PRs What tickets were closed/worked on yesterday? What tickets are being worked on today? Any blockers? \ud83d\udd35Senior dev to send client daily update with new changes highlighted. 5 Daily standup (internal) Client feedback Prep: \ud83d\udfe2 Go through the PR list. \ud83d\udd35Review PRs What tickets were closed/worked on yesterday? What tickets are being worked on today? Any blockers? \ud83d\udd35Senior dev to send client daily update with new changes highlighted. 6 Daily standup (internal) Client feedback Client update email Prep: \ud83d\udfe2 Go through the PR list. \ud83d\udd35Review PRs What tickets were closed/worked on yesterday? What tickets are being worked on today? Any blockers? \ud83d\udd35Senior dev to send client daily update with new changes highlighted.  \ud83d\udfe2 To be sent by PMO 7 Daily standup (internal) Client update email Prep: \ud83d\udfe2 Go through the PR list. \ud83d\udd35Review PRs What tickets were closed/worked on yesterday? What tickets are being worked on today? Any blockers? \ud83d\udd35Senior dev to send client daily update with new changes highlighted. 8 Daily standup (internal) Client feedback Prep: \ud83d\udfe2 Go through the PR list. \ud83d\udd35Review PRs What tickets were closed/worked on yesterday? What tickets are being worked on today? Any blockers? \ud83d\udd35Senior dev to send client daily update with new changes highlighted. 9 Daily standup (internal) Client feedback Prep: \ud83d\udfe2 Go through the PR list. \ud83d\udd35Review PRs What tickets were closed/worked on yesterday? What tickets are being worked on today? Any blockers? \ud83d\udd35Senior dev to send client daily update with new changes highlighted. 10 Sprint Retrospective (company wide - this may not align with the end of sprint)\u000b Daily standup (internal) Client feedback \ud83d\udfe2\ud83d\udd35\ud83d\udfe3 Prep: Fill out retrospective board What went well?  What didn\u2019t go well? What should we continue? Prep: \ud83d\udfe2 Go through the PR list. \ud83d\udd35Review PRs What tickets were closed/worked on yesterday? What tickets are being worked on today? Any blockers? \ud83d\udd35Senior dev to send client daily update with new changes highlighted. <p>Our work planning is in two week sprints. Let\u2019s break down the key activities that happen during the sprint cycle.</p> <p>Backlog Generation</p> <p>Before the sprint begins we are continuously grooming our backlog of issues. Every staff member involved in a project is responsible for, and expected to develop tickets and add them to the backlog. </p> <ol> <li>Tickets may be raised organically (e.g. a developer realising something needs to be taken care of while working in a particular area of the code base),   </li> <li>systematically (e.g. the PM writing tickets based on the deliverable requirements) or   </li> <li>externally (e.g. our customer is very engaged with the project and likes to add tickets to the board). </li> </ol> <p>Writing a ticket is not a commitment to actually do the work, it is a marker to show that there is something that can or should be done to move the project forward.</p> <p>Sprint Planning</p> <p>Throughout the sprint, all team members are expected to groom the ticket queue by creating new tickets for the next sprint. The day before the sprint planning meeting, the senior developer on the project will review the backlog and create / identify all the issues that are (in his/her opinion) needed to achieve the next iteration of the project. The senior developer will also estimate initial sizes for each ticket and assign a developer to each ticket.</p> <p>The sprint planning meeting should happen the day before the sprint starts and is a 45-minute commitment from each developer on the project, the project technical lead, and the Scrum master. It may include the client if they are engaged at the sprint level. The sprint planning call is used to assign tickets to developers and address any immediate questions. Any further technical discussions are scheduled separately. </p> <p>During the sprint planning call, the PM and senior developer will check that they are in alignment with regards to what needs to be built next. The developers on the project will confirm they are able to carry out the issues that have been assigned to them. </p> <p>\u26a0\ufe0f Most importantly, each person who has been assigned tickets will negotiate the size allocation for each ticket and ultimately agree to complete the work in the allocated time.</p> <p>Story points for each ticket are collaboratively determined during sprint planning. The sprint technical lead provides initial estimates, and developers can negotiate adjustments. Once agreed, this becomes a mini-contract between the company and the developer. Any deviation from the agreed time allocation without prior approval from the project Scrum lead will result in timesheets not being approved.</p> <p>The Sprint</p> <p>Now the sprint is underway. Team members work through the issues in the \u2018this sprint\u2019 column of the scrum board and generate pull requests.</p> <p>The daily PR Review</p> <p>Senior developers will, as their first task every day, carry our PR reviews. This is again a time boxed activity expected to take 15-20 minutes. </p> <p>Developers are expected to structure their PR\u2019s in a way that makes it easy to review. For example code should already be Black formatted, and follow our coding conventions.</p> <p>The daily standup</p> <p>Each day, everyone that is involved in project delivery will meet for a single 15-20 minute daily standup. This event is in person, on-camera and is mandatory. During this call, the PMs will work sequentially through each project board, taking feedback from the team members and reviewing the tickets closed during the previous day\u2019s activity. This meeting is strictly time-boxed so no \u2018chit chat\u2019 is allowed. Neither is any deep technical discussion - those discussions can be taken offline as 1:1 calls by the relevant team members.</p> <p>Our company operates on a \"hours logged for story points\" basis. If you are an employee and have been allocated a number of story points, your timesheet needs to be coherent, meaning that the logged hours should accurately reflect the effort put into each story point. This ensures that the workload is balanced and properly documented, aligning with our operational model. We use a calibration approach (see start of this document) to ensure that there is a fair accounting for all effort going into each project.</p> <p>It's the developer's responsibility to identify any potential deviations from the expected delivery timeline early, ideally around the halfway mark. If it's apparent that the task won't be completed within the allocated story points, they must immediately inform the Scrum master. This could result in changing the story point sizing, pausing the work, or continuing for the remaining allocated time, with further evaluation afterward.</p> <p>During the daily stand-up, the Scrum master will move tickets into the \"In Progress\" column, and once the tasks are completed, the developer will move them into the \"Needs Review\" column.</p> <p>When a developer deviates from the expected norms, the first instance results in a warning from the Scrum master or team leads. The second time also warrants a warning. From the third instance onward, no further discussion will be held, and the timesheet for that day will be rejected. It's the developer's responsibility to adhere to expected work practices to avoid impacts on timesheet approval.</p> <p>To calibrate our work properly, tickets must be properly created, assigned, sized, and allocated to developers. Daily stand-ups provide insight into completed work, while the pull request (PR) queue serves as a secondary calibration tool. A senior developer reviews PRs daily, rejecting those lacking passing tests, not following coding standards, or being unreasonably long without pre-approval. This ensures compliance and provides an opportunity for peer review, catching potential oversights.</p> <p>Developers are expected to produce work that aligns with the ticket sizes and allocated days. While performance isn't measured solely by the number of PRs, the amount of work submitted should be proportionate to the time spent. This helps the technical lead and Scrum master gauge productivity effectively.</p> <p>When there is little question from the stand-up about productivity, for example, the developer has demonstrated that they closed their allocated share of tickets and moved the project forward substantively, the team lead and the Scrum master do not need to do further probing into the productivity of that developer. If there is a question, then they will go and review the PRs and the timesheets submitted by that developer for that day and understand if there is a disconnect between the work that should have been produced and the work that was claimed for on the timesheets and the work that was closed in the tickets.</p> <p>Company-Wide Sprint Retrospective </p> <p>At the end of each sprint, there is a company-wide sprint retrospective meeting lasting one hour, with all staff present. During this session, achievements are celebrated, and feedback is shared. The meeting consists of two parts: </p> <ol> <li>the first 30 minutes for highlighting achievements of the past sprint by each project or developer,   </li> <li>and the second 30 minutes for reviewing the sprint using a review board. The review may include classic questions like what to start, stop, or continue doing, or other engaging questions. Outcomes of the sprint review include acknowledgment and motivation for staff, cross-project knowledge sharing, and stimulation of ideas and inspiration. This also helps with rapid onboarding for staff moving between projects.       </li> </ol>"},{"location":"company/kartoza/scrum_at_kartoza/#_1","title":"Scrum at Kartoza","text":""},{"location":"company/kartoza/setting_up_your_computer/","title":"Setting up your PC","text":""},{"location":"company/kartoza/setting_up_your_computer/#hardware","title":"Hardware","text":"<p>Here are the standard minimum guidelines for hardware for Kartoza Staff:</p> <p>Laptops are preferred in general. Many of our staff work in areas with unreliable power supply and so you need to be able to work offline for at least four hours.</p>"},{"location":"company/kartoza/setting_up_your_computer/#admin-staff","title":"Admin Staff","text":"<p>Admin staff tend to have less demanding activities which is reflected in the hardware:</p> Feature Requirements RAM 8GB Hard Disk 256GB SSD Internal Display 1920 x 1080 or better External Display 1920 x 1080 or better Operating System Ubuntu LTR CPU Mid range e.g. i5 4 core or Athlon equivalent"},{"location":"company/kartoza/setting_up_your_computer/#gis-staff","title":"GIS Staff","text":"<p>GIS Staff need laptops with good storage capacity for accommodating large GIS datasets, and good processing power to perform time-consuming analysis quickly.</p> Feature Requirements RAM 16GB Hard Disk 1TB SSD Internal Display 1920 x 1080 or better External Display 1920 x 1080 or better Operating System Ubuntu LTR CPU Mid range e.g. i5 4 core or Athlon equivalent"},{"location":"company/kartoza/setting_up_your_computer/#developer-staff--devops","title":"Developer Staff &amp; Devops","text":"<p>Developer Staff and Devops need laptops with processing power so they can run multiple containers to emulate the deployment environment for their apps. Developer staff tend to have more technical skills and may install their own preference of Operating System if they prefer.</p> Feature Requirements RAM 16GB Hard Disk 500GB SSD Internal Display 1920 x 1080 or better External Display 1920 x 1080 or better Operating System Ubuntu LTR or user preference CPU Mid range e.g. i5 4 core or Athlon equivalent"},{"location":"company/kartoza/setting_up_your_computer/#additional-hardware","title":"Additional Hardware","text":"<p>All staff should in addition be issued with:</p> <ul> <li>A USB headset. USB headsets include their own DSP (Digital Sound Processor) and will generally have a better sound quality than an analogue headset.</li> <li>An external disk for backups. This should again be encrypted. The disk should be 4x the size of the hard disk. Use D\u00e9j\u00e0 Dup Backups to run automatic backups on a nightly basis.</li> <li>A kensington lock. This should be used whenever the laptop is left unattended in a public place (i.e. anywhere other than your home).</li> <li>A Yubikey. This will be used to authenticate to Google Apps for Domains (Via Yubikey TOTP), BitWarden, your local PC login (via FIDO2) and other services such as NextCloud. Each staff member should be issued with two of these devices and the second should be stored at home in a safe place in case the first is lost. One of following models are suggested:</li> </ul> <p> </p>"},{"location":"company/kartoza/setting_up_your_computer/#base-install-requirements","title":"Base Install Requirements","text":"<p>Every staff computer should have the following as a minimum:</p> <ul> <li> <p>Encrypted disk. Under Linux use LUKS when you install to encypt at a minimum your home partition. Ideally your whole system should be encrypted since if you run docker, postgres and other similar services, you have exposure to data loss if someone steals your PC.</p> </li> <li> <p>Strong password. The password for your account should not be used for any other system.</p> </li> <li> <p>Yubikey PAM Integration. We recommend as an added precaution to set up the YubiKey PAM module which will require to touch your YubiKey after typing in your system password to autheticate. The process for doing this is described here.</p> </li> </ul> <p>Yubkey locks the FIDO2 Pin by default. You should follow  these steps to unlock it first before running through the above tutorial. Note they assume you have installed the PPA in the above tutorial above first.</p> <p>Install the YubiKey GUI manager, then use the options as shown below.</p> <pre><code>sudo apt install yubikey-manager-qt\nykman-gui\n</code></pre> <p></p>"},{"location":"company/kartoza/setting_up_your_computer/#online-accounts","title":"Online Accounts","text":"<p>You need to have online accounts with the following services:</p> <ul> <li>GitHub - then set up your YubiKey as your 2FA here. As a backup 2FA you should use the GitHub mobile app. Note that using SMS for 2FA is not considered secure.</li> </ul> <p></p> <ul> <li>Google. Set up your YubiKey as your 2FA here. As a backup 2FA you should use the Google mobile app. Note that using SMS for 2FA is not considered secure.</li> </ul> <p></p> <ul> <li>Hetzner. If you are a staff member with permission to access Hetzner, set up your YubiKey as your 2FA here. Note that using SMS for 2FA is not considered secure.</li> </ul> <p></p> <ul> <li> <p>ERNext. Our admin team will provision an account for you.</p> </li> <li> <p>NextCloud. Our admin team will provision an account for you. NextCloud. If you are a staff member with permission to access Hetzner, set up your YubiKey as your 2FA here. Note that using SMS for 2FA is not considered secure.</p> </li> </ul> <p></p>"},{"location":"company/kartoza/setting_up_your_computer/#kartoza-vpn","title":"Kartoza VPN","text":"<p>We use wireguard to access our internal systems.</p> <p>On ubuntu you can install it like this:</p> <pre><code>sudo apt install wireguard\n</code></pre> <p>Also you can install the Gnome QR code app from here: https://apps.gnome.org/Decoder/</p> <ul> <li>get the config QR code from Leon via screenshare</li> <li>Use the Gnome QR Code app to scan it</li> <li>Save the resulting file to e.g. kartoza-vpn.conf</li> <li>Run this command:</li> </ul> <p><pre><code>nmcli connection import type wireguard file ~/.wireguard/kartoza-vpn.conf\n</code></pre> - The new entry should be added to your network manager now:</p> <ul> <li></li> </ul>"},{"location":"company/kartoza/setting_up_your_computer/#installing-and-importing-the-ca-certificate-for-secure-access-to-internal-company-websites","title":"Installing and Importing the CA Certificate for Secure Access to Internal Company Websites","text":"<p>Installing the Kartoza CA (Certificate Authority) certificate and importing it into your browsers is necessary for secure access to certain internal company websites.</p>"},{"location":"company/kartoza/setting_up_your_computer/#why-install-the-ca-certificate","title":"Why Install the CA Certificate?","text":"<ol> <li> <p>Secure Communication: CA certificates are used to establish secure connections (HTTPS) between your browser and web servers. This ensures that data transmitted between the client and server is encrypted and secure.</p> </li> <li> <p>Trust Verification: When you access our website, your browser checks the website's SSL/TLS certificate to verify its authenticity. This SSL/TLS certificate is issued by a trusted CA. If the CA certificate is not recognized by your browser, it will display a warning, indicating that the connection may not be secure.</p> </li> <li> <p>Internal Websites: Many companies use self-signed certificates or certificates issued by an internal CA for their internal websites. These internal CAs are not recognized by default by most browsers. Installing the internal CA certificate in your browser ensures that the browser trusts the certificates issued by the internal CA, allowing secure access to the internal websites without warnings.</p> </li> </ol>"},{"location":"company/kartoza/setting_up_your_computer/#steps-to-install-the-ca-certificate","title":"Steps to Install the CA Certificate","text":""},{"location":"company/kartoza/setting_up_your_computer/#1-obtain-the-ca-certificate","title":"1. Obtain the CA Certificate","text":"<p>For Developers, IT engineers and DevOps engineers, the Kartoza CA-certificate can be downloaded from the devops repository. One should have access to the Kartoza organization before trying to down load the certificate. The certificate will either a have a <code>.crt</code> or <code>.pem</code> extension.</p>"},{"location":"company/kartoza/setting_up_your_computer/#2-install-the-ca-certificate-on-your-machine","title":"2. Install the CA Certificate on Your Machine","text":""},{"location":"company/kartoza/setting_up_your_computer/#for-windows","title":"For Windows:","text":"<ol> <li>Double-click the CA certificate file.</li> <li>Click \"Install Certificate\".</li> <li>Choose \"Local Machine\" and click \"Next\".</li> <li>Select \"Place all certificates in the following store\".</li> <li>Click \"Browse\" and select \"Trusted Root Certification Authorities\".</li> <li>Click \"Next\" and then \"Finish\".</li> </ol>"},{"location":"company/kartoza/setting_up_your_computer/#for-macos","title":"For macOS:","text":"<ol> <li>Double-click the CA certificate file.</li> <li>The Keychain Access application will open.</li> <li>Select \"System\" from the keychains list.</li> <li>Drag the certificate file into the Keychain Access window.</li> <li>Double-click the imported certificate.</li> <li>Expand the \"Trust\" section and select \"Always Trust\".</li> </ol>"},{"location":"company/kartoza/setting_up_your_computer/#for-linux","title":"For Linux:","text":"<ol> <li>Copy the CA certificate file to the <code>/usr/local/share/ca-certificates</code> directory:</li> </ol> <pre><code>sudo cp kartoza_ca_chain.crt /usr/local/share/ca-certificates/\n</code></pre> <ol> <li>Update the CA certificates:</li> </ol> <pre><code>sudo update-ca-certificates\n</code></pre>"},{"location":"company/kartoza/setting_up_your_computer/#3-import-the-ca-certificate-into-your-browser","title":"3. Import the CA Certificate into Your Browser","text":""},{"location":"company/kartoza/setting_up_your_computer/#for-google-chrome","title":"For Google Chrome:","text":"<ol> <li>Open Chrome and go to <code>Settings</code>.</li> <li>Search for \"Certificates\" and click on \"Manage certificates\".</li> <li>Go to the \"Authorities\" tab.</li> <li>Click \"Import\" and select the CA certificate file.</li> <li>Follow the prompts to complete the import.</li> </ol>"},{"location":"company/kartoza/setting_up_your_computer/#for-firefox","title":"For Firefox:","text":"<ol> <li>Open Firefox and go to <code>Preferences</code> or <code>Options</code>.</li> <li>Search for \"Certificates\" and click on \"View Certificates\".</li> <li>Go to the \"Authorities\" tab.</li> <li>Click \"Import\" and select the CA certificate file.</li> <li>Ensure the option to \"Trust this CA to identify websites\" is checked.</li> <li>Click \"OK\".</li> </ol>"},{"location":"company/kartoza/strategic_objective/","title":"Kartoza's Strategic Objective","text":"<p>The Kartoza Strategic Objective is the basis for all corporate and individual decision making.</p> <p>Statistically we are the largest FOSS geospatial service provider in South Africa and in the top ten globally.</p> <p>We show up on the first page of google.com and google.co.za searches for these keywords: 'open source GIS'; 'FOSS GIS training'; 'FOSS GIS support'; 'geospatial web development'; 'QGIS development'</p> <p>We aim to grow Kartoza revenue by 15% year on year.</p> <p>We aim to grow our staff complement on a sustainable basis till we reach around 25.</p> <p>We understand that every result is preceded by a 1-2-3-4 step process. It is within these processes that we spend our time, as we relentlessly \"work\" the systems of the business to perfection.</p> <p>Our guiding documents are this Strategic Objective, Our Kartoza Operating Principles, and our collection of Working Procedures.</p> <p>Kartoza's primary offerings are geospatial products and services using Free and Open Source Software. These facilitate spatial decision making and provide tools for economic empowerment.</p> <p>Through intense commitment to our employees, we will contribute to the success of our clients. The consequence of having loyal, smart, hard-working, long-term, and well-compensated staff is superb quality service to customers.</p> <p>Our business is complex, with many human, mechanical and computer systems in simultaneous motion. Success depends on refined communication and organisational processes, dedicated staff, documented procedures, first-class office space and equipment, rigorous quality assurance with continuous measurement, assertive innovation, intense planned maintenance and system improvement, aggressive and measured marketing, and relentless attention to detail in every nook and cranny.</p> <p>Our competitive advantages include an established track record, our ability to solve complex problems with great design, products designed around the unique needs of the customer, thoughtful customer service that is immediate and consistent, the latest high-tech equipment and personal and corporate integrity. We use extraordinarily efficient business systems. We constantly refine and improve all internal systems and mechanisms.</p> <p>To grow, we follow a two-pronged strategy of:</p> <ol> <li>Pursuing substantial consulting, development and implementation projects in our target markets</li> <li>Building products and services that generate passive income, juxtaposed with assertive marketing efforts.</li> </ol> <p>Although we tightly direct Kartoza's operation through guiding documentation, we will modify that documentation immediately if an enhancement can be made: \"Our operational framework is rigid, but that framework can be modified instantly.\"</p> <p>We segment responsibilities into specialised \"expert compartments\" with appropriate cross-training among departments. We have backup personnel for all positions.</p> <p>Kartoza is globally active and locally relevant. Our primary vertical markets include:</p> <ul> <li>Agriculture</li> <li>Disaster preparedness, response and management</li> <li>Land information management</li> <li>Education</li> <li>Humanitarian support</li> <li>SDI support</li> <li>Monitoring and observation</li> <li>Biodiversity and conservation support</li> </ul> <p>Kartoza aims to model itself on the concept of a B-Corporation, in the sense that through the work we do and the people we employ, we aim to be socially and environmentally responsible.</p>"},{"location":"company/kartoza/working_in_sprints/","title":"Scrum Workshop Plan with GitHub Integration","text":""},{"location":"company/kartoza/working_in_sprints/#index","title":"Index","text":"<ol> <li>Summary</li> <li>Workshop Agenda</li> <li>Sprint Planning Mock-up</li> <li>Daily Stand-ups &amp; Sprint Execution</li> <li>Sprint Retrospective Mock-up</li> <li>Discussion &amp; Q&amp;A</li> <li>GitHub Integration Workflow</li> <li>Detailed Scrum Process Flow</li> <li>Key Takeaways</li> </ol>"},{"location":"company/kartoza/working_in_sprints/#summary","title":"Summary","text":"<p>This workshop is designed to train the team on our Scrum practices, with a focus on practical exercises and integration of VS Code, GitHub, and Copilot. The workshop includes mock sessions for sprint planning, stand-ups, sprint execution, retrospectives, and practical GitHub workflows.</p> <pre><code>flowchart TD\n    A[Workshop Overview] --&gt; B[Sprint Planning - 30 mins]\n    A --&gt; C[Daily Stand-up &amp; Execution - 30 mins]\n    A --&gt; D[Retrospective - 30 mins]\n    A --&gt; E[Discussion &amp; Q&amp;A - 30 mins]\n    B --&gt; F[Review Backlog &amp; Set Sprint Goals]\n    C --&gt; G[Simulate Stand-ups, Discuss Progress &amp; Blockers]\n    D --&gt; H[Start, Stop, Continue Framework]\n    E --&gt; I[Q&amp;A and Scrum Challenges]</code></pre>"},{"location":"company/kartoza/working_in_sprints/#context","title":"Context","text":"<p>It we follow some simple guidelines and give our staff clear procedures, we can improve our project execution, prevent overruns etc:</p> <ol> <li> <p>Sprint planning: There should be a proper sprint planning session in place for each project. This will be a 1-2 hour call before each sprint where all the work for the coming sprint will be planned, sized and understandable by the people that it will be allocated to.</p> </li> <li> <p>Daily Standups: They are supposed to provide accountability for the things staff promised to do the previous day and transparency about what they will do today. With the written standups, (which we keep for reference) it is very labour intensive to PMO to crosstabulate what is written with the scrum board status. It is thus better to have a daily 15 standup on video call to run through all the team's status and next tasks. These calls should be strictly time boxed.</p> </li> <li> <p>Time Logging: In the longer term we would like to capture timesheet logged hours direct from GH issues closed. We need to hold the staff accountable for doing work in X hours if they promised X hours. Each ticket is like a mini contract and if they want to deviate from it there should be a negotiation with the PM in terms of \"wont do\", \"can't do\", \"need more time\", etc. This will take a while to implement, but in the mean time coming up with some simple metrics of how many story points were closed by each dev should let you see when people are not being productive.</p> </li> <li> <p>Ticket Writing: Ticket writing is a difficult art, but each ticket should be descriptive enough that the person being tasked understands all of the requirements. In some cases just a title may be enough, but in most cases they should have a proper technical description, diagrams, wireframes etc. We are all responsible for writing tickets but the PM is responsible for NOT assigning tickets that are not adequately explained for a developer to implement. Developers should raise tickets as they work wheb they realise that new tasks will need to be done.</p> </li> <li> <p>Sprint Retrospectives: At the end of each sprint we should be holding retrospectives to review what work was done. Invite stakeholders to the retrospectives to show of the version increment. e.g. Inviting me along to look at the cemetry app as it was being built would have been wonderful. Like a standup, a retrospective should also have 3 questions: What should we stop doing, what should we start doing and what should be continue doing?</p> </li> <li> <p>Client management: Things will change in the project as requirements emerge, priorities shift etc. But we need to do a series of mini negotiations with the client as the project runs to shift time between buckets, write amendments if needed/possible etc. We do this so that we are always operating inside the parameters of the project.</p> </li> <li> <p>High Level View: The PMO lead should focus on the high level view. Check the task progress indicators like burn downs, make sure milestones are met by meeting with the PMO team weekly and checking through all the projects and their execution states. Bubble any issues up to management as needed. You should not be operating at the ticket level but at the board / project level. </p> </li> <li> <p>We support you: We are here to support you, if you need help please shout. Especially for technical things  or client check ins, please let us know when you need something.</p> </li> </ol>"},{"location":"company/kartoza/working_in_sprints/#workshop-agenda-2-hours","title":"Workshop Agenda (2 Hours)","text":"<ol> <li>Sprint Planning Mock-up (30 min)</li> </ol> <pre><code>flowchart TD\n    A[Start Sprint Planning] --&gt; B[Review Backlog Items]\n    B --&gt; C[Select Sprint Goal]\n    C --&gt; D[Estimate Tasks]\n    D --&gt; E[Capacity Planning]\n    E --&gt; F[Define Sprint Commitment]\n    F --&gt; G[End Sprint Planning]</code></pre> <ul> <li>Goal: Simulate a sprint planning session, where the team reviews a mock backlog, selects tasks, estimates them, and sets sprint commitments.</li> <li> <p>Activities:</p> <ul> <li>5 min: Review mock backlog (3-4 user stories).</li> <li>10 min: Prioritize tasks and define the sprint goal.</li> <li>10 min: Estimate tasks using story points.</li> <li>5 min: Finalize sprint commitments based on capacity.</li> </ul> </li> <li> <p>Daily Stand-ups &amp; Sprint Execution (30 min)</p> </li> </ul> <pre><code>flowchart TD\n    A[Start Stand-up] --&gt; B[Discuss Yesterday's Progress]\n    B --&gt; C[Identify Today's Tasks]\n    C --&gt; D[Address Blockers]\n    D --&gt; E[Update Scrum Board]\n    E --&gt; F[Continue Sprint Execution]\n    F --&gt; G[End of Day]</code></pre> <ul> <li>Goal: Simulate daily stand-ups and sprint execution.</li> <li> <p>Activities:</p> <ul> <li>5 min: Introduction to the stand-up format.</li> <li>10 min: Simulate two daily stand-ups.</li> <li>10 min: Discuss blockers and task progression.</li> <li>5 min: Reflect on sprint execution.</li> </ul> </li> <li> <p>Sprint Retrospective Mock-up (30 min)</p> </li> </ul> <pre><code>flowchart TD\n    A[Start Retrospective] --&gt; B[Start: What to Start Doing?]\n    B --&gt; C[Stop: What to Stop Doing?]\n    C --&gt; D[Continue: What to Continue?]\n    D --&gt; E[Discuss and Prioritize Improvements]\n    E --&gt; F[End Retrospective]</code></pre> <ul> <li>Goal: Reflect on the sprint using the \"Start, Stop, Continue\" framework.</li> <li> <p>Activities:</p> <ul> <li>5 min: Introduction to the framework.</li> <li>10 min: Participants share items for each category.</li> <li>10 min: Group discussion on improvements.</li> <li>5 min: Summarize feedback.</li> </ul> </li> <li> <p>Discussion &amp; Q&amp;A (30 min)</p> </li> </ul> <pre><code>flowchart TD\n    A[Start Discussion] --&gt; B[Share Learnings]\n    B --&gt; C[Discuss Scrum Challenges]\n    C --&gt; D[Q&amp;A Session]\n    D --&gt; E[Workshop Wrap-Up]</code></pre> <ul> <li>Goal: Reflect on the workshop and discuss challenges with Scrum implementation.</li> <li>Activities:<ul> <li>10 min: Share key takeaways from the workshop.</li> <li>10 min: Discuss common Scrum challenges such as over-commitment and blockers.</li> <li>10 min: Open Q&amp;A session.</li> </ul> </li> </ul>"},{"location":"company/kartoza/working_in_sprints/#github-integration-workflow","title":"GitHub Integration Workflow","text":"<p>Here, we focus on integrating GitHub into the development workflow using VS Code and Copilot, showing how to manage issues, branches, and pull requests (PRs).</p> <pre><code>flowchart TD\n    A[Create GitHub Issue] --&gt; B[Create Branch in VS Code]\n    B --&gt; C[Work on Code with Copilot]\n    C --&gt; D[Commit Changes]\n    D --&gt; E[Push to GitHub]\n    E --&gt; F[Open Pull Request]\n    F --&gt; G[Merge after Review]</code></pre> <ul> <li>Goal: Use VS Code to integrate with GitHub and GitHub Copilot, managing the workflow from issue creation to pull request.</li> <li>Activities:<ul> <li>Create issues directly from VS Code using GitHub extension.</li> <li>Create a new branch for the issue.</li> <li>Use Copilot to help write code based on the issue description.</li> <li>Commit and push changes to GitHub.</li> <li>Open a pull request directly from VS Code and review it with the team.</li> </ul> </li> </ul>"},{"location":"company/kartoza/working_in_sprints/#detailed-scrum-process-flow","title":"Detailed Scrum Process Flow","text":"<p>This diagram illustrates the full Scrum process from planning to review, helping visualize how each event fits into the cycle.</p> <pre><code>flowchart TD\n    A[Product Backlog] --&gt; B[Sprint Planning]\n    B --&gt; C[Sprint Backlog]\n    C --&gt; D[Daily Stand-ups] --&gt; E[Sprint Execution]\n    E --&gt; F[Increment]\n    F --&gt; G[Sprint Review]\n    G --&gt; H[Sprint Retrospective]\n    H --&gt; A</code></pre> <ul> <li>Sprint Planning: Set the sprint goal and define the sprint backlog.</li> <li>Daily Stand-ups: Daily check-ins to assess progress and blockers.</li> <li>Sprint Execution: Development work is done to meet the sprint goal.</li> <li>Sprint Review: Review the increment with stakeholders.</li> <li>Sprint Retrospective: Reflect on the sprint and identify improvements for the next cycle.</li> </ul>"},{"location":"company/kartoza/working_in_sprints/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>By the end of the workshop, participants will have hands-on experience with the Scrum framework and GitHub workflows.</li> <li>The team will learn how to streamline their development process using modern tools and focus on continuous improvement.</li> </ul>"},{"location":"development/","title":"Development","text":"<p>Note that all users, regardless of role, should understand and review the security section.</p> <p>Kartoza is a consulting and development firm. In many instances, projects require developers to leverage existing tools and codebases, and to work with other organisations in a way that is consistent in their own conventions.</p> <p>This documentation seeks to outline processes and practices so that internal project development and practices within the team remain consistent. Where it is noted that beneficial conventions listed here are not implemented in client projects, recommendations may be made that clients adopt such standards.</p> <p>For the most part, however, it is less a concern of how a particular outcome is achieved, but rather that the result is consistent with the appropriate conventions.</p> <ul> <li>Documentation</li> <li>Conventions</li> <li>Technologies</li> <li>Environments</li> <li>Resources</li> <li>Testing</li> </ul>"},{"location":"development/developer_practices/","title":"Improving our workflows","text":""},{"location":"development/developer_practices/#dev-team","title":"Dev Team","text":"<ul> <li>Time budget awareness</li> <li>Contngency time</li> <li>Escalating issues when you are hard blocked</li> <li>Need better buffer with clients in terms of delivery planning - let clients know that we work 7 hours a day or need contingency. Need to account for 0.5 day technical debt, internal calls, admin etc. Make use of Marina to communicate this to client.</li> <li>PMs need to ensure that sufficient team members are allocated to meet project timelines</li> <li>Importance of following design</li> <li>Allocating resources to a team who are inefficient derailing</li> <li>Enforcing that clients abide by scope of work</li> <li>Ensure that contracts have terms to limit scope of work to a time budget</li> <li>Figure out how to ignore the urge to add 'extra' features</li> <li>Recognising with your task is going to take longer than planned size</li> <li>How to communicate scope changes about task size </li> <li>Developers should recognise out of scope requests and offer the client the opportunity to create a new contract / contract extension</li> <li>Delivering quality work, not just doing the bare minimum</li> <li>Use your PM to proxy communicate with clients when needed</li> <li>Develop skills in sizing tasks. Always break down tasks into atomic units. You get a psychological advantage of closing off lots of small tasks.</li> <li>Learn to hack your own brain to keep yourself motivated and be able show client regular progress</li> <li>We should match the skills of our team to projects requiring their skills and not assign tasks where they will have to spend too much time learning new technologies not familiar to them.</li> </ul>"},{"location":"development/developer_practices/#gis-team","title":"GIS Team","text":"<ul> <li>Quoting for digitising work: Break the country / region into 30km2 and calculate around 2 days per block. Then add another 2 days for QA per regions.</li> <li>Project kick offs:</li> <li>Drawio meeting where we have a high level but technical project overview discussion and expectations setting</li> <li></li> <li>Where project scope is unclear, there should be a budgeted activity for scope clarification and the scope should be firmed up with the client before starting actual work. Marina should be in the loop on this so that we can hold the client accountable to limit the scope to what we have agreed.</li> <li>Clients should be given parameters for engagement: fixed number of iterations e.g. for review, deadlines for design requirements, clear communication as to when activities have been budgeted for.</li> <li>Plan for learning curves in new technology </li> <li>Build in expert time in projects to swiftly bypass blockers</li> <li>Escalate problems efficiently so that you dont stick on things for days that a team mate could solve in minutes</li> </ul>"},{"location":"development/developer_practices/#training","title":"Training","text":"<ul> <li>Training packages need to be properly setup - Abi needs to review the pack system that Charlie setup </li> <li>Training packages need to be efficient in the training environment e.g. zips already unzipped</li> <li>Onsite training prep pack: cable adapters etc. and also pre training lab check</li> <li>Learner levels should be managed so that we dont mix levels in a group</li> <li>Prefiltering learners with a survey / questionnaire to prepare for what levels learners are</li> <li>Advertising drive at the start of the year</li> <li>SAQA certification we are suckers without SAQA / SAGC / SACE</li> <li>Early bird discounts</li> <li>Post all courses at the start of the year</li> <li>Post training surveys</li> <li>Liven up the training area of the site with interviews and cool media</li> <li>Referral swag or free training for refers</li> <li>Price sensitivity - our prices seem reasonable compared to competitors</li> <li>Mention on the site we have discounts for schools / edu (specify costs)</li> <li>Moodle and platforms like GeoNode training etc need continuous maintenance - test data wiped regularly etc.</li> </ul>"},{"location":"development/developer_practices/#infrastructure","title":"Infrastructure","text":"<ul> <li>GIS infrastructure needs need to be shared with Leon</li> <li>We underutilise our GIS infrastructure and we should look for ways to leverage it more</li> <li>Geoserver and Geocontext are used for clients too, but should we have different infrastructure for clients?</li> <li>sagta map downloader</li> <li>data downloader (by Charlie)</li> <li>staff need to be familiar with our infrastructure to know how to leverage it in work and sales</li> </ul>"},{"location":"development/developer_practices/#devops","title":"Devops","text":"<ul> <li>Level up our SDLC maturity level to allow us to provide more robust solutions</li> <li>Migrate to new infrastrcture Q1 MVP, Q2 Migrations</li> <li>Optimised local environment managed by devops so devs can be productive within a day</li> <li>Operational Areas:</li> <li>Kartoza inhouse systems (should be behind VPN e.g. NC)</li> <li>Kartoza common infrastructre (e.g. postgis.kartoza.com) that can be client facing</li> <li>Developer tools / developer local env</li> <li>Training deploy and scrub platforms</li> <li>Bespoke Client Hosted Applications</li> <li>Generic Client Hosted Applications</li> <li>Kartoza Hardware Inventory</li> <li>Communication plan for clients with the upcoming migrations</li> <li>Communication plan for developers on processes and workflows for SDLC</li> <li>Application catalogue for everything we manage with details of who the client is, what the billing regieme is, who the internal owner is, where the documentation is etc. Workflow for new infrastructure needs to include maintaining the application catalogue. Every application and resource deployed should be tagged with an app id which is tracable back to the app catalogue.</li> <li>Can we use ERP next for the above?</li> <li>Goal to have no orphaned infrastructure</li> <li>Standardising architectures as much as possible</li> <li>Need to be able to specify per-region deployments where customer constraints require it</li> <li>Observability needs to be built into the stack from the ground up (dashboards, alerting, log views)</li> <li>Devops environment needs to be documented from the ground up</li> <li></li> </ul>"},{"location":"development/developer_practices/#management","title":"Management","text":"<ul> <li>Automation of standup history in timesheet app</li> <li>Implementation of planner sheet integrated with ERP Next in timesheet app</li> <li>Burn down charts in timesheet app</li> <li>Strong focus on team leader roles</li> <li>Focus on building our team communication skills (not only in the technical sense but in being proactive communicators)</li> <li>Internal training e.g. Seabilwe presenting basic GIS skills to non GIS staff</li> <li>Succession planning: To ensure that the company does not have reliance on a single person</li> <li>Put in place mechanisms to onramp developers efficiently into our devops stack e.g. developer VMs</li> <li>Proposal development:</li> <li>Internal testing: plan to include testing as a standard and integral part of every project, budgeted as a line item. Testing blowback (developer work that results in lots of failed tests) should reflect in developer KPI's<ul> <li>Functional testing in CI</li> <li>Regression testing in CI</li> <li>Workflow testing in CI (using Playwright)</li> <li>Integration/Deployment testing using DevOps / ArgoCD</li> <li>Manual testing with a test script</li> </ul> </li> <li>Two weeks at end of contract for sign off</li> <li>Client management: we need to ensure that our contracting and project plans manage expectations of clients and that contracts halt until each phase is signed off</li> <li>Proposal development should factor for 7 hour days not 8 in the project plan</li> <li>Look for standardisation in project technology:</li> <li>Devops SDLC</li> <li>Django</li> <li>Postgres / GIS</li> <li>ReactJS</li> <li>MapLibre</li> <li>Vector Tiles</li> <li>Establish spaces for the company to hang out together</li> <li>We need a plan for security:</li> <li>Securing staff computers - policies and communication with staff about best practices - <ul> <li>local disk encryption</li> <li>have I been pwned checks</li> <li>require password managers</li> <li>strong password requirements</li> <li>etc.</li> </ul> </li> <li>Security at network level:<ul> <li>Internal systems (NC, Gdrive, ) requireing YubiKey</li> <li>Move internal systems into VPN</li> </ul> </li> <li>Share incentive scheme in place in 2023</li> <li>Preferential shares eligible for dividends and bonuses</li> <li>Shares vest when the company lists on the stock exchange (not applicable for us?)</li> <li>Establishment of Kartoza.eu and managing impact on Kartoza.za</li> <li>Team meet up</li> <li>Sales handovers - standardize the handover of new work coming in</li> </ul>"},{"location":"development/conventions/","title":"Conventions","text":"<p>Please read and adhere to our coding standards for all of your work.</p> <ul> <li>IDEs</li> <li>Processes</li> <li>Git</li> </ul>"},{"location":"development/conventions/#project-conventions","title":"Project Conventions","text":"<p>SDLC, agile, scrumboards: i.e. project processes</p>"},{"location":"development/conventions/#language-conventions","title":"Language Conventions","text":"<p>A foolish consistency...</p>"},{"location":"development/conventions/#python","title":"Python","text":"<p>python specific content</p>"},{"location":"development/conventions/#javascript","title":"JavaScript","text":"<p>js specific content</p>"},{"location":"development/conventions/#c","title":"C++","text":"<p>c++ specific content</p>"},{"location":"development/conventions/coding_standards/","title":"Python Coding Standards","text":""},{"location":"development/conventions/coding_standards/#general-approach","title":"General Approach","text":"<ul> <li>Use github for revision control, issue tracking and management and use the recommended workflow below whenever possible:</li> <li>Create new Ticket on https://github.com///issues/new <li>Fill in the needed information and description</li> <li>Assign yourself</li> <li>Note ticket number</li> <li>Fork InaSAFE (if not already done)</li> <li>Create new branch called fix-yourticketnumber in your fork</li> <li>Implement feature</li> <li>Pull request your branch into InaSAFE Develop</li> <li> <p>Develop must always pass test and code compliance</p> </li> <li> <p>Adherence to regression/unit testing wherever possible (<code>make test</code>) with a minimum code coverage of 80%.</p> </li> <li>Simple deployment procedure - all dependencies must be delivered with   the plugin installer for QGIS or exist in standard QGIS installs.</li> <li>Develop in the spirit of XP/Agile, i.e. frequent releases, continuous   integration and iterative development. The master branch should always   be assumed to represent a working demo with all tests passing.</li> <li>If a method or function is longer than a single screen, it is probably a   candidate for refactoring into smaller methods / functions. Writing smaller   methods makes your code easier to read and to test.</li> <li>If you use a few lines of code in more than one place, refactor them into   their own function.</li>"},{"location":"development/conventions/coding_standards/#platform-support-for-qgis-plugins","title":"Platform support for QGIS plugins","text":"<p>Currently the following platforms should be supported:</p> <ul> <li>OSX - latest release</li> <li>Linux - Ubuntu current LTS</li> <li>Windows - 10,11</li> </ul>"},{"location":"development/conventions/coding_standards/#compliance","title":"Compliance","text":"<ul> <li>Coding must follow a style guide. In case of Python it is   PEP8 and   using the command line tool pep8 (or <code>make pep8</code>) to enforce this.   The pep8 checks E121-E128 have been disabled until pep8 version 1.3 becomes   widely available.</li> <li>Python documentation guide</li> <li>Code must pass a pylint validation. You can test   this using the make target <code>make pylint</code>. In some cases you may wish to   override a line or group of lines so that they are not validated by pylint.   You can do this by adding either:</li> </ul> <pre><code>     import foo  # pylint: disable=unused-imports\n</code></pre> <p>or:</p> <pre><code>     # pylint: disable=unused-imports\n     import foo\n     print 'hello'\n     print 'goodbye'\n     # pylint: enable=unused-imports\n</code></pre> <p>Note: The use of messages codes (e.g. <code>disable=W1234</code>) should be considered deprecated. Any new exceptions should be added using the keyword format (e.g. <code>disable=unused-exceptions</code>).</p> <p>.. note:: You can globally ignore messages by adding them to :file:<code>pylintrc</code>      in the :samp:<code>[MESSAGES CONTROL]</code> section.</p> <p>The following pylint messages have been thus globally excluded from the check. For a discussion of these see also github issue #245.</p> <ul> <li>All type R: Refactor suggestions such as limiting the number of local                 variables. We may bring some back later.</li> <li>All type I: Information only</li> <li>W0142: Allow the Python feature F(*args, **kwargs)</li> <li>W0201: Allow definition of class attributes outside the constructor.</li> <li>W0212: Allow access to protected members (e.g. _show_system_info)</li> <li>W0231: Allow classes without constructors.</li> <li>W0232: Un-instantiated classes is a feature used in this project.</li> <li>W0403: Relative imports are OK for modules that live in the same dir</li> <li>W0511: Appearance of TODO and FIXME is not a sign of poor quality</li> <li>E1101: Disable check for missing attributes.</li> <li>E1103: This one does not understand numpy variables.</li> <li>C0103: Allow mathematical variables such as x0 or A.</li> <li>C0111: Allow missing docstrings in some cases</li> <li>C0302: No restriction on the number of lines per module</li> </ul> <p>The following pylint check has been removed from Jenkins due to a bug in astroid.   * E1002: Use of super on an old style class.</p> <p>It is of course possible to run all pylint checks on any part of the code   if desired: E.g pylint safe/storage/raster.py</p>"},{"location":"development/conventions/coding_standards/#naming-conventions","title":"Naming conventions","text":"<p>Variable names should as far as possible follow python naming conventions (see Qt Notes below for exceptions to this rule).</p> <p>We reject the idea the code should be obfuscated with hard to understand symbol names. For this reason all classes, methods, functions, variable names should be written in full. At the same time overly verbose names should be avoided. Here is an example of what we mean by this:</p> <p>Bad:</p> <pre><code>    cur_dpth = 0  # obscure\n    currentDepth = 0  # camel case is not python standard\n    content_of_page = 'foo'  # overly verbose\n</code></pre> <p>Good</p> <pre><code>    current_depth = 0\n    page_content = 'foo'\n</code></pre> <p>Avoid 'yoda speak' in variable names. </p> <p>Bad:</p> <pre><code>    title_dialog = self.tr('Save Scenario')\n</code></pre> <p>Good:</p> <pre><code>    dialog_title = self.tr('Save Scenario')\n</code></pre> <p>This is a summary of the naming conventions you should use:</p> <ul> <li>package dir name: concise (preferably single word) lower case, underscore   separated e.g. <code>utilities</code>.</li> <li>module file name: concise (preferably single word) lower case, underscore   separated e.g. <code>utilities.py</code>.</li> <li>class name: Concise singular camel case phrase e.g. <code>PrintDialog</code>.</li> <li>method and function name: Concise lower case underscore separated name   .e.g. <code>remove_entry</code>. Avoid java style get suffixes as it adds no   useful meaning to a symbol name.</li> <li>variable naming: Concise, unabbreviated, lower case, underscore separated   e.g. <code>population_count</code>.</li> </ul>"},{"location":"development/conventions/coding_standards/#code-formatting","title":"Code formatting","text":"<p>The guidelines above still leave substantial room for your own approach to code style so the following provide some more explicit guidelines.</p> <p>We follow a 'pull left' policy in our code. This means that instead of e.g.::</p> <pre><code>    def polygonize_thresholds(raster_file_name,\n                          threshold_min=0.0,\n                          threshold_max=float('inf')):\n</code></pre> <p>You should rather do this:</p> <pre><code>    def polygonize_thresholds(\n        raster_file_name,\n        threshold_min=0.0,\n        threshold_max=float('inf')):\n</code></pre> <p>The same applies in all other contexts. For example, calling a function:</p> <pre><code>    clipped_exposure = clip_layer(\n        layer=exposure_layer,\n        extent=geo_extent,\n        cell_size=cell_size,\n        extra_keywords=extra_exposure_keywords,\n        hard_clip_flag=self.clip_hard)\n</code></pre> <p>We do this because the 80 character line limit in PEP8 can cause visual clutter in your code as you manage line breaks as you run up to the 80 column limit. By always pulling code left as much as possible, we reduce the amount of line continuation management we have to do.</p>"},{"location":"development/conventions/coding_standards/#ordering-of-imports","title":"Ordering of imports","text":"<p>When importing please adhere to the following rules:</p> <p>Do not do <code>*</code> imports e.g. </p> <p><pre><code>from PyQt4.QtGui import *\n</code></pre> is bad.</p> <p>Either import the individual modules you need e.g.</p> <p><pre><code>from PyQt4.QtGui import QProgressDialog\n</code></pre> or import the whole package and use the namespace to reference a module e.g.:</p> <pre><code>    from PyQt4 import QtGui\n\n    progress = QtGui.QProgressDialog()\n</code></pre> <p>Imports should be made in the following order:</p> <ul> <li>core python imports (e.g. <code>import os</code>)</li> <li>third party imports (e.g. <code>from PyQt4 import QtGui</code>)</li> <li>application imports (e.g. <code>from foo import bar</code>)</li> </ul>"},{"location":"development/conventions/coding_standards/#doc-strings-and-comments","title":"Doc strings and comments","text":"<p>All code should be self documenting. Please take special note and follow these PEP guidelines and sphinx documents:</p> <ul> <li>http://www.python.org/dev/peps/pep-0287/</li> <li>http://sphinx-doc.org/markup/desc.html#info-field-lists</li> <li>http://thomas-cokelaer.info/tutorials/sphinx/docstring_python.html</li> </ul> <p>We follow these specific guidelines for our code:</p> <ul> <li>Docstrings must triple quoted with <code>\"\"\"</code></li> <li>Inline comments should start with a # and a single space.</li> <li>Comments should be complete sentences ending with a full stop / period.</li> <li>If a comment is a phrase or sentence, its first word should be capitalized,   unless it is an identifier that begins with a lower case letter (never alter   the case of identifiers!).</li> </ul> <p>We use the following style for documenting functions and class methods:</p> <pre><code>class MyObject(object):\n\n   \"\"\"My new class.\"\"\"\n\n    def set_keyword_db_path(self, path):\n        \"\"\"Set the path for the keyword database (sqlite).\n\n        The file will be used to search for keywords for non local datasets.\n\n        :param path: A valid path to a sqlite database. The database does\n            not need to exist already, but the user should be able to write\n            to the path provided.\n        :type path: str\n\n        :returns: Flag indicating if the path was set successfully.\n        :rtype: boolean\n        \"\"\"\n        self.keyword_db_path = str(path)\n</code></pre> <p>Another example:</p> <pre><code>class MyObject(object):\n\n   \"\"\"My new class.\"\"\"\n\n    def add_layers(scenario_dir, paths):\n        \"\"\"Add the layers described in a scenario file to QGIS.\n\n        :param scenario_dir: Base directory to find path.\n        :type scenario_dir: str\n\n        :param paths: Path of scenario file (or a list of paths).\n        :type paths: str, list\n\n        :raises: Exception, TypeError, FileNotFoundError\n\n        .. note::\n            * Exception - occurs when paths have illegal extension\n            * TypeError - occurs when paths is not string or list\n            * FileNotFoundError - occurs when file not found\n    \"\"\"\n</code></pre> <p>Note the following in the above examples:</p> <ul> <li>The first line of a docstring should be a precis of the class/method/function   expressed in less than 80 chars, terminated with a full stop and exclude   redundant phrases such as 'Class to do x' or 'This method does...'.</li> <li>There should be an empty line following the first docstring line.</li> <li>More detailed explanation and usage examples can follow this first line. The   detailed explanation should not repeat the information provided in the   parameters and returns sections.</li> <li>A line break should follow the optional detailed description.</li> <li>param and type are grouped together with no line break between them.</li> <li>If the param description is more than one line, indent the successive lines   with 4 spaces.</li> <li>A newline should be placed after each type and rtype.</li> <li>If multiple types are allowed, separate them with commas e.g. <code>:rtype: str,   boolean</code>.</li> <li>If a function or method returns nothing, no returns section is used.</li> <li>If a function or method does not raise anything explicitly, no raises section   is used.</li> <li>If a function or method is extremely obvious there is no need to have   anything more than a single line docstring.</li> <li>If a function or method returns a tuple it should be be documented as   <code>:rtype: (&lt;type&gt;, &lt;type&gt;, ..)</code> e.g. <code>:rtype: (int, int)</code>.</li> </ul> <p>Please also see the api documentation how-to section for more information on how to document your code properly.</p>"},{"location":"development/conventions/coding_standards/#annotating-api-changes-and-additions","title":"Annotating API changes and additions","text":"<p>Whenever you add or change a module, class, function or method, you should annotate it accordingly. The method for doing this is described on the <code>Sphinx paragraph markup page &lt;http://sphinx-doc.org/markup/para.html&gt;</code>_. Here are a couple of examples:</p> <p>Adding a new module:</p> <pre><code>    \"\"\"Impact function utilities.\n\n    .. versionadded:: 2.1\n    \"\"'\n</code></pre> <p>Adding a new method to a class:</p> <pre><code>    \"\"\"Computes the number of affected people.\n\n    .. versionadded:: 2.1\n    \"\"\"\n</code></pre> <p>Changing an existing method API:</p> <pre><code>    def show_static_message(self, message, foo):\n    \"\"\"Send a static message to the message viewer.\n\n    .. versionchanged:: 2.1\n        Added foo parameter.\n\n    Static messages cause any previous content in the MessageViewer to be\n    replaced with new content.\n\n    :param message: An instance of our rich message class.\n    :type message: Message\n\n    :param foo: Some new parameter.\n    :type foo: str\n\n    \"\"\"\n    dispatcher.send(\n        signal=STATIC_MESSAGE_SIGNAL,\n        sender=self,\n        message=message)\n</code></pre>"},{"location":"development/conventions/coding_standards/#strings-and-internationalisation","title":"Strings and internationalisation","text":"<ul> <li>Simple strings in source code should be quoted with <code>'</code></li> <li>Favour interpolation over concatenation. For example this is bad:</li> </ul> <pre><code>    world = 'World'\n    foo = 'Hello ' + world\n</code></pre> <p>And this is good:</p> <pre><code>    world = 'World'\n    food = 'Hello %s' % world\n</code></pre> <ul> <li>Use parenthesis for long strings. For example this is bad:</li> </ul> <pre><code>    foo = 'The quick brown fox jumps over the lazy dog. ' +\n          'The slow fat rat runs around the mouldy cheese.'\n</code></pre> <p>And this is good:</p> <pre><code>    bar = (\n        'The quick brown fox jumps over the lazy dog. '\n        'The slow fat rat runs around the mouldy cheese.')\n</code></pre> <p>Note: The good example above follows the 'pull left' principle.</p> <ul> <li>All strings should be internationalisation enabled. Please see [i18n]   for details.</li> <li> <p>When using gettext, alias the uggettext as tr, and do not use the common   convention of <code>_('foo')</code> as the underscore trips up some tools like pylint,   sphinx. Also using <code>tr</code> makes it easy to migrate code to and from Qt's   translation system and gettext. Note: gettext use is deprecated in InaSAFE.</p> </li> <li> <p>If you use a literal string or expression in more than one place, refactor   it into a function or variable.</p> </li> </ul>"},{"location":"development/conventions/coding_standards/#standard-headers","title":"Standard headers","text":"<p>Each source file should include a standard header containing copyright, authorship and version metadata as shown in the exampled below.</p> <p>Example standard header</p> <pre><code># -*- coding: utf-8 -*-\n\"\"\"One line description.\n\n.. tip::\n   Detailed multi-paragraph description...\n\"\"\"\n\nimport os  # python core imports first\nimport qgis.core  # then external imports\nimport safe.utils.gis  # then project imports (always using full path)\n\n__copyright__ = \"Copyright 2016, The InaSAFE Project\"\n__license__ = \"GPL version 3\"\n__email__ = \"info@inasafe.org\"\n__revision__ = '$Format:%H$'\n</code></pre> <p>Note:: Please see [faq_developer] for details on how the revision tag is replaced with the SHA1 for the file when the release packages are made.</p>"},{"location":"development/conventions/coding_standards/#qt-guidelines","title":"Qt Guidelines","text":"<p>Compile UI files at run time. There is no need to precompile UI files using pyuic4. Rather you can dynamically compile them using this technique (see technical docs here:</p> <pre><code>    import os\n    from PyQt4 import QtGui, uic\n\n    BASE_CLASS = uic.loadUiType(os.path.join(\n        os.path.dirname(__file__), 'foo_dialog_base.ui'))[0]\n\n\n    class FooDialog(QtGui.QDialog, BASE_CLASS):\n\n\"\"\"Dialog for defining the plugin properties.\n\n        \"\"\"\n        def __init__(self, parent=None):\n            \"\"\"Constructor.\"\"\"\n            super(FooDialog, self).__init__(parent)\n            # Set up the user interface from Designer.\n            self.setupUi(self)\n</code></pre> <p>Don't use old style signal/slot connectors:</p> <pre><code>    QtCore.QObject.connect(\n        self.help_button, QtCore.SIGNAL('clicked()'), self.show_help)\n</code></pre> <p>Use new style connectors::</p> <pre><code>    self.help_button.clicked.connect(self.show_help)\n</code></pre> <p>Use multi-inheritance for designer based classes so that we can use autoconnect slots.:</p> <pre><code>    class FooDialog(QtGui.QDialog, Ui_FooBase):\n        \"\"\"Dialog to prompt for widget names.\"\"\"\n\n        def __init__(self, parent=None):\n            \"\"\"Constructor for the dialog.\n\n            This dialog will allow the user to select foo names from  a list.\n\n            :param parent: Optional widget to use as parent\n            :type parent: QWidget\n            \"\"\"\n            QtGui.QDialog.__init__(self, parent)\n            # Set up the user interface from Designer.\n            self.setupUi(self)\n            # ... further implementation here ...\n</code></pre> <p>Then we can do this to listen for a click on button bar:</p> <pre><code>    def on_bar_clicked(self):\n        \"\"\"Auto slot to listen for button click.\"\"\"\n        pass\n</code></pre> <p>The callback above is called when the button is clicked simply by virtue of the fact that it uses the naming convention <code>on_&lt;object&gt;_clicked</code>.</p> <p>Note that in some cases you need to explicitly specify which signature is being listened for by using the pyqtSignature decorator:</p> <pre><code>    @pyqtSignature('int')\n    def on_polygon_layers_combo_currentIndexChanged(self, theIndex=None):\n        \"\"\"Automatic slot executed when the layer is changed to update fields.\n\n        :param theIndex: Passed by the signal that triggers this slot.\n        :type theIndex: int\n        \"\"\"\n        layerId = self.polygon_layers_combo.itemData(\n            theIndex, QtCore.Qt.UserRole)\n        return layer_id\n</code></pre> <p>Failure to do this may result in the slot being called multiple times per event which is usually undesirable.</p> <p>Also in some cases using the Qt API will lead you into conflict with our PEP8 naming conventions for methods and variables. This is unavoidable but should be used only in these specific instances e.g.:</p> <pre><code>    def on_foo_indexChanged():\n        pass\n</code></pre> <p>Qt's naming convention causes a bit of a clash when using with 'normal' python underscore names. For this reason we adopt the following strategy:</p> <ul> <li>in designer use underscore based naming for objects</li> <li>in your concrete implementations you should be able to then use mostly   underscore separated names except in cases where using autoconnect slots.</li> <li>in designer you should call the form a name ending in Base e.g.   <code>FooDialogBase</code>. By convention the concrete implementation is called the   same sans the Base suffix e.g. <code>FooDialog</code>.</li> </ul>"},{"location":"development/conventions/coding_standards/#code-statistics","title":"Code statistics","text":"<ul> <li>https://www.ohloh.net/p/inasafe/analyses/latest</li> <li>https://github.com/AIFDR/inasafe/network</li> <li>https://github.com/AIFDR/inasafe/graphs</li> </ul>"},{"location":"development/conventions/coding_standards/#working-with-git","title":"Working with GIT","text":"<ul> <li>Additions to the develop branch should be made via the GitHub pull request mechanism</li> <li>Pull requests should preferably be squashed into a single commit before applying (see http://eli.thegreenplace.net/2014/02/19/squashing-github-pull-requests-into-a-single-commit)</li> <li>Commits and pull requests should reference the issue number they close or contribute to</li> </ul>"},{"location":"development/conventions/coding_standards/#landscape","title":"Landscape","text":"<pre><code># coding=utf-8-\n\n\"\"\"One line description with full stop. New line after file-docstring.\"\"\"\n\nimport os\nimport qgis.core\nimport safe.utils.gis\n\n__copyright__ = \"Copyright 2016, The InaSAFE Project\"\n__license__ = \"GPL version 3\"\n__email__ = \"info@inasafe.org\"\n__revision__ = '$Format:%H$'\n\nclass MyObject(object):\n\n    \"\"\"Class docstring with one new line before and after and full stop.\"\"\"\n\n    def my_function(self, foo):\n    \"\"\"One line descriptive sentence with full stop and a new line between each block.\n\n    Extra information goes here.\n\n    :param foo: Text with full stop.\n    :type foo: object\n\n    :return: It returns something.\n    :rtype: basestring\n    \"\"\"\n    i = 0  # No new line after method/function docstring.\n    return i\n</code></pre>"},{"location":"development/conventions/dev_processes/","title":"Development Processes","text":"<p>Development processes</p> <p>https://kartoza.com/en/coding-standards/ https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html</p>"},{"location":"development/conventions/git/","title":"Git","text":"<ol> <li>Tell everyone to use GitLens. It's amazing</li> <li>Outline other standard processes and tools (precommits, hooks, workflows, gitflows etc)</li> </ol>"},{"location":"development/conventions/ides/","title":"IDEs","text":"<p>This section covers Integrated Development Environments. Whilst it is not required that all users leverage the same IDE, it is beneficial to have a consistent environment across the team and ensure all team members have access to the appropriate tools.</p> <ul> <li>VSCode (almost anything)</li> <li>Pycharm</li> <li>QTCreator</li> </ul>"},{"location":"development/conventions/ides/#configuration","title":"Configuration","text":"<p>Recommended and required configurations should be listed here</p> <p>General opinions can be handed off to Environment Links</p>"},{"location":"development/conventions/ides/#vscode","title":"VSCode","text":"<ul> <li>GitLens (recommended): Really great UX and insights into git</li> <li>GitMoji (recommended): Don't forget to use emoji in your commit messages</li> </ul>"},{"location":"development/conventions/project_processes/","title":"Project Processes","text":"<p>Make GitHub projects (in the project repo or kartoza organisation?)</p> <p>Keep those projects up to date</p> <pre><code>graph LR\n    A[Check for existing task]--&gt;B[Update task state];\n    B--&gt;C[Perform work on task];\n    C--&gt;D[Commit changes];\n    D--&gt;B;</code></pre>"},{"location":"development/documentation/","title":"Project Documentation","text":"<p>The subdirectories within this folder contain the generic boiler plate files required for project documentation.</p> <p>Copy the template files and rename them as needed (e.g. <code>mkdocs-base-template.yml</code> would be copied and become <code>mkdocs-base.yml</code>).</p> <p>The <code>repo-template</code> folder represents the start of a blank repository.</p> <p>Edit the <code>nav</code> section of the <code>mkdocs-base-template.yml</code> to have the correct relevant pathways.</p>"},{"location":"development/environments/","title":"Environments","text":"<p>Development environments configurations and conventions (conda/ venv/ poetry/ git codespaces/ docker workspaces etc)</p> <p>Links and external resources</p>"},{"location":"development/environments/links/","title":"Development Resources","text":"<p>External resources, links, and recommendations fo development environment configuration.</p>"},{"location":"development/environments/links/#vscode","title":"VSCode","text":"<p>see links</p>"},{"location":"development/environments/links/#sql","title":"SQL","text":"<p>Use postgresql for everything. Dbeaver and Pgadmin are good tools. https://www.beekeeperstudio.io/</p>"},{"location":"development/environments/mapstore/windows_machine_setup/","title":"Getting Started","text":"<p>IDE used is VSCode</p>"},{"location":"development/environments/mapstore/windows_machine_setup/#important-links","title":"Important links","text":"<ul> <li>Mapstore Home</li> <li>Developer documentation</li> <li>User Guide</li> <li>Github link</li> </ul> <p>Mapstore requires two disitinct set of tools: Apache Maven for Java and NPM for JavaScript</p>"},{"location":"development/environments/mapstore/windows_machine_setup/#installing-maven-for-windows","title":"Installing Maven for Windows","text":""},{"location":"development/environments/mapstore/windows_machine_setup/#1maven-download","title":"1.Maven Download","text":"<ol> <li>Download Maven link</li> <li>Extract the downloaded file to a folder that is accesible</li> </ol>"},{"location":"development/environments/mapstore/windows_machine_setup/#2add-maven_home-system-variable","title":"2.Add MAVEN_HOME System Variable","text":"<ol> <li>Open System Enviroment Variables(this needs to be opened in admin mode)</li> <li>Click on the button \"Enviroment Variables...\"</li> <li>Add a new user variable called MAVEN_HOME</li> <li>Edit the \"Path\" variable under System variables</li> <li>Enter %MAVEN_HOME%\\bin in the new field. CLick OK to save changes to the variable</li> <li>Open the command prompt and type mvn -version to make sure installtion was succesful</li> </ol> <p>Make sure that JAVA_HOME system variable is configured properly and pointing to the version of java you are using Java files can usually be found at C:\\Program Files\\Java</p>"},{"location":"development/environments/mapstore/windows_machine_setup/#installing-nodejs-and-npm","title":"Installing Node.js and NPM","text":"<ol> <li>Download Node.js from the following link</li> <li>Follow installer instructions</li> </ol>"},{"location":"development/environments/mapstore/windows_machine_setup/#quick-setup-and-run-of-mapstore","title":"Quick Setup and run of MapStore","text":"<ul> <li>Clone the repository to your current working folder   git clone https://github.com/geosolutions-it/MapStore2.git in the terminal</li> <li>Run npm cache clean in the terminal</li> <li>Run npm install in the terminal</li> </ul> <p>Note: If you're struggling to run the local backend instance use the online instance of MapStore as the backend.</p>"},{"location":"development/environments/mapstore/windows_machine_setup/#swap-out-local-instance-of-backend-for-online-instance","title":"Swap out local instance of backend for online instance","text":"<ol> <li>Open the following file MapStore2/build/DevServer.js</li> <li>Set the variable MAPSTORE_BACKEND_URL to the url https://dev-mapstore.geosolutionsgroup.com/mapstore</li> </ol> <p>Changing the MAPSTORE_BACKEND_URL in console using &gt;set MAPSTORE_BACKEND_URL=https://dev-mapstore.geosolutionsgroup.com/mapstore did not work on my machine</p>"},{"location":"development/environments/mapstore/windows_machine_setup/#start-front-end","title":"Start front end","text":"<ul> <li>Run npm run fe:start in the terminal to start the app. The app does take some time to install, please be patient</li> </ul>"},{"location":"development/environments/mapstore/windows_machine_setup/#running-the-back-end","title":"Running the back end","text":"<p>Please note the backend works with jdk version 1.8. Any version higher than that and the build fails - Run backend with npm run be:start</p>"},{"location":"development/environments/mapstore/windows_machine_setup/#build-a-war-file","title":"Build a .war file","text":"<p>When running ./build.sh the following error appears \"Unable to move the cache: Access is denied\" The build does not continue after this point</p> <p>When running the command mvn compile war:war -e* the following error appears: Error assembling WAR: webxml attribute is required (or pre-existing WEB-INF/web.xml if executing in update mode) * - To try and remedy this error I added the following in the product/pom.xml  - <code>&lt;plugin&gt;             &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;             &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt;             &lt;extensions&gt;false&lt;/extensions&gt;             &lt;version&gt;2.1.1&lt;/version&gt;             &lt;configuration&gt;                 &lt;overlays&gt;                     &lt;overlay&gt;                         &lt;groupId&gt;it.geosolutions.mapstore&lt;/groupId&gt;                         &lt;artifactId&gt;mapstore-webapp&lt;/artifactId&gt;                         **&lt;failOnMissingWebXml&gt;false&lt;/failOnMissingWebXml&gt;**                        &lt;/overlay&gt;                 &lt;/overlays&gt;             &lt;/configuration&gt;         &lt;/plugin&gt;</code> - This not did fix the error. Still investigating a fix</p>"},{"location":"development/environments/vscode/extension_install/","title":"Bulk Extension Installation","text":"<p>If you want to install VSCode extensions suggested by Rudolf, you can follow these steps.</p> <ol> <li>Open this file.</li> <li>Copy the contents.</li> <li>Open VSCode.</li> <li>Open a new terminal</li> <li>Paste the file content in the terminal.</li> </ol> <p>This should install all the extensions (can take a while). This setup is a bit general case, with a heavy focus on python and django naturally.</p>"},{"location":"development/environments/vscode/links/","title":"VSCode Resources","text":"<p>External resources, links, and recommendations fo VSCode  configuration.</p> <p>Suggestions from Rudolph, 2022/04</p> <ol> <li>Lightweight postman alternative inside vsc    https://marketplace.visualstudio.com/items?itemName=humao.rest-client</li> <li>Manage all your unit tests in a neat organised side bar in vsc - check docs for install guide. Also works with basically all test frameworks!    https://marketplace.visualstudio.com/items?itemName=hbenl.vscode-test-explorer    https://marketplace.visualstudio.com/items?itemName=ms-vscode.test-adapter-converter    https://marketplace.visualstudio.com/items?itemName=LittleFoxTeam.vscode-python-test-adapter</li> <li>Track how much time you spend on all your projects. also tracks lines of code added/deleted/etc    https://marketplace.visualstudio.com/items?itemName=iceworks-team.iceworks-time-master</li> <li>Docker management    https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-docker    https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers</li> <li>Django snippets - this extension is a bit hidden away for some reason. This is really awesome for django dev specific snippets for models, forms, views etc. really useful!    https://marketplace.visualstudio.com/items?itemName=thebarkman.vscode-djaneiro</li> <li>Python Docstrings - but I'm sure there are better extensions     https://marketplace.visualstudio.com/items?itemName=njpwerner.autodocstring    I have a few other quality of life extensions:    https://marketplace.visualstudio.com/items?itemName=vscode-icons-team.vscode-icons - essential    https://marketplace.visualstudio.com/items?itemName=adam-watters.vscode-color-pick - front end devs looking for colour codes    https://marketplace.visualstudio.com/items?itemName=johnpapa.vscode-peacock - livens up vsc theme with a splash of colour    https://marketplace.visualstudio.com/items?itemName=formulahendry.auto-rename-tag - html auto rename the closing tag    https://marketplace.visualstudio.com/items?itemName=formulahendry.auto-close-tag - creates closing tag (helpful when using html in not html files like Vue, but might be unnecessary lol)    Honorary mentions:    https://marketplace.visualstudio.com/items?itemName=donjayamanne.python-extension-pack    https://marketplace.visualstudio.com/items?itemName=alefragnani.project-manager    https://marketplace.visualstudio.com/items?itemName=esbenp.prettier-vscode    https://marketplace.visualstudio.com/items?itemName=donjayamanne.git-extension-pack    https://marketplace.visualstudio.com/items?itemName=uctakeoff.vscode-counter    -All the intellisense extensions for all the languages :)    A lot of these extensions have alternatives as well so it's recommended to play around with a few of them.</li> </ol>"},{"location":"development/resources/","title":"GIS Resources","text":"<p>Where to find internal resource collections (projects, templates, data, assets etc)</p> <p>External resource links:</p> Resource About Google Cloud Platform Google Cloud consists of a set of physical assets, such as computers and hard disk drives, and virtual resources, such as virtual machines (VMs), that are contained in Google's data centers around the globe."},{"location":"development/resources/gcp/","title":"Google cloud platform guide","text":"<p>A guide for Google cloud platform (GCP).</p>"},{"location":"development/resources/gcp/#apis-and-services","title":"APIs and services","text":"<p>When using working with GCP, the user will need to enable the APIs required for the features they will be using. Here is a list of APIs the user will need to activate for storage, workflows, dataflows, and triggers.</p> <ul> <li>Cloud:</li> <li>Cloud Pub/Sub API</li> <li>Cloud storage</li> <li>Cloud logging API</li> <li>Cloud storage JSON API</li> <li>BigQuery API</li> <li>Cloud datastore API</li> <li>Compute Engine API</li> <li>Workflow:</li> <li>Workflow API</li> <li>Workflow executions API</li> <li>Eventarc API</li> <li>Cloud scheduler API</li> <li>Dataflow:</li> <li>Dataflow API</li> </ul> <p>This does not mean the user needs to make use of all these APIs, it only points out important cases</p> <p>To activate any of the above, do the following: - On the dashboard, click on APIs and services</p> <p></p> <ul> <li>Select Library. This will take the user to a library which contains the available APIs</li> <li>The simplest method, if the API name is known, is to search for the API in the search bar</li> <li>For this guide, type \u201cCloud pub/sub api\u201d in the search bar and press Enter</li> </ul> <p></p> <ul> <li>Select the Cloud Pub/Sub API result</li> <li>Click Enable</li> <li>Do this for all required APIs and services</li> </ul> <p>If an API is required for a function by GCP which has not been activated, the user will be prompted to enable it.</p>"},{"location":"development/resources/gcp/#cloud-storage","title":"Cloud storage","text":""},{"location":"development/resources/gcp/#create-a-new-bucket","title":"Create a new bucket","text":"<p>How to create a Storage bucket: - Go to Cloud Storage and click on Browser</p> <p></p> <ul> <li>Click Create bucket</li> </ul> <p></p> <ul> <li>Provide a unique name for the bucket and click Continue</li> <li>Choose a region of your choice and click Continue</li> </ul> <p>Using a single Region type will be the most cost-effective option. Only use Multi-region when required.</p> <ul> <li>The user can decide on the storage type, storage class, access control and data protection which will suffice for their requirements</li> </ul> <p>Default parameters/options should be sufficient for most cases. It is also important to consider the different storage classes, as this can affect the cost.</p> Storage class Minimum storage duration (days) Retrieval fees (per GB) Standard storage None None Nearline storage 30 $0.01 Coldline storage 90 $0.02 Archive storage 365 $0.05 <ul> <li>Click Create</li> </ul> <p>For more information on Cloud Storage and buckets, see https://cloud.google.com/storage/docs/json_api/v1/buckets.</p>"},{"location":"development/resources/gcp/#load-data-into-a-bucket","title":"Load data into a bucket","text":"<ul> <li>Select the bucket to which data will be added</li> <li>Click on Create folder</li> <li>Open the newly created folder</li> <li>Click Upload files</li> <li>Select the file which needs to be stored in the bucket, and click Open. The file will be uploaded to the bucket</li> </ul> <ul> <li>The file can be renamed as follows:</li> <li>To the right, open the object overflow menu (button with three dots)</li> <li>Select Rename</li> <li>Rename the file as desired</li> <li>A file can be deleted from the bucket by selecting it (tick next to the file) and clicking on Delete</li> </ul>"},{"location":"development/resources/gcp/#workflow","title":"Workflow","text":"<p>This sections deals with creating a workflow in GCP.</p> <ul> <li>In the GCP menu, click on Workflows</li> </ul> <p></p> <ul> <li>Click Create to start setting up a workflow</li> </ul> <p></p> <ul> <li>Provide a workflow name, description (optional) and region</li> </ul> <p>Using a single region option is a more cost-effective approach. Only use multi-regional if required.</p> <ul> <li>Select a service account. Do the following if you want to create a new service account:</li> <li>Click on Create a new service account</li> <li>Provide an account name, ID and description (optional). The user can also generate a random ID.</li> <li>Click on Create and Continue</li> <li>Now select the newly created service account</li> </ul> <p></p> <p>Description parameters are optional, but it is highly recommended to provide a good, but short description. This helps a lot if multiple users will be working on the GCP project.</p> <ul> <li>The user can also provide a label to link this workflow to a group. This is optional.</li> <li>Click on Add label</li> <li>Provide a Key and Value</li> <li>GCP provides two trigger types:</li> </ul> <p></p> <ul> <li>Choose either type:</li> <li>Cloud scheduler: This is a recurring schedule option<ul> <li>Click on Add new trigger and select Cloud scheduler</li> <li>Provide the following:</li> <li>A name, region, and time zone</li> <li>The frequency needs to be set as explained here: https://cloud.google.com/scheduler/docs/configuring/cron-job-schedules#defining_the_job_schedule</li> <li>Click Continue</li> <li>Set the workflow argument, log level and the service account as desired</li> <li>Click Create</li> </ul> </li> <li>Eventarc: This is an event driven option (e.g. file uploaded to a bucket)<ul> <li>Click on Add new trigger and select Eventarc</li> <li>Set the Trigger name as desired</li> <li>Choose an Event provider. Depending on the chosen provider, the parameters which follows may differ. For this example/guide Cloud Storage will be used</li> <li>Under Event, the user will note that there are numerous options, but for now we will use google.cloud.storage.object.v1.finalized \u2013 This event occurs when a new object has been created in a bucket</li> <li>To select the bucket which should be monitored, click on Browse. Then select the desired bucket, or create a new bucket. Click Select</li> <li>Choose the Service account</li> <li>Grant permission as required for the chosen Region and Service account. If permissions are already set up, this option will not show to the user</li> <li>Click Save trigger</li> </ul> </li> <li>Set up the workflow which needs to be performed. See the following for more information on this: https://cloud.google.com/workflows/docs/reference/syntax</li> <li>Click Deploy</li> <li>If the workflow has been successfully deployed, the user can perform the workflow. Click on Execute to go to the execute workflow section</li> </ul> <p></p> <ul> <li>Click Execute</li> <li>Here is an example of an output from a Workflow</li> </ul> <p></p>"},{"location":"development/resources/gcp/#dataflow","title":"Dataflow","text":"<p>The GCP dataflow can be split into three main categories, namely Java, Python and notebooks. Essentially the outcome and aim of each category will be the same. For this guide focus will be on Python.</p> <p>To enable dataflow developing for an account, add the Dataflow developer role to the service account.</p> <p>Here is a short tutorial on creating a Dataflow:</p> <ul> <li>Let's first create a text file which will contain some words for our example:</li> <li>Create a new text file at a desired directory on your computer. The text file name can be anything (e.g. \u201cjust_some_text.txt\u201d);</li> <li>Add some words to the text file. Which words you add does not matter, but here is an example:</li> </ul> <p></p> <ul> <li>Upload the new file:</li> <li>Open Cloud storage</li> <li>Open an existing bucket, or create a new bucket</li> <li>Click Upload files</li> <li>Select the newly created text file and click OK</li> <li>Create a tempory storage folder for Dataflow:</li> <li>Click on Create folder and name it \"temp\"</li> <li>On the dashboard, click on Dataflow</li> </ul> <p></p> <ul> <li>Click on Create job from template</li> </ul> <p>A user can also create their own template, but for this example we will not be doing that.</p> <p></p> <ul> <li>Provide a Job name</li> <li>Choose a Regional endpoint</li> </ul> <p>The region will be best to be the same as your cloud storage. This will minimize costs.</p> <ul> <li>Set the Dataflow template to Word count. The Word count dataflow will execute code on a provided text file and return the number of cases the file contains each word</li> <li>To set directories the syntax will be \u201cgs://{bucket}/{folder}/{filename}\u201d. For example, \u201cgs://newb2/just_some_text.txt\u201d will refer to the text file named \u2018just_some_text.txt\u2019 in the bucket named \u2018newb2\u2019. These directories can also refer to folders in a bucket. Set the directories as follows:</li> <li>Input file(s) in Cloud storage: The text file which contains the words (e.g. \u201cgs://newb2/just_some_text.txt\u201d). This will be the input for the dataflow</li> <li>Output Cloud Storage file prefix: This is the prefix which will be used for the output file (\u201cgs://newb2/just_some_text_result.txt\u201d). The counts for each word will be stored in this file</li> <li>Temporary location: Provide the temporary folder we created here (\u201cgs://newb2/temp\u201d). The dataflow will use this folder to store any temporary data used for processing</li> <li>Here is an example of the parameters:</li> </ul> <p></p> <ul> <li>Click Run job</li> <li>The job will run for a few minutes. The output should be similar to the following:</li> </ul> <p></p> <ul> <li>Once the dataflow finished running, go back to your bucket. There should be a new file, based on the name you have provided, in your bucket. In this example, the file is named \u201cjust_some_text_result.txt-00000-000001\u201d</li> </ul> <p></p> <ul> <li>Download the file</li> <li>The user should see something similar to the following:</li> </ul> <p></p> <p>This concludes the basic example for Dataflows.</p>"},{"location":"development/resources/gcp/#bigquery","title":"BigQuery","text":""},{"location":"development/resources/gcp/#create-a-new-table","title":"Create a new table","text":"<ul> <li>Click the View actions button for the project</li> <li>Click Create data set</li> </ul> <ul> <li>Select the Project ID</li> <li>Provide a Data set ID</li> <li>Select a Data location (region)</li> </ul> <p>For cost-effectiveness, make use a single zone region. Multi-region is more costly.</p> <ul> <li>Click Create data set</li> </ul> <p></p> <ul> <li>Click View actions for the newly created data set;</li> <li>Click Create table;</li> </ul> <p></p> <ul> <li>Provide the required parameters and click on Create table</li> </ul> <p></p> <p>A dataset can contain multiple tables.</p>"},{"location":"development/resources/gcp/#table-schema","title":"Table schema","text":"<p>The schema for a table contains all of the fields, field types, size, description, and other parameters based on the field type. To add or edit a Schema to the table, do the following:</p> <ul> <li>Click on the table so that it opens in a new tab in GCP</li> <li>Click Edit schema</li> </ul> <p></p> <ul> <li>Click the Add field button</li> <li>Provide a Field name, Type, Mode and Description. Some schema options might be optional (e.g. Description). The parameters will also differ depending on the Type;</li> <li>The user can add several fields;</li> </ul> <p></p> <ul> <li>Click Save to update the Schema</li> </ul>"},{"location":"development/resources/gcp/#notes","title":"Notes","text":"<p>Some issues the user should avoid or may encounter:</p> <ul> <li>BigQuery will not allow loading of a table with the incorrect number of columns:</li> <li>Deleting unwanted fields will solve this problem</li> <li>Always base the schema on the table being loaded, but this can introduce other issues</li> <li>The names of each field needs to be the same. So the user needs to check the data prior to uploading it. This will also make it easier to automatically delete unwanted fields</li> <li>The data type needs to be the same for all features/rows of the table. A string cannot be imported into a float fields type</li> <li>Found an issue where the number of columns changes from a certain row (e.g. row 81), therefore the data cannot be loaded. These were resolved by \u2018just deleting\u2019 a bunch of columns at the end of the file. Could not find any data in those columns, so not exactly sure what caused to problem (maybe spaces or tabs in the table elements)</li> <li>Most of these issues can be solved prior to uploading data, or in the Python code itself when a trigger event occurs</li> <li>Issues when importing a date column. BigQuery needs the data to be perfect for improting.</li> </ul>"},{"location":"development/resources/gcp/#python","title":"Python","text":"<p>Python can be used for trigger events for Storage bucket or Dataflow pipelines. Other languages, namely Java, are also supported.</p>"},{"location":"development/resources/gcp/#locally-run-python-code","title":"Locally run python code","text":"<p>Python code can be run locally before running it on the GCP. Makes it easier to test code before uploading it. Firstly the user needs to create a service account JSON file, followed by setting up environmental variables in their OS and installing a few libraries/modules.</p>"},{"location":"development/resources/gcp/#service-account-key","title":"Service account key","text":"<p>The service account key will be used by the user\u2019s OS to access the GCP account. The JSON key can be created as follows:</p> <ul> <li>Go to IAM and admin</li> <li>Select Service account</li> <li>Select the service account for which you want to create the JSON key</li> <li>Click on the Keys tab</li> <li>Click Add key and then Create new key</li> <li>Select JSON and click Create</li> <li>The new key will be created and the browser will download the JSON file</li> </ul> <p>If the download fails, try using a different browser as some browsers causes issues.</p>"},{"location":"development/resources/gcp/#environmental-variables","title":"Environmental variables","text":"<p>Windows environmental variables needs to be set up as follows: - Type \u2018environmental variables\u2019 in the Windows start menu - Select Edit the system environmental variables - Under System variables, click New</p> <p></p> <ul> <li>Set the variables as follows:</li> <li>Service account JSON key:<ul> <li>Variable name: GOOGLE_APPLICATION_CREDENTIALS; and</li> <li>Variable value: Directory with filename to the JSON key file.</li> </ul> </li> <li>GCP project:<ul> <li>Variable name: GOOGLE_CLOUD_PROJECT; and</li> <li>Variable value: Project ID.</li> </ul> </li> </ul> <p></p> <ul> <li>Click OK</li> <li>The new environmenal variables will now be added</li> </ul> <p>On Linux the user can do the following to set up the environmental variables: - Open the terminal - Service account JSON key:   - Run the following: export GOOGLE_APPLICATION_CREDENTIALS=\"KEY_PATH\", where KEY_PATH is the directory to the JSON key file - GCP project:   - Run the following: export GOOGLE_CLOUD_PROJECT=\"PROJECT\", where PROJECT is the ID of the project.</p> <p>Accessing the GCP using Python code should now work.</p>"},{"location":"development/resources/gcp/#python-modules-and-libraries-to-install","title":"Python modules and libraries to install","text":"<p>Required roles: - Dataflow admin; - Dataflow developer; - Dataflow worker; and - Service account user.</p> <p>Install pip, if not installed: - sudo apt update - sudo apt install python3-pip</p> <p>The following Python modules will be required: - pip install google-cloud-storage; - pip install google-cloud-bigquery; and - pip install google-cloud.</p> <p>These modules should be installed locally if the user wants to perform Python runs locally, but also on the GCP terminal when code should be run on the platform as well. To check the list of installed Python modules, use \u2018pip list\u2019.</p> <p>The following libraries should be installed for local Python runs: - gcloud command:   - sudo snap install google-cloud-cli; and   - sudo snap install google-cloud-sdk.</p> <p>If the user will be making use of Apache-beam (Data flow pipeline), do the following to be able to run code locally: - Open the console - pip install wheel - Installing Apache-beam differs for Windows and Linux:   - Windows: pip install \u201capache-beam[gcp]\u201d   - Linux: pip install \u2018apache-beam[gcp]\u2019</p> <p>Take note that Windows uses a souble-quote, Linux uses a single-quote.</p>"},{"location":"development/resources/gcp/#running-the-code-locally","title":"Running the code locally","text":"<p>Before adding code to GCP (e.g. for a trigger), it will be best to test the code locally as it can be slow to upload or deploy the code. Simply run the Python code using the normal console command: - python3 python_file.py</p>"},{"location":"development/resources/gcp/#bucket-trigger-events","title":"Bucket trigger events","text":"<p>A trigger event can be set up for a Storage bucket. Depending on the event (e.g. object.finalize will occur when a new file finished uploading to the bucket), code can be triggered. This guide will only explain how to do this for Python, but it can also be set up for other languages (e.g. Java). Add required roles: - cloudfunctions.functions.get - cloudfunctions.functions.create - cloudfunctions.functions.delete - cloudfunctions.functions.call - Any other required roles</p>"},{"location":"development/resources/gcp/#trigger-event-for-python","title":"Trigger event for Python","text":"<p>A trigger event for Python can be set up as follows: - main.py: This Python file will contain the functions which will be triggered when an event occurs. The file has to be named main.py. - requirements.txt: This file will consist of any modules which will be imported and used by the code. Here is two examples of requirements added to the text file:   - google.cloud.storage   - google.cloud.bigquery</p> <p></p> <ul> <li>The code can be deployed by going to Cloud functions in GCP;</li> <li> <p>The code can also be deployed on the GCP to the bucket using the following command: gcloud functions deploy FUNCTION_NAME \\ --runtime python39 \\ --trigger-resource BUCKET_NAME \\ --trigger-event TRIGGER_EVENT \\ --project PROJECT_ID</p> </li> <li> <p>The user can manually upload the data to the bucket or use the following command: gsutil cp FILE_NAME BUCKET_DIR</p> </li> <li>To check the logs of the trigger, do the following command: gcloud functions logs read --limit 50 --project PROJECT_ID</li> <li>Variable descriptions:</li> <li>FUNCTION_NAME: The name of the method/function in the main.py file;</li> <li>BUCKET_NAME: The name of the bucket which will trigger the event. This should only be the bucket name (e.g. example_bucket);</li> <li>TRIGGER_EVENT: The event which will trigger the code (e.g. google.storage.object.finalize will trigger when a file finished uploading);</li> <li>PROJECT_ID: The ID of the project (not the name of the project)</li> <li>FILE_NAME: The local file the user wants to upload; and</li> <li>BUCKET_DIR: The bucket directory on GCP. This will consist of \u2018gs://\u2019 and the BUCKET_NAME (e.g. gs://example_bucket)</li> </ul>"},{"location":"development/resources/gcp/#pubsub-trigger-events","title":"Pub/Sub trigger events","text":"<p>This section focuses on setting up a scheduled trigger event using Pub/Sub, Cloud function and Cloud scheduler, but other approaches are available.</p> <ul> <li>In GCP, open Pub/Sub in the menu</li> <li>Click Create topic</li> <li>Provide a Topic ID</li> <li>The other parameter can be left as is. Click Create topic</li> </ul> <p></p> <ul> <li>Open your newly created topic</li> <li>Click on Trigger Cloud function at the top of the page. This will allow you to add function which will be triggered if the Pub/Sub is called</li> <li>Provide the required parameters for the Create function step:</li> <li>Environment can be left as 1st gen, but 2nd can also be used</li> <li>Provide a name in the Function name field</li> <li>Choose a Region which will be the same as the data/buckets/BigQuery which will be accessed</li> <li>Set the Timeout to 540 seconds (9 minutes)</li> <li>Set other settings as desired</li> </ul> <p></p> <ul> <li>Under Source code select the programming language for Runtime (e.g. Python)</li> <li>Paste your code in the MAIN.PY tab</li> <li>Entry point should consist of the name of the function which will be called in the code segment pasted in MAIN.PY</li> </ul> <p></p> <p>The Entry point function needs to have two parameters, namely \u2018event\u2019 and \u2018context\u2019. These parameters are included when the trigger event happens.</p> <ul> <li>Click Deploy function</li> <li>Now that the Pub/Sub topic and trigger function has been set up, we can set the schedule for when this should occur</li> <li>Go to Cloud Scheduler in the GCP menu</li> <li>Click Create job</li> <li>Set the parameters as follows:</li> <li>Name: As desired</li> <li>Region: Select the region which is used by your Sub/Pub topic</li> <li>Description: As desired</li> <li>Frequency: This consist of 5 values using this structure * * * * *, which is minute, hour, day of the month, month, and day of the week. * refers to all cases for that parameter. See https://cloud.google.com/scheduler/docs/configuring/cron-job-schedules for more information on how to set frequencies</li> <li>Time zone: Set as desired. South Africa Standard Time (SAST) is available as an option</li> </ul> <p></p> <ul> <li>Click Continue to go to the Configure the execution section</li> <li>Select Pub/Sub as the Target type</li> <li>Select the Topic which contains the trigger function</li> <li>Add a message as desired</li> <li>Set other options as desired</li> <li>Click Create</li> </ul> <p>The job will be executed based on the frequency set by the user. It will call the Pub/Sub topic, which in turn will trigger the Cloud function. To have a look at the function and execution, do the following: - Go to Cloud function in the GCP menu - Open the function. The user should see the metric</p> <p></p> <ul> <li>This section shows all information related to the function, such is when it was called, if it failed, completed, or is currently running</li> <li>Go to LOGS to see the terminal responses and print from the active function</li> </ul> <p></p>"},{"location":"development/resources/gcp/#dataflow-pipeline-using-python","title":"Dataflow pipeline using Python","text":"<p>When the user wants a Dataflow pipeline to make use of Python code a template needs to be generated from the Python code (other languages also supported, namely Java). This can be done as follows:</p> <ul> <li>This needs to be done on the GCP console (Linux-based terminal) and cannot be done locally:</li> <li>Open the cloud shell on GCP</li> <li>Click on More</li> <li>Select Upload</li> <li>Upload the files (e.g. Python code) as needed</li> </ul> <p></p> <ul> <li>The files will now be uploaded to the console home directory;</li> <li>Run the following command in the GCP console to create the pipeline template:</li> </ul> <p>python3 -m MODULE \\     --runner DataflowRunner \\     --project PROJECT_ID \\     --staging_location STAGING_LOC \\     --temp_location TEMPORARY_LOC \\     --template_location TEMPLATE_LOC \\     --region REGION</p> <ul> <li>MODULE: The Python file uploaded to the GCP console</li> <li>PROJECT_ID: ID of the project (not the project name)</li> <li>STAGING_LOC: The staging location for the pipeline (e.g. gs://pipeline_bucket/staging)</li> <li>TEMPORARY_LOC: Temporary location for the pipeline (gs://pipeline_bucket/temp);</li> <li>TEMPLATE_LOC: The location to which the JSON template will be saved (gs://pipeline_bucket/template);</li> <li>REGION: Region (e.g. us-east1)</li> </ul> <p>If a template fails to generate, there is likely a problem in the code. The code needs to make use of Apache-beam, and minor issues will be a problem. For instance, code might work locally, but an issue might occur when running in the pipeline itself. For more information on Python template generation go to  https://cloud.google.com/dataflow/docs/quickstarts/create-pipeline-python.</p>"},{"location":"development/resources/gcp/#compute-engine","title":"Compute Engine","text":"<p>A Dataflow pipeline requires a virtual machine (VM). This can be set up using GCP Compute engine. Follow these instructions to set up a VM:</p> <ul> <li>In the GCP navigation menu, select Compute engine</li> <li>Click on VM instances</li> <li>Click Create instance</li> </ul> <p></p> <ul> <li>Provide the following parameters:</li> <li>Name;</li> <li>Choose a Region and Zone. Best will be to make use of the same region than what your other data in a bucket or BigQuery makes use of;</li> <li>Choose a Series and Machine type</li> <li>Other parameters can be left on default or changed as desired.</li> </ul> <p></p> <ul> <li>Click Create</li> </ul> <p>Be sure to choose the machine type as required. The costs will be higher to more processing power is used</p> <p>Here is an example using a medium VM:</p> <p></p> <p>Here is an example of a high end VM:</p> <p></p> <p>Notice the cost difference! There are a large number of machine options, so be sure to choose as required.</p>"},{"location":"development/resources/gcp/#deploy-pipeline-python-code","title":"Deploy pipeline Python code","text":"<p>Dataflow makes use of Apache Beam. This can be developed/deployed using the SSH console for a VM. Do this as follows:</p> <ul> <li>Go to Compute engine in GCP;</li> <li>Create a virtual machine (see Compute engine), or select an existing virtual machine;</li> <li>Click on SSH. This will open the SSH console;</li> <li>Install Python modules:</li> <li>pip install apitools</li> <li>pip install \u201capache_beam[gcp]\u201d (this can run a very long time)</li> <li>pip install google</li> <li>pip install google-cloud-storage</li> <li>pip install google-cloud-bigquery</li> <li>pip install google-cloud</li> <li>Any other required Python modules (e.g. geopandas)</li> </ul> <p>If git is not installed for the VM, follow the instructions here to install git: https://www.atlassian.com/git/tutorials/install-git.</p> <ul> <li>Clone the repo from GitHub or other sources. This will download the repo into the compute engine root folder</li> <li>Navigate in the console to the code which needs to be run</li> <li>If providing parameters as as variable from the console:</li> <li>Set the bucket, if required, as follows:<ul> <li>BUCKET=\"\" <li>echo $BUCKET</li> <li>Set the project as follows:<ul> <li>PROJECT=\"\" <li>echo $PROJECT</li> <li>Set up the environmental variables (see Environmental variables). This is required for authentication and local runs in the VM:</li> <li>The user will require the credentials json file for the service account (see Service account key).</li> <p>Executing the pipeline:</p> <ul> <li>There are two options available. Local is faster, but cloud runs the pipeline on the cloud but is slower (the is shown In the GCP course). Here is how code can be run:</li> <li>Execute pipeline locally:<ul> <li>python3 file.py --parameter $BUCKET --project $PROJECT \u2013otherParameters $PARAMETER --DirectRunner</li> </ul> </li> <li>Execute in the cloud:<ul> <li>python3 file.py --bucket $BUCKET --project $PROJECT  \u2013otherParameters $PARAMETER --DataFlowRunner</li> </ul> </li> </ul>"},{"location":"development/resources/gcp/#bigquery-in-python","title":"BigQuery in Python","text":"<p>This section will deal with BigQuery in Python code.</p> <p>The table schema, basically the attributes structure, is important to set up correctly in BigQuery and will cause issues if the user attempts to load a table into BigQuery using the incorrect structure for the table being loaded. A table schema makes use of JSON, and is easy to set up. Here is a code example:</p> <p>SCHEMA = [         bigquery.SchemaField('Date', 'Date', mode='NULLABLE'),         bigquery.SchemaField('Max_Temperature', 'FLOAT', mode='NULLABLE'),         bigquery.SchemaField('Min_Temperature', 'FLOAT', mode='NULLABLE'),         bigquery.SchemaField('Precipitation', 'FLOAT', mode='NULLABLE'),         bigquery.SchemaField('Relative_Humidity', 'FLOAT', mode='NULLABLE'),         bigquery.SchemaField('Solar', 'FLOAT', mode='NULLABLE'),         bigquery.SchemaField('Streamflow', 'FLOAT', mode='NULLABLE'), ]</p>"},{"location":"development/technologies/","title":"Technologies","text":"<p>It makes sense to attempt some level of standardisation across development practices, including technologies and frameworks in use, so that the barrier for entry to projects is lowered and it becomes simple to maintain consistent practices across the organisation.</p> <ul> <li>Languages</li> <li>Frameworks</li> </ul>"},{"location":"development/technologies/#evaluating","title":"Evaluating","text":"<p>It may be helpful to include some \"smell tests\" for evaluating good projects or technologies</p>"},{"location":"development/technologies/frameworks/","title":"Frameworks","text":"<p>Standardise on the frameworks used to encourage collaboration and efficiency.</p>"},{"location":"development/technologies/frameworks/#cross-platform","title":"Cross platform","text":"<ul> <li>QT (multiple)</li> <li>React native (js)</li> <li>Flutter (dart)</li> <li>Quasar (js)</li> <li>Beeware (python)</li> <li>Kivy (python)</li> </ul>"},{"location":"development/technologies/frameworks/#desktop","title":"Desktop","text":"<ul> <li>See the cross platform section.</li> <li>tauri (js/ rust): Uses a rust backend and javascript frontend as a more performant alternative to electron, which bundles chromium into every build.</li> </ul>"},{"location":"development/technologies/frameworks/#mobile","title":"Mobile","text":"<p>If you're not going native, use the cross platform frameworks please.</p>"},{"location":"development/technologies/frameworks/#web","title":"Web","text":"<ul> <li>Django (python): Default to django. It's batteries included.</li> <li>Flask (python): Light weight python server</li> <li>FastApi (python): It's shiny, but pretty great. If you're a fan of starlette and async, and you only want a REST API, FastApi is where you might want to head. 90% of the time, use django+drf, or extend flask (more of the team can help you out there too). If you have to ask if you should use it, you probably shouldn't.</li> <li>Express (js): Serverside js.</li> <li>NextJS (js): Serverside react.</li> <li>Rails (ruby): If you are using or extending something in ruby (not recommended)</li> <li>tauri: Uses a rust backend and javascript frontend as a more performant alternative to electron, which bundles chromium into every build.</li> </ul>"},{"location":"development/technologies/frameworks/#docs","title":"Docs","text":"<p>Use Material Mkdocs</p>"},{"location":"development/technologies/frameworks/#jamstack","title":"Jamstack","text":"<p>Default to Hugo.</p>"},{"location":"development/technologies/frameworks/#gis","title":"GIS","text":"<p>OpenLayers, Leaflet, and CesiumJS. Turf.js, MapLibre. GeoDjango. pg_featureserv/ pg_tilserv.</p>"},{"location":"development/technologies/frameworks/#ui","title":"UI","text":"<p>Ensuring that consistent UI component libraries are used improves quality, efficiency, and consistency across applications and the overall UX.</p>"},{"location":"development/technologies/frameworks/#css-frameworks","title":"CSS frameworks","text":"<p>Bootstrap, bulma, tailwind</p>"},{"location":"development/technologies/frameworks/#component-libraries","title":"Component libraries","text":"<p>Nobody will stop you from starting an app in kotlin, but it's unlikely to get the momentum or collaboration that is desired within the organisation.</p>"},{"location":"development/technologies/languages/","title":"Languages","text":"<p>Focus is on python, javascript, and C++, but anything goes really.</p> <p>Note that python is the preferred language for tool and application implementation for a number of reasons, including having consistent tooling and ensuring that everyone on the team is able to collaborate. By remaining consistent and lowering the</p> <p>Javascript cannot be escaped on the modern web, and so is a requirement by default. In instances where is makes sense, using serverside javascript is certainly supported, but python is still the preferred approach.</p> <p>C++ is used for QGIS development and the focus for conventions will be on using C++ with QT.</p> <p>There are no strict rules applied to technology, so everybody is free to use whatever works, and a key ability in open source ecosystems is to remain adaptable and be capable of leveraging the existing tools that are the best available. Stand on the shoulders of giants where you can.</p> <p>Because these are not considered core competencies, there aren't really any expectations, requirements, or recommendations that apply to other ecosystems. How users choose to code in rust, go, ruby, or c is entirely up to them. But there are caveats.</p> <p>Boring solutions often work best. The newest shiny can be fun but it's rarely a good idea to dive into early adoption. Team work is important, and it's hard to work as a team when you're using esoteric tools.</p> <p>When in doubt, have a look at a chart of Diffusion of innovations and try assess if the current position is at the end of the early adopter and start of the early majority group.</p> <p>This includes the development of mobile, cross platform, or native apps. Whilst universal support for PWAs lingers, it is still a way off from being the de facto standard for the development industry. In the meantime, stick to using tools that the rest of the team can get up to speed with quickly or have experience with, such as QT, python, or javascript. Nobody will stop you from starting an app in kotlin, but it's unlikely to get the momentum or collaboration that is desired within the organisation.</p>"},{"location":"development/testing/","title":"Introduction","text":"<p>Software testing is a crucial phase in the software development life cycle (SDLC). Involving evaluating quality, functioning and performance of software.</p> <p>It involves:</p> <ul> <li>Analysing product requirements for completeness and correctness</li> <li>Reviewing product architecture</li> <li>Working on improving coding techniques with product developers</li> <li>Examining program behaviour</li> </ul> <p>The <code>main</code> goal is to detect software failures/defects early enough before deployment.</p> <p>Note: Faults and failures in a software my not necessarily mean errors/bugs in the code. It may be a <code>missing feature</code> which is a <code>requirement</code> by a client.</p> <p>For further reading: software testing basics.</p>"},{"location":"development/testing/#classification-of-software-testing","title":"Classification of software testing","text":"<p>Classified into two:</p> <ul> <li> <p><code>Manual testing</code>: Manual execution of test cases. Tester takes the role of the end user.</p> </li> <li> <p><code>Automated testing</code>: Automating manual testing using scripts and software. It comes in handy during regression testing.</p> </li> </ul>"},{"location":"development/testing/#levels-of-testing","title":"Levels of testing","text":"<ul> <li> <p><code>Unit Testing</code>: Individual units or components of a software application are tested in isolation.</p> </li> <li> <p><code>Integration Testing</code>: Different units or components are combined and tested as a group.</p> </li> <li> <p><code>System Testing</code>: The entire software system is tested as a whole.</p> </li> <li> <p><code>Acceptance Testing</code>: The software is tested to ensure it meets the specified requirements and is accepted by users.</p> </li> </ul>"},{"location":"development/testing/#testing-approaches","title":"Testing approaches","text":"<p>There are three testing approaches:</p> <ul> <li>1\ufe0f\u20e3 <code>Static testing</code>: It is mostly regarded as implicit, involves proofreading, checking syntax and data flows etc. (verification).</li> <li> <p>2\ufe0f\u20e3 <code>Dynamic testing</code>: It is done when running the program (validation). It normally starts when the program is still in development stage to ensure that certain functions work as they are supposed to.</p> </li> <li> <p>3\ufe0f\u20e3 <code>Passive testing</code>: It means verifying the system behaviour without any interaction with the software program. In this case no test data is provided but the tester studies logs and traces for specific patterns and specific behaviour.</p> </li> </ul>"},{"location":"development/testing/#testing-process","title":"Testing process","text":"<ul> <li><code>Test Planning</code>: Defining the scope, objectives, and resources for testing.</li> <li><code>Test Design</code>:  Creating test cases based on requirements and design specifications.</li> <li><code>Test Execution</code>: Running the tests and collecting results.</li> <li><code>Defect Tracking</code>: Identifying and documenting any defects found during testing.</li> <li><code>Test Reporting</code>: Communicating the test results to stakeholders.</li> </ul>"},{"location":"development/testing/#testing-concepts","title":"Testing concepts","text":"<p>Software testing touches on a few concepts:</p> <ul> <li>End-to-end testing</li> <li>Test driven development</li> <li>Behaviour driven development</li> <li>Functional testing</li> <li>Integrated testing</li> <li>Regression testing</li> <li>System testing</li> </ul>"},{"location":"development/testing/#frameworks-and-packages","title":"Frameworks and packages","text":"<p>Frameworks and packages:</p> <ul> <li>behave </li> <li>playwright</li> <li>playwright-bdd</li> <li>pytest </li> <li>black</li> <li>pyperformance</li> </ul>"},{"location":"development/testing/#best-practices","title":"Best practices","text":"<ul> <li><code>Early Testing</code>: Start testing as early as possible in the development life cycle.</li> <li><code>Regression Testing</code>: Ensure that new changes do not negatively impact existing functionality(Automated).</li> <li><code>Test Data Management</code>: Use realistic and diverse test data to uncover potential issues.</li> </ul>"},{"location":"development/testing/bdd/","title":"Behaviour driven development","text":""},{"location":"development/testing/bdd/#introduction","title":"Introduction","text":"<p>Behavioural-driven development (BDD) is an agile software development technique. BDD encourages the collaboration between the developers and the client. BDD focuses more on clear understanding of software behaviour through discussion with the client.</p> <p>It extends TDD by writing test cases in a natural language that non-programmers can read. Behavior-driven developers use their native language in combination with the ubiquitous language of domain-driven design to describe the purpose and benefit of their code. This allows the developers to focus on why the code should be created, and minimizes translation between the technical language in which the code is written and the domain language spoken by the business, users, stakeholders, project management, etc. Source, BDD philosophy.</p> <p>The developer should break customer requirements into simple examples using Gherkin. Gherkin is a business-readable, domain-specific language created for behaviour descriptions. Gherkin communicates to the developer in a human-like language.</p> <p>Behave is a tool implemented in python to make all this possible.</p>"},{"location":"development/testing/bdd/#playwright-bdd","title":"Playwright-BDD","text":"<p><code>playwright-bdd</code> is a nodejs package that allows you to work on behaviour-driven development. It runs the bdd tests using playwright runner.</p> <p><code>playwright-bdd</code> combines the power of CucumberJS and playwright.</p>"},{"location":"development/testing/bdd/#installation","title":"Installation","text":"<p>Create a directory <code>playwright-bdd</code>.</p> <p>To install <code>playwright-bdd</code> using npm:</p> <p><pre><code>npm i -D playwright-bdd\n</code></pre> This package uses <code>@playwright/test</code> and <code>@cucumber/cucumber</code> as a peer dependencies. For brand-new projects they will be installed automatically with playwright-bdd.</p> <p>For existing projects you may need to update them to the latest versions:</p> <pre><code>npm i -D @playwright/test @cucumber/cucumber\n</code></pre> <p>To install playwright browsers:</p> <pre><code>npx playwright install\n</code></pre>"},{"location":"development/testing/bdd/#setting-up","title":"Setting up","text":"<p>The examples are based off sawps project.</p> <p>In the playwright directory, create two directories:</p> <ul> <li>features: Contains <code>*.feature</code> files that use the Gherkin syntax</li> <li>steps: Contains <code>*.spec.ts</code> files that contains the steps taken when testing the features.</li> </ul> <p>Copy the following documents:</p> <pre><code># map-filter.feature\nFeature: Map filter\n\n    Map filter\n    Scenario: Map filter\n        Given I am on the project landing page \"http://localhost:61100/\"\n        When I click on \"Explore\" button\n        Then I should be redirected to the \"**/map\" view\n        Then I should see the map canvas on the page\n        When I configure filters\n        Then I should see data on the map and legend should be visible\n</code></pre> <pre><code>// map-filter.spec.ts\n\nimport { expect } from '@playwright/test';\nimport { createBdd } from 'playwright-bdd';\n\nconst { Given, When, Then } = createBdd();\n\n/* Scenario: Check Explore page */\n\nGiven('I am on the project landing page {string}', async ({ page }, url) =&gt; {\n  await page.goto(url);\n  await page.getByRole('link', { name: \"LOGIN\" }).click();\n  await page.getByPlaceholder('E-mail address').fill('admin@example.com');\n  await page.getByPlaceholder('Password').fill('admin');\n  await page.getByRole('button', { name: 'LOGIN' }).click();\n});\n\nWhen('I click on {string} button', async ({ page }, name) =&gt; {\n  await page.getByRole('link', { name }).first().click();\n});\n\nThen('I should be redirected to the {string} view', async ({ page }, url) =&gt; {\n  //await expect(page).toHaveTitle(new RegExp(keyword));\n  await page.waitForURL(url);\n});\n\nThen('I should see the map canvas on the page', async ({ page },) =&gt; {\n  await expect(page.getByRole('tab', { name: 'MAP' })).toBeVisible();\n  // Map canvas is visible\n  const map = 'canvas.maplibregl-canvas.mapboxgl-canvas';\n  await expect(page.locator(map)).toBeVisible();\n});\n\nWhen('I configure filters', async ({ page },) =&gt; {\n  await expect(page.getByRole('tab', { name: 'MAP' })).toBeVisible();\n  // Map canvas is visible\n  await page.locator('#combo-box-demo').click();\n\n  await page.getByRole('option', { name: 'Panthera leo' }).click();\n\n  await page.locator('nav').filter({ hasText: 'Organisation selected' }).getByLabel('Open').click();\n\n  await page.getByRole('button', { name: 'Close' }).click();\n\n});\n\nThen('I should see data on the map and legend should be visible', async ({ page },) =&gt; {\n\n  await expect(page.getByText('Panthera leo population (2024)')).toBeVisible();\n\n  // Map canvas is visible\n  const map = 'canvas.maplibregl-canvas.mapboxgl-canvas';\n\n  await expect(page.locator(map)).toBeVisible();\n});\n</code></pre> <p>Create a <code>playwright.config.ts</code> file. This file will contain the playwright configurations necessary for the project.</p> <p>Copy the following contents to the file:</p> <pre><code>import { defineConfig, devices } from '@playwright/test';\nimport { defineBddConfig } from 'playwright-bdd';\n\n/**\n * Read environment variables from file.\n * https://github.com/motdotla/dotenv\n */\n// require('dotenv').config();\n\n/**\n * See https://playwright.dev/docs/test-configuration.\n */\n\n/* Test directory with features and steps */\nconst testDir = defineBddConfig({\n  paths: ['features/*.feature'],\n  require: ['steps/*.ts'],\n  verbose: true,\n});\n\nexport default defineConfig({\n  testDir,\n  /* timeout */\n  timeout: 30 * 1000,\n  /* Run tests in files in parallel */\n  fullyParallel: true,\n  /* Fail the build on CI if you accidentally left test.only in the source code. */\n  forbidOnly: !!process.env.CI,\n  /* Retry on CI only */\n  retries: process.env.CI ? 2 : 0,\n  /* Opt out of parallel tests on CI. */\n  workers: process.env.CI ? 1 : undefined,\n  /* Reporter to use. See https://playwright.dev/docs/test-reporters */\n  reporter: 'html',\n  /* Shared settings for all the projects below. See https://playwright.dev/docs/api/class-testoptions. */\n  use: {\n    /* Base URL to use in actions like `await page.goto('/')`. */\n    baseURL: 'http://localhost:61100/',\n\n    /* Collect trace when retrying the failed test. See https://playwright.dev/docs/trace-viewer */\n    trace: 'on-first-retry',\n  },\n\n  /* Configure projects for major browsers */\n  projects: [\n    //{ name: 'setup', testMatch: /.*\\.setup\\.ts/ },\n    {\n      name: 'chromium',\n      use: { ...devices['Desktop Chrome'],\n      // Use prepared auth state.\n     },\n    },\n});\n</code></pre> <p>Create a <code>.gitignore</code> file at the root directory of the playwright-bdd project. <pre><code>**/.features-gen/**/*.spec.js\n</code></pre> This will ignore the generated tests not to be committed by git.</p>"},{"location":"development/testing/bdd/#running-tests","title":"Running tests","text":"<p>To run tests on your terminal; <pre><code>npx bddgen &amp;&amp; npx playwright test\n</code></pre> The first command <code>npx bddgen</code> generates the tests from the feature and step files. <code>npx playwright test</code> runs the test using the playwright runner.</p> <p>For more on playwright-bdd, review the docs here.</p>"},{"location":"development/testing/bdd/#behave","title":"behave","text":""},{"location":"development/testing/bdd/#installation_1","title":"Installation","text":"<p>Behave installation:</p> <ul> <li> <p>Created a new conda environment: <pre><code>  conda create -n testing python\n</code></pre></p> </li> <li> <p>Installed <code>behave</code> using pip <pre><code>  pip install behave\n</code></pre></p> </li> </ul>"},{"location":"development/testing/bdd/#gherkin-feature-testing-language","title":"Gherkin Feature Testing language","text":"<p>Features are made up of scenarios: <pre><code>  Feature: feature name\n\n    Scenario: some scenario\n      Given some condition\n       Then some result is expected.\n</code></pre></p> <p>An example:</p> <ul> <li>Make <code>features</code> directory</li> <li>Make <code>tutorial.feature</code> file inside features directory</li> <li> <p>Copy sample feature scenarios to <code>tutorial.feature</code>: <pre><code>Feature: showing off behave\n\n  Scenario: run a simple test\n     Given we have behave installed\n      When we implement a test\n      Then behave will test it for us!\n</code></pre></p> </li> <li> <p>Make a new directory <code>features/steps</code> and create <code>tutorial.py</code> file.</p> </li> <li> <p>Copy below code: <pre><code>from behave import *\n\n@given('we have behave installed')\ndef step_impl(context):\n    pass\n\n@when('we implement a test')\ndef step_impl(context):\n    assert True is not False\n\n@then('behave will test it for us!')\ndef step_impl(context):\n    assert context.failed is False\n</code></pre></p> </li> <li> <p>Run code in the directory: <pre><code>behave\n</code></pre></p> </li> <li> <p>The results would be as follows:</p> </li> </ul> <p></p> <p>For a more comprehensive tutorial, kindly look at behave tutorial</p> <ul> <li>NOTE: Testing layout.</li> </ul> <p></p>"},{"location":"development/testing/bdd/#django-test-integration","title":"Django test integration","text":"<ul> <li>There are two projects that integrate django and behave:<ul> <li>django-behave</li> <li>behave_django</li> </ul> </li> </ul>"},{"location":"development/testing/bdd/#behave_django","title":"<code>behave_django</code>","text":""},{"location":"development/testing/bdd/#setting-up-behave_django","title":"Setting up <code>behave_django</code>","text":"<p>To install: <code>pip install behave_django</code></p> <p>Add <code>behave_django</code> to <code>INSTALLED_APPS</code> in <code>core.settings.dev</code> module.</p> <p>Under <code>django_project</code> directory:</p> <ul> <li>Create <code>tests</code> directory and <code>behave.ini</code> file <pre><code>mkdir tests\n\ntouch behave.ini\n</code></pre></li> <li> <p>In the <code>behave.ini</code> file, add the following: <pre><code>[behave]\npaths = tests/acceptance\njunit_directory = tests/reports\njunit = yes\n</code></pre></p> </li> <li> <p>Under <code>tests</code>,  create <code>acceptance</code> directory: <pre><code>cd tests\n\nmkdir acceptance\n</code></pre></p> </li> <li> <p>Under <code>acceptance</code> create two directories: <code>features</code> and <code>steps</code> <pre><code>mkdir features steps\n</code></pre></p> </li> <li>In the <code>acceptance</code> directory create an <code>environment.py</code> file <pre><code>\"\"\"\nbehave environment module\n\"\"\"\n# Customise according to project\n\ndef before_feature(context, feature):\n    if feature.name == 'Fixture loading':\n        context.fixtures = ['behave-fixtures.json']\n\n    elif feature.name == 'Fixture loading with decorator':\n        # Including empty fixture to test that #92 is fixed\n        context.fixtures = ['empty-fixture.json']\n\n\ndef before_scenario(context, scenario):\n    if scenario.name == 'Load fixtures for this scenario and feature':\n        context.fixtures.append('behave-second-fixture.json')\n\n    if scenario.name == 'Load fixtures then reset sequences':\n        context.fixtures.append('behave-second-fixture.json')\n        context.reset_sequences = True\n\n    if scenario.name == 'Load fixtures with databases option':\n        context.databases = '__all__'\n\n\ndef django_ready(context):\n    context.django = True\n</code></pre></li> </ul>"},{"location":"development/testing/bdd/#creating-bdd-tests","title":"Creating bdd tests","text":"<p>Add <code>*.feature</code> file in features directory. Feature files use the Gherkin syntax.</p> <pre><code>Feature: auth\n\n    Scenario: Unauthenticated user can't access the page\n        Given I am not authenticated\n        When I access the page\n        Then Status code is 302\n\n\n    Scenario: Authenticated user can access the page\n        Given I am authenticated\n        When I access the page\n        Then Status code is 200\n</code></pre> <p>Add <code>*.py</code> file in steps directory.</p> <pre><code>from behave import given, when, then, use_step_matcher\nfrom django.contrib.auth.models import User\n\nuse_step_matcher(\"re\")\n\n\n@given(\"I am not authenticated\")\ndef not_auth(context):\n    pass\n\n\n@when(\"I access the page\")\ndef access_page(context):\n    context.response = context.test.client.get(\"/map/\")\n\n\n@then(\"Status code is (?P&lt;status&gt;\\d+)\")\ndef status_code(context, status):\n    code = context.response.status_code\n    assert code == int(status), \"{0} != {1}\".format(code, status)\n\n\n@given(\"I am authenticated\")\ndef auth_success(context):\n    user = User.objects.create_superuser(\"admin\", \"admin@example.com\", \"admin\")\n    context.test.client.force_login(user)\n</code></pre>"},{"location":"development/testing/bdd/#running-tests_1","title":"Running tests","text":"<p>First collect static: <code>python manage.py collectstatic</code></p> <p>To run: <code>python manage.py behave</code></p> <p>Results:</p> <p></p>"},{"location":"development/testing/end2endtesting/","title":"End-to-end testing","text":""},{"location":"development/testing/end2endtesting/#introduction","title":"Introduction","text":"<p>E2E verifies working order of a system from start to end, taking into account real world scenarios the system can run(simulate user experience)</p> <p>Most of the projects of the applications at the company are mainly in Django with a React frontend.</p> <p>To accomplish E2E testing, we use playwright. It supports all the major browsers used today.</p>"},{"location":"development/testing/end2endtesting/#getting-started","title":"Getting started","text":"<p>To get started with Playwright, you need to ensure you have playwright installed in your device locally. There are a variety of playwright packages, but as per our standards, we will use Playwright Node.js package.</p>"},{"location":"development/testing/end2endtesting/#installing-nodejs","title":"Installing Node.js","text":"<p>To set up Node.js in Debian/Ubuntu or Fedora, the binary distributions for Node.js are available at Nodesource. The binary setups are directly installed.</p> <p>For NixOS, the configuration can be found at NixOS packages. You can go ahead and search for the Node.js version best suited. The configuration for that package is then added to <code>etc/nixos/configuration.nix</code>. <pre><code>environment.systemPackages = [\n    pkgs.nodejs_18\n  ];\n</code></pre></p> <p>For NixOS users, it is an added advantage if <code>direnv</code> is installed. When you navigate into the specific directory e.g., ci-tests, your environment will be set up.</p> <p>NOTE: For this to work for Nix-OS users, ensure the directory has an <code>.envrc</code> and <code>default.nix</code> file. <code>shell.nix</code> file also works.</p>"},{"location":"development/testing/end2endtesting/#installing-playwright-using-npm","title":"Installing playwright using npm","text":"<p>At the root of your project directory <code>Project</code>, navigate to <code>playwright</code>.</p> <pre><code>cd playwright\n</code></pre> <p>In the <code>playwright</code> directory, there are two more directories:</p> <pre><code>$ ls\nci-tests staging-tests\n</code></pre> <p>To set up a new playwright project use: <pre><code>npm init playwright@latest\n</code></pre></p> <p>To update an existing project use: <pre><code>npm install -D @playwright/test@latest\n</code></pre></p> <p>To configure playwright step by step, you will have to:</p> <ul> <li> <p>To install all browsers and all its dependencies: <pre><code>npx playwright install --with-deps\n</code></pre></p> </li> <li> <p>To install one browser and its dependencies <pre><code>npx playwright install chromium --with-deps\n</code></pre></p> </li> </ul>"},{"location":"development/testing/end2endtesting/#setting-up-playwright","title":"Setting up playwright","text":""},{"location":"development/testing/end2endtesting/#for-continuous-integrationci","title":"For continuous integration(<code>CI</code>):","text":"<p>Playwright does support Continuous Integration. For more information, visit the playwright ci docs.</p> <p>Navigate to <code>playwright/ci-tests</code> directory.</p> <p>In the directory ensure the below files are present:</p> <pre><code>$ cd ci-tests\n$ ls\npackage.json playwright.config.ts\n</code></pre> <p>Then you can proceed with setting up playwright:</p> <ul> <li> <p>To update npm dependencies <pre><code>npm install\n</code></pre></p> </li> <li> <p>To install CI dependencies <pre><code>npm ci\n</code></pre></p> </li> <li> <p>To install playwright package, browsers and linux dependencies <pre><code>npx playwright install --with-deps\n</code></pre></p> </li> <li> <p>To run tests: <pre><code>npx playwright test\n</code></pre> By default, this test will run in <code>headless</code> mode(No browser will be opened).</p> </li> </ul>"},{"location":"development/testing/end2endtesting/#for-staging-tests","title":"For staging tests","text":"<p>In setting up environment for staging tests, it uses the same approach as in setting up for CI. The only difference is that installing dependencies for CI won't be required.</p> <p>NOTE: Both <code>ci-tests</code> and <code>staging-tests</code> directory will have scripts to assist in setting up the environment easily.</p> <p>The scripts are:</p> <ul> <li><code>create-auth.sh</code>: Used to create a cookie file with the session state saved.</li> <li><code>record-test.sh</code>: Used to record new tests.</li> <li><code>run-tests.sh</code>: Used to run tests.</li> </ul> <p>These scripts check if you have the required environment is set up, if it is not, the script will set up everything. After setting up the environment, the script will proceed to run the next step.</p> <ul> <li> <p>Start off by creating the session state file. </p> </li> <li> <p>Shows how to run the script in your terminal</p> </li> <li> <p>The script will prompt you if you want to save the <code>auth.json</code> file.</p> </li> <li> <p>Proceed to log in. </p> </li> <li> <p>The <code>auth.json</code> will be created. You can then proceed to record your tests. </p> </li> <li> <p>To record your tests, proceed to run the next script <code>record-test.sh</code>. The script takes a name argument for the file to be created <code>./record-test.sh demo</code>. </p> </li> <li> <p>The script will open a browser and load the required page. It will use the session state that was previously created.  </p> </li> <li> <p>Click on the page elements to record a test. </p> </li> <li> <p>NOTE: For playwright version 1.40 and above, you get a mini-toolbar that helps you in:</p> </li> <li> <p>Assert if an element is visible.</p> </li> <li>Assert if an element contains a specific text</li> <li> <p>Assert if an element has a certain value </p> </li> <li> <p>This is the generated script from the above action of recording tests. <code>expect</code> is used in assertions as seen below. </p> </li> <li> <p>To run the tests, use <code>./run-tests.sh</code>.  </p> </li> <li> <p>It will open a GUI playwright test runner with all tests. You can then proceed to run the tests. </p> </li> </ul>"},{"location":"development/testing/end2endtesting/#for-visual-studio-code","title":"For visual studio code","text":"<p>Install extension <code>Playwright extention</code></p> <p>Click on the vscode's extension icon:</p> <p></p> <p>Search for <code>playwright test</code>, select the below playwright test extension:</p> <p></p> <p>Install the extension:</p> <p></p> <p>On your keyboard, press <code>ctrl + shift + P</code>. Search for <code>playwright</code>, select <code>Install Playwright</code>.</p> <p></p> <p>It will open up the following menu:</p> <p></p> <p>For option <code>1</code>: You can choose to install one or all the browsers.</p> <p>For option <code>2</code>: - Use <code>TypeScript</code> as a default(current preferred standard). - You can enable to add <code>GitHub actions</code> if the tests are for <code>CI</code>. - Enable to <code>Install Linux dependencies</code> if you are on Debian/Ubuntu. You can check this option if you are installing playwright for the first time.</p> <p>Press <code>Ok</code> to proceed: It will install and set up the project.</p> <p></p>"},{"location":"development/testing/end2endtesting/#running-playwright-tests-in-visual-studio-code","title":"Running playwright tests in visual studio code","text":"<p>To run tests in vscode, click on this testing icon.</p> <p></p> <p>It will scan your <code>tests</code> directory for playwright tests.</p> <p></p> <p>To run, click on the triangle icon:</p> <p></p> <ul> <li><code>1</code>: Lists down all tests functions. You can test normally and also debug from here.</li> <li><code>2</code>: You can run the test functions from here.</li> <li><code>3</code>: Shows the test results for each session.</li> </ul> <p>The tests will run and the results shown.</p> <p>For more information, look at the playwright docs for vscode.</p>"},{"location":"development/testing/functionaltesting/","title":"Functional testing","text":"<p>Each function is compared to specifications to ascertain that it returns the required outputs.</p>"},{"location":"development/testing/functionaltesting/#framework","title":"Framework","text":"<p>For Functional testing, Pytest is preferred.</p>"},{"location":"development/testing/functionaltesting/#setting-up","title":"Setting up","text":"<p>Before installing <code>pytest</code>, ensure you have python installed. Manage your environment using <code>conda</code>, <code>venv</code>, <code>pyenv</code>, <code>poetry</code> or <code>shell.nix</code>(NixOS).</p>"},{"location":"development/testing/functionaltesting/#installation","title":"Installation","text":"<p>First, we start by creating an environment(<code>conda</code> specific.) <pre><code># New environment\nconda create --name &lt;env_name&gt;\n</code></pre></p> <p>To install <code>pytest</code>, open your terminal: <pre><code>pip install pytest \n</code></pre></p> <p>For an existing project with a <code>requirements.txt</code> or <code>environment.yml</code> file <pre><code># requirements.txt\nconda create --name &lt;env_name&gt; --file requirements.txt\n\n# or\n\n# environment.yml\nconda env create -f environment.yml\n</code></pre></p>"},{"location":"development/testing/functionaltesting/#usage","title":"Usage","text":"<p>To use <code>pytest</code> you need to create a <code>tests</code> directory in your project. Inside the test directory, you can create tests.</p> <p>In the test file, import the pytest module. <pre><code>import pytest\n</code></pre></p>"},{"location":"development/testing/functionaltesting/#creating-tests","title":"Creating tests","text":"<p>The test files have a naming convention. The file name has to have prefix <code>test_</code> or suffix <code>_test.py</code>. For example: <code>test_*.py</code> and <code>*_test.py</code> respectively.</p> <p>NOTE: Using the prefix <code>test_</code> is preferred.</p> <p>The test functions also do have a naming convention. The function has to have a prefix <code>test_.</code> For example: <pre><code>...\n\n# test function\ndef test_addition():\n    ...\n</code></pre></p> <p>To create new functional tests using pytest:</p> <p>Function in <code>Project/example.py</code> module. <pre><code>...\n\n# function module\ndef addition(a, b):\n    return a + b \n</code></pre></p> <p>Test function in <code>tests/</code> directory. <pre><code>import pytest\nimport Project.example\n\n# test function\ndef test_addition():\n    c = addition(3, 2)\n    assert c == 5\n\n...\n</code></pre></p>"},{"location":"development/testing/functionaltesting/#running-tests","title":"Running tests","text":"<p>To run the tests, open your terminal and navigate to your test directory: <pre><code>pytest\n</code></pre></p> <p>The above command will run all tests present in the <code>tests/</code> directory.</p> <p>To run a single test use: <pre><code>pytest tests/test_example.py\n</code></pre></p> <p>For more, refer to invoking pytest.</p>"},{"location":"development/testing/functionaltesting/#pytest-fixtures","title":"Pytest fixtures","text":"<p>Fixtures are functions that provide resources or perform actions required for test execution. By encapsulating these setup and teardown steps into fixtures: - It streamlines the test code and promotes code re-usability - It improves the overall readability and maintainability of the test suite.</p> <p>Fixture example that contains raster path that can be used by multiple tests:  <pre><code>import pytest\n\n# fixtures\n@pytest.fixture()\ndef vector_data():\n    path = 'vector.geojson'\n    return path\n\n@pytest.fixture()\ndef raster_data():\n    path = 'raster.tif'\n    return path\n\ndef test_mask_raster(raster_data, vector_path):\n    raster = mask_raster(raster_data, vector_path)\n    ...\n</code></pre></p>"},{"location":"development/testing/functionaltesting/#pytest-in-django-projects","title":"Pytest in Django projects","text":"<p><code>pytest-django</code> is a pytest plugin used to test django applications.</p>"},{"location":"development/testing/functionaltesting/#installation_1","title":"Installation","text":"<p>To install <code>pytest-django</code>, open you terminal and activate your environment. Then type: <pre><code>pip install pytest-django\n</code></pre></p>"},{"location":"development/testing/functionaltesting/#setting-up-and-configuration","title":"Setting up and configuration","text":"<p><code>pytest-django</code> is set up in the <code>django_project</code> directory. Navigate to the <code>django_project</code>, ensure there is a <code>pytest.ini</code> with the following configurations: <pre><code>[pytest]\nenv =\n    DJANGO_SETTINGS_MODULE=core.settings.test\n    AZURE_B2C_CLIENT_ID=\n</code></pre></p> <p>In the <code>django_project</code>, there are django apps each in a different directory. You need to create a <code>tests</code> folder in each directory and add an <code>__init.py__</code> file to make a package.</p> <p>In the <code>tests</code> directory, create tests based on guidelines. An example of the directory structure: <pre><code>$ project             # root directory\n    django_project    # django project\n      __init__.py\n      core\n  -&gt;  &lt;django_app_1&gt;  # django application\n        __init__.py\n    -&gt;  tests         # tests directory\n          __init__.py\n      -&gt;  test_example.py   # test file &lt;test_*.py&gt;\n      frontend\n      manage.py\n      pytest.ini\n</code></pre></p>"},{"location":"development/testing/functionaltesting/#running-tests_1","title":"Running tests","text":"<p>To run <code>pytest</code> in django, navigate to the <code>django_project</code> directory the run: <pre><code>pytest\n</code></pre> All the tests in each django application will be executed.</p> <p>To run a particular test use: <pre><code>pytest &lt;django_app&gt;/tests/test_example.py\n</code></pre></p>"},{"location":"development/testing/functionaltesting/#pytest-with-bdd","title":"Pytest with BDD","text":"<p>BDD library for pytest.</p>"},{"location":"development/testing/integrationtesting/","title":"Integration testing","text":"<p>Tests how multiple units operate together.</p> <p>It aims to uncover defects that may arise at the interfaces between these interconnected parts, ensuring seamless data flow and functionality across the entire system.</p> <p>NOTE: It occurs after unit testing and just before system testing.</p>"},{"location":"development/testing/integrationtesting/#types-of-integration-testing","title":"Types of integration testing","text":"<ul> <li>Big bang integration testing</li> <li>Incremental integration testing</li> </ul>"},{"location":"development/testing/integrationtesting/#big-bang-integration-testing","title":"Big bang integration testing","text":"<p>All modules are integrated simultaneously and tested as a whole.</p>"},{"location":"development/testing/integrationtesting/#incremental-integration-testing","title":"Incremental integration testing","text":"<p>Modules are integrated and tested in smaller increments or groups.</p> <p>Subtypes include:</p> <ul> <li><code>Top-down integration</code>: Start with high-level modules and work downward.</li> <li><code>Bottom-up integration</code>: Start with low-level modules and work upward.</li> <li><code>Sandwich/Hybrid integration</code>: Combine top-down and bottom-up approaches.</li> </ul>"},{"location":"development/testing/integrationtesting/#test-cases","title":"Test cases","text":"<ul> <li> <p><code>Verification of Navigation Flow</code>: Test that clicking on a link/button on the landing page redirects the user to the correct section/page (e.g., navigation from landing to product details).</p> </li> <li> <p><code>Data Exchange Between Components</code>: Verify that data entered on the login page is correctly passed to the user authentication module and that the user is redirected to the appropriate dashboard.</p> </li> <li> <p><code>Testing External Service Integration</code>: If the application interacts with external services.</p> </li> <li> <p><code>Testing API Integrations</code>: If the application uses APIs, test that data is correctly exchanged between the frontend and backend APIs. Verify that the API responses are processed appropriately and displayed on the user interface.</p> </li> <li> <p><code>Verification of Error Handling</code>: Test error scenarios, such as entering incorrect login credentials, and confirm that error messages are displayed correctly without affecting the overall application.</p> </li> <li> <p><code>Testing Concurrent User Actions</code>: Simulate scenarios where multiple users interact with the application simultaneously (e.g. updating profiles) to ensure that data integrity is maintained.</p> </li> <li> <p><code>Integration with Third-Party Plugins</code>: If the application relies on third-party plugins or modules, verify that the integration is seamless, and functionalities provided by these plugins work as expected.</p> </li> <li> <p><code>Verification of User Roles</code>: Test scenarios involving different user roles (e.g., admin, regular user) to ensure that each role has the appropriate access permissions and functionalities.</p> </li> <li> <p><code>Testing Session Management</code>: Verify that user sessions are managed correctly, and actions performed in one session do not impact the state of other user sessions.</p> </li> <li> <p><code>Testing Cross-Browser Compatibility</code>: Ensure that the integration works consistently across different web browsers, confirming that users have a similar experience regardless of the browser they use.</p> </li> <li> <p><code>Verification of Database Integrity</code>: Check that changes made through the user interface (e.g., updating profile information) are accurately reflected in the underlying database.</p> </li> </ul>"},{"location":"development/testing/regression/","title":"Regression testing","text":"<p>Regression testing is a type of testing after a code update to ensure that the code did not introduce new bugs.</p> <p>This means that a testing suite needs to exist prior to conducting regression tests.</p> <p>Regression testing logic:</p> <p></p> <p>Regression tests can be done in various approaches:</p> <ul> <li>CI/CD tests</li> <li>Functional and TDD tests</li> <li>End-to-end tests</li> </ul>"},{"location":"development/testing/regression/#regression-tests-in-cicd","title":"Regression tests in CI/CD","text":"<p>In CI/CD, regression tests can be set up in GitHub actions. This is implemented by structuring the workflow in <code>.github/workflows/*.yml</code> file.</p> <p>The CI pipeline is where automatic tests are triggered by code commit or a pull request to the main branch. This ensures a faster testing approach.</p> <p>The approach:</p> <ul> <li>Build the containers and set up the project</li> <li>Test various aspects of the software</li> <li>If all checks pass, the changes are open to being merged to the main branch.</li> </ul>"},{"location":"development/testing/regression/#functional-and-tdd-tests","title":"Functional and TDD tests","text":"<p>This is another way to conduct regression tests. This approach can be implemented using various software testing techniques. Examples are:</p> <ul> <li>Test-Driven Development</li> <li>Functional testing</li> </ul>"},{"location":"development/testing/regression/#end-to-end-tests","title":"End-to-end tests","text":"<p>Recording tests can be done using playwright. Playwright allows one to be able to record their workflow step by step then generates a script. You can run the script after each new deployment to staging/production.</p> <p>All checks are expected to pass, unless there is a major workflow change. This method comes in handy when testing if a bug has been fixed.</p> <p>For more information, look at end-to-end-testing.</p>"},{"location":"development/testing/systemtesting/","title":"System testing","text":"<p>This type of testing evaluates the system as a whole rather than individual components. The primary goal is to verify that the software system functions correctly and meets its intended requirements and objectives.</p> <p>It evaluates the overall functionality and performance of a complete system.</p> <p>It tests if the system meets the specified requirements and if it is ideal to be deployed for end users.</p> <p>It is done after integration testing and before acceptance testing.</p> <p>For more: Look at system testing.</p>"},{"location":"development/testing/systemtesting/#key-aspects","title":"Key aspects","text":"<ul> <li> <p><code>Functional Testing</code>: Verifying that the system functions according to the specified requirements. This involves testing all the features and capabilities of the system.</p> </li> <li> <p><code>Performance Testing</code>: Evaluating the system's performance characteristics, such as responsiveness, scalability, and stability under varying conditions. This may include load testing, stress testing, and scalability testing.</p> </li> <li> <p><code>Security Testing</code>: Ensuring that the system is secure and that sensitive data is protected. This involves testing for vulnerabilities, access controls, and encryption mechanisms.</p> </li> <li> <p><code>Usability Testing</code>: Assessing the user-friendliness and overall user experience of the system. This includes evaluating the system's interface, navigation, and overall ease of use.</p> </li> <li> <p><code>Compatibility Testing</code>: Verifying that the system works correctly on different platforms, browsers, and devices. This is particularly important in today's diverse technology landscape.</p> </li> <li> <p><code>Regression Testing</code>: Checking whether new changes introduced during development have adversely affected existing functionalities. This helps ensure that new features do not introduce unexpected issues.</p> </li> <li> <p><code>System Integration Testing</code>: Confirming that different components of the system work together seamlessly. This involves testing interactions between modules or subsystems to identify and resolve integration issues.</p> </li> <li> <p><code>Data Integrity Testing</code>: Ensuring that data is accurately and consistently stored, retrieved, and processed by the system.</p> </li> <li> <p><code>Installation Testing</code>: Verifying that the installation process of the software is smooth and that the software can be successfully installed and uninstalled without issues.</p> </li> <li> <p><code>Recovery Testing</code>: Evaluating the system's ability to recover from failures, such as crashes or hardware failures, and ensuring data integrity after recovery.</p> </li> </ul>"},{"location":"development/testing/tdd/","title":"Test-Driven Development","text":"<p>It is an agile technique used in the early onset of software development.</p> <p>It entails creating tests for each specific function before writing the function itself. The test is bound to fail at first, but the developer is supposed to ensure the test cases now passes when writing the function.</p> <p>Test-Driven Development starts with designing and developing tests for every small functionality of an application. TDD framework instructs developers to write new code only if an automated test has failed. This avoids duplication of code.</p> <p>TDD workflow:</p> <p></p>"},{"location":"development/testing/tdd/#tdd-using-pytest-in-django-projects","title":"TDD using pytest in django projects","text":""},{"location":"development/testing/tdd/#setting-up","title":"Setting up","text":"<p>Navigate to the <code>django_project</code> directory.</p> <p><code>pytest</code> is used to create this tests. It is quite similar to Functional testing. <code>pytest</code> is installed while setting up the docker containers for the project. Ensure <code>pytest-django</code> is added to the settings.</p> <p>In the <code>django_directory</code>, you have to set up the <code>pytest</code> configuration for the project. This is declared in the <code>pytest.ini</code> file.</p> <p>Example: <pre><code>[pytest]\nenv =\n    DJANGO_SETTINGS_MODULE=core.settings.test\n    AZURE_B2C_CLIENT_ID=\n</code></pre> In this case, <code>pytest</code> uses the in-built django test cases(<code>core.settings.test</code>).</p> <p>The convention for using pytest is the same: - Test files organisation: Having a <code>tests/</code> directory in each django application. - Naming convention for test files: <code>tests_&lt;file_name&gt;.py</code> - Naming convention for test functions: <code>test_&lt;function_name&gt;</code> </p> <p>Note: Each django application in the django directory should have its own <code>tests/</code> directory.</p>"},{"location":"development/testing/tdd/#adding-tests","title":"Adding tests","text":"<p>Function to be tested from a django application (<code>*.views</code>):</p> <pre><code>def check_email_exists(request):\n    if request.method == 'GET':\n        email = request.GET.get('email', '').strip()\n        current_user_email = request.user.email\n\n        if email and email != current_user_email:\n            email_exists = User.objects.filter(email=email).exists()\n            return JsonResponse({'exists': email_exists})\n\n    return JsonResponse({'exists': False})\n</code></pre> <p>Below is an example of a test case (<code>test_email_exist.py</code>:</p> <pre><code>from django.urls import reverse\nfrom django.test import TestCase                    # using django test case\nfrom django.test.client import RequestFactory       # using request factory\nfrom stakeholder.views import check_email_exists    # project dependent import \nfrom django.contrib.auth import get_user_model\n\n# test class\nclass CheckEmailExistsViewTest(TestCase):\n    def setUp(self):\n        self.user = get_user_model().objects.create_user(\n            username='testuser', \n            email='test@example.com', \n            password='testpassword'\n        )\n        self.factory = RequestFactory()\n\n    # test function \n    # Note: naming convention\n    def test_email_exists(self):\n        url = reverse('check_email_exists')\n        data = {\n            'email': 'test@gmail.com',\n            'csrfmiddlewaretoken': self.client.cookies.get('csrftoken', '')\n        }\n\n        request = self.factory.get(url, data)\n        request.user = self.user\n\n        results = check_email_exists(request)\n        self.assertEqual(results.status_code, 200)\n\n        expected_data = {'exists': False}\n        self.assertJSONEqual(results.content, expected_data)\n</code></pre>"},{"location":"development/testing/tdd/#running-tests","title":"Running tests","text":"<p>Navigate to the <code>django_project</code>, on your terminal run:</p> <p><pre><code>pytest\n</code></pre> </p> <p><code>pytest</code> will scan the whole project for test functions then the tests will be executed.</p> <p>Results:</p> <p></p> <p>The results will show all tests ran, failed and passed.</p> <p>To run tests on a specific module use:</p> <p><pre><code>pytest stakeholder/tests/test_email_exists.py\n</code></pre> </p> <p>Results:</p> <p></p>"},{"location":"devops/","title":"DevOps","text":"<p>Resources, information, and processes related to DevOps and system administration.</p> <p>Note that all users, regardless of role, should understand and review the security section.</p> <p>Note that this is not limited to DevOps team members and may include conventions and configuration for the management of workstations, servers, and other systems, which is relevant to multiple roles.</p> <ul> <li>Security</li> <li>Procedures</li> <li>Infrastructure</li> </ul>"},{"location":"devops/applications/standard-applications/","title":"Standard Deployment Applications","text":"<ul> <li>Geoserver</li> <li>Filebrowser</li> <li>pg-backups</li> <li>Postgres</li> <li></li> </ul>"},{"location":"devops/infrastructure/","title":"Infrastructure","text":"<ul> <li>Development Infrastructure</li> <li>Personal Infrastructure</li> <li>Containers</li> <li>Kubernetes</li> <li>Rancher</li> <li>Rancher Desktop</li> <li>Development</li> <li>Staging</li> <li>Production</li> <li>CI/CD</li> </ul>"},{"location":"devops/infrastructure/#rancher-topics","title":"Rancher Topics","text":"<p>Using Rancher, K3S, and Longhorn for single node Kubernetes deployment</p>"},{"location":"devops/infrastructure/personal_infrastructure/","title":"Personal Kubernetes Infrastructure","text":""},{"location":"devops/infrastructure/personal_infrastructure/#initial-setup","title":"Initial Setup","text":"<p>This are initial notes from Tim on setting up a personal docker, rancher desktop and rancher server testbed environment.</p> <p>I started by setting up rancher desktop on my Fedora based linux laptop. I used the appimage (despite a concerning looking warning that firefox gives about the image when downloading it.)</p> <p>Here is the installation guide.</p> <p>Note: After upgrading to fedora 36 rancher desktop no longer started until I applied the following fix: https://github.com/rancher-sandbox/rancher-desktop/issues/2139#issuecomment-1114933138</p> <p>Once installed I performed the following commands to do a quick start setup of nginx running in the k8 cluster.</p> <pre><code>kubectl get nodes\n</code></pre> <p>NAME                   STATUS   ROLES                  AGE   VERSION lima-rancher-desktop   Ready    control-plane,master   80m   v1.22.7+k3s1 </p> <pre><code>kubectl create deployment nginx --image=nginx\n</code></pre> <p>deployment.apps/nginx created</p> <pre><code>kubectl get pods\n</code></pre> <p>NAME                     READY   STATUS              RESTARTS   AGE nginx-6799fc88d8-7k4kd   0/1     ContainerCreating   0          10s</p> <pre><code>kubectl describe nginx-6799fc88d8-7k4kd\n</code></pre> <p>error: the server doesn't have a resource type \"nginx-6799fc88d8-7k4kd\"</p> <pre><code>kubectl describe pod nginx-6799fc88d8-7k4kd\n</code></pre> <p>Name:         nginx-6799fc88d8-7k4kd Namespace:    default Priority:     0 Node:         lima-rancher-desktop/192.168.5.15 Start Time:   Sat, 23 Apr 2022 22:57:06 +0100 Labels:       app=nginx pod-template-hash=6799fc88d8 Annotations:  none Status:       Running IP:           10.42.0.9 IPs: IP:           10.42.0.9 Controlled By:  ReplicaSet/nginx-6799fc88d8 Containers: nginx:     &gt; Container ID:   containerd://f85e833716a254f9e981ebf6c0f432edab366aacdfa74cc46b84904e6afc8760 Image:          nginx     &gt; Image ID:       docker.io/library/nginx@sha256:859ab6768a6f26a79bc42b231664111317d095a4f04e4b6fe79ce37b3d199097 Port:           none Host Port:      none State:          Running Started:      Sat, 23 Apr 2022 22:57:23 +0100 Ready:          True Restart Count:  0 Environment:    none Mounts:       &gt; /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hl4hc (ro) Conditions: Type              Status Initialized       True Ready             True ContainersReady   True PodScheduled      True Volumes: kube-api-access-hl4hc:     &gt; Type:                    Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds:  3607 ConfigMapName:           kube-root-ca.crt ConfigMapOptional:       nil DownwardAPI:             true QoS Class:                   BestEffort Node-Selectors:              none Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s                              &gt; node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type    Reason     Age   From               Message   ----    ------     ----  ----               ------- Normal  Scheduled  33s   default-scheduler  Successfully assigned default/nginx-6799fc88d8-7k4kd to lima-rancher-desktop Normal  Pulling    33s   kubelet            Pulling image \"nginx\" Normal  Pulled     17s   kubelet            Successfully pulled image \"nginx\" in 16.398944871s Normal  Created    17s   kubelet            Created container nginx Normal  Started    16s   kubelet            Started container nginx</p> <pre><code>kubectl get pods\n</code></pre> <p>NAME                     READY   STATUS    RESTARTS   AGE nginx-6799fc88d8-7k4kd   1/1     Running   0          44s</p> <pre><code>kubectl get pods -o wide\n</code></pre> <p>NAME                     READY   STATUS    RESTARTS   AGE   IP          NODE                   NOMINATED NODE   READINESS GATES nginx-6799fc88d8-7k4kd   1/1     Running   0          57s   10.42.0.9   lima-rancher-desktop   none           none</p> <pre><code>kubectl exec -it nginx-6799fc88d8-7k4kd /bin/sh\n</code></pre> <p>kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future &gt; &gt; version. Use kubectl exec [POD] -- [COMMAND] instead.</p> <pre><code>curl 10.42.0.9\n</code></pre> <p>Welcome to nginx! html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; }</p> <p>Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to nginx.org Commercial support is available at nginx.com</p> <p>Thank you for using nginx.</p> <pre><code>kubectl get pods\n``\n\n&gt; NAME                     READY   STATUS    RESTARTS   AGE\n&gt; nginx-6799fc88d8-7k4kd   1/1     Running   0          2m45s\n\nThese steps are verbatim from [this youtube video](https://www.youtube.com/watch?v=LwE8IA3glE4).\n\nFrom this initial run through we can assume these basic concepts:\n\n![K8 Concepts](img/k8-concepts.png)\n\n## Deploying Rancher on Rancher Desktop\n\nI remember that Dominic explained about namespaces in his initial walk through so let me try to create a new  namespace.\n\n```bash\nkubectl create namespace tim\n</code></pre> <p>While trying to figure out how to list my namespaces, I found this nice k8 cheatsheet.</p> <p>The above cheatsheet didnt actually contain the tip I needed but a bit of googling came up with this:</p> <pre><code>kubectl get namespaces --show-labels\n</code></pre> <p>NAME              STATUS   AGE     LABELS default           Active   141m    kubernetes.io/metadata.name=default kube-system       Active   141m    kubernetes.io/metadata.name=kube-system kube-public       Active   141m    kubernetes.io/metadata.name=kube-public kube-node-lease   Active   141m    kubernetes.io/metadata.name=kube-node-lease tim               Active   3m34s   kubernetes.io/metadata.name=tim</p> <p>You can see my tim namespace listed as the last entry there. So based on doing that, I think I can update my concept diagram to look like this:</p> <p></p> <pre><code>kubectl\n</code></pre> <p>apiVersion: v1 kind: Pod metadata: name: nginxpod namespace: tim labels: name: nginxpod spec: containers:</p> <ul> <li>name: web     image: nginx</li> </ul> <p>I saved the above as nginx.yml and was able to run it like this:</p> <pre><code>kubectl apply -f nginx.yml\n</code></pre> <p>Then I could check in the tim namespace to see if it was running:</p> <pre><code>kubectl get pods -A\n</code></pre> <p>NAMESPACE     NAME                                      READY   STATUS      RESTARTS   AGE kube-system   local-path-provisioner-84bb864455-dsv47   1/1     Running     0          150m kube-system   helm-install-traefik-crd--1-xrhvf         0/1     Completed   0          150m kube-system   svclb-traefik-p9zwj                       2/2     Running     0          150m kube-system   helm-install-traefik--1-m2r2x             0/1     Completed   1          150m kube-system   coredns-96cc4f57d-5bzj8                   1/1     Running     0          150m kube-system   traefik-56c4b88c4b-mpwfm                  1/1     Running     0          150m kube-system   metrics-server-ff9dbcb6c-6gzt5            1/1     Running     0          150m default       nginx-6799fc88d8-7k4kd                    1/1     Running     0          69m tim           nginxpod                                  1/1     Running     0          14s</p> <p>We can see my nginx pod in my namespace as the last entry.</p>"},{"location":"devops/infrastructure/personal_infrastructure/#exit","title":"exit","text":""},{"location":"devops/infrastructure/personal_infrastructure/#installing-rancher-on-rancher-desktop","title":"Installing Rancher on Rancher Desktop","text":"<p>I went here for instructions.</p> <pre><code>helm repo add rancher-latest https://releases.rancher.com/server-charts/latest\nkubectl create namespace cattle-system\nhelm install rancher rancher-latest/rancher   --namespace cattle-system   --set hostname=crest   --set replicas=1 --set ingress.tls.source=secret\n</code></pre> <p>A little note here: the above tutorial provides different pathways to get a certificate. I am using ingress.tls.source=secret because I am just running on my local sytstem. In production you probably want to use a different option. Also I reduced replicas to 1 since I only have 1 pod in my local test environment.</p> <p>After running, I got a nice message saying rancher is setting itself up:</p> <p>NAME: rancher LAST DEPLOYED: Sun Apr 24 11:25:45 2022 NAMESPACE: cattle-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES:</p> <p>Rancher Server has been installed.</p> <p>NOTE: Rancher may take several minutes to fully initialize. Please standby while Certificates are being issued, Containers are started and the Ingress rule comes up.</p> <p>Check out our docs at https://rancher.com/docs/</p> <p>If you provided your own bootstrap password during installation, browse to https://crest to get started.</p> <p>If this is the first time you installed Rancher, get started by running this command and clicking the URL it generates:</p> <p>echo https://crest/dashboard/?setup=$(kubectl get secret --namespace cattle-system bootstrap-secret -o go-template='{{.data.bootstrapPassword|base64decode}}')</p> <p>To get just the bootstrap password on its own, run:</p> <p>kubectl get secret --namespace cattle-system bootstrap-secret -o go-template='{{.data.bootstrapPassword|base64decode}}{{ \"\\n\" }}'</p> <p>Happy Containering! ```</p> <p>Let's use our experience from the simple nginx deployment to see what is running on the system now:</p> <pre><code>kubectl get pods -A\n</code></pre> <p>NAMESPACE                   NAME                                      READY   STATUS              RESTARTS      AGE kube-system                 helm-install-traefik-crd--1-xrhvf         0/1     Completed           0             13h kube-system                 helm-install-traefik--1-m2r2x             0/1     Completed           1             13h kube-system                 svclb-traefik-p9zwj                       2/2     Running             2 (10m ago)   13h kube-system                 local-path-provisioner-84bb864455-dsv47   1/1     Running             1 (10m ago)   13h kube-system                 coredns-96cc4f57d-5bzj8                   1/1     Running             1 (10m ago)   13h tim                         nginxpod                                  1/1     Running             1 (10m ago)   11h default                     nginx-6799fc88d8-7k4kd                    1/1     Running             1 (10m ago)   12h kube-system                 traefik-56c4b88c4b-mpwfm                  1/1     Running             1 (10m ago)   13h kube-system                 metrics-server-ff9dbcb6c-6gzt5            1/1     Running             1 (10m ago)   13h cattle-system               rancher-6448c4dcdf-8wpsk                  1/1     Running             0             3m37s cattle-fleet-system         gitjob-cc9948fd7-jxgg5                    1/1     Running             0             44s cattle-fleet-system         fleet-controller-5746685958-f4rx5         1/1     Running             0             44s cattle-system               helm-operation-zfbfq                      0/2     Completed           0             68s cattle-system               helm-operation-5sg9s                      0/2     Completed           0             16s cattle-system               helm-operation-n6ggh                      2/2     Running             0             10s cattle-fleet-local-system   fleet-agent-6c6c8c45f8-vtbnm              0/1     ContainerCreating   0             7s cattle-system               rancher-webhook-6958cfcddf-z9rxr          0/1     ContainerCreating   0             5s</p> <p>We can see various jobs are still spinning up in the cattle-system.</p> <p>Next I went on a little detour on creating a self signed certificate that I can install in my rancher instance.</p> <pre><code>openssl req -new -newkey rsa:4096 -x509 -sha256 -days 365 \\\n-nodes -out tls.crt -keyout tls.key\n</code></pre> <p>Note: I believe it is required to name the key tls.* so as to match the secret name.</p> <p>Which outputs this:</p> <p>Generating a RSA private key writing new private key to 'tls.key' You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. Country Name (2 letter code) [XX]:pt State or Province Name (full name) []: Locality Name (eg, city) [Default City]: Organization Name (eg, company) [Default Company Ltd]: Organizational Unit Name (eg, section) []: Common Name (eg, your name or your server's hostname) []: Email Address []:tim@kartoza.com </p> <p>Then we have two certs in our directory:</p> <pre><code>ls\n</code></pre> <p>tls.crt  tls.key  nginx.yml</p> <p>Then on this rancher page, I followed these notes to install my cert:</p> <pre><code>kubectl -n cattle-system create secret tls tls-rancher-ingress \\\n  --cert=tls.crt \\\n  --key=tls.key\nsecret/tls-rancher-ingress created\n</code></pre> <p>Ok then back to the main thread of the rancher installation tutorial I continued:</p> <pre><code>kubectl -n cattle-system rollout status deploy/rancher\n</code></pre> <p>Which returns this:</p> <p>deployment \"rancher\" successfully rolled out</p>"},{"location":"devops/infrastructure/personal_infrastructure/#testing-it-out","title":"Testing it out","text":"<p>The instructions say to open the host in your browser (in my case I used my local hostname of crest), but nothing opened.</p> <p>I took a look in rancher desktop and played with the port forwarding. The default install looked like this:</p> <p></p> <p>So I went ahead and tried to forward that rancher port:</p> <p></p> <p>Then tried to open https://localhost:37443/</p> <p>Which gave me an error:</p> <p></p> <p>The thing seems to be that you need to rather forward this port:</p> <p></p> <p>Then I was able ot open the site (different port number now) an set up my credentials following the hints provided.</p> <p>Note: Since I am using a self signed cert I had to do the normal firefox security warning process to proceed to the site.</p> <p></p> <p>I did however still have some errors now showing in rancher:</p> <p></p>"},{"location":"devops/infrastructure/personal_infrastructure/#deploying-a-small-application-from-a-helm-chart","title":"Deploying a small application from a helm chart","text":"<p>I know helm charts are like the package managers of kubernetes, but I am not familiar with them, so I went to make a small test with the filebrowser.org (awesome app btw). I found these instructions and so ran and deployed it like this:</p> <pre><code>helm repo add utkuozdemir https://utkuozdemir.org/helm-charts\nhelm install my-release utkuozdemir/filebrowser\nexport POD_NAME=$(kubectl get pods kubernetes.io/name=filebrowser,app.ease\" -o jsonpath=\"{.items[0].metadata.name}\nexport CONTAINER_PORT=$(kubectl get pod E -o jsonpath=\"{.spec.containers0].ports[0]. \necho \"Visit http://127.0.0.1:8080 to use \nkubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT\n</code></pre> <p>From this I learned a couple of things:</p> <ol> <li>We can pull helm charts from the internet with a system of repos</li> <li>We can use xpath style queries to parse out bits of info from kubectl</li> <li>The last line gives some hints about how to foward traffic out of k8</li> </ol>"},{"location":"devops/infrastructure/rancher-k3s-single-node/","title":"Using Rancher, K3S, and Longhorn for single node Kubernetes deployment","text":"<p>Prerequisites: This process assumes the use of a clean Ubuntu 20.04 server with a public IP address and an authorized key recognised by the server in the current users ~/.ssh.</p>"},{"location":"devops/infrastructure/rancher-k3s-single-node/#rancher-deployment-process","title":"Rancher Deployment Process","text":"<p>Get the docker-ansible project to run an ansible controller in a docker container</p> <pre><code>user@dev$ git clone https://github.com/kartoza/docker-ansible.git\n</code></pre> <p>Build the container image</p> <pre><code>user@dev$ cd docker-ansible\n</code></pre> <pre><code>user@dev$ docker build . -t ansible\n</code></pre> <p>Replace the remote IP address in the inventory file</p> <pre><code>user@dev$ echo \"&lt;server-ip&gt; ansible_user=root ansible_private_key_file=/root/.ssh/id_ed25519\" &gt; ${PWD}/ansible/inventory/hosts.ini\n</code></pre> <p>Remove the default playbooks directory</p> <pre><code>user@dev$ rm -r ./ansible/playbooks\n</code></pre> <p>Fetch the playbook repository which includes the singlenode rancher deployment</p> <pre><code>user@dev$ cd ./ansible &amp;&amp; git clone https://github.com/kartoza/playbooks.git\n</code></pre> <p>Go back to the repository root</p> <pre><code>user@dev$ cd /..\n</code></pre> <p>Run the docker container (Powershell/ Windows users should read the docker-ansible readme)</p> <pre><code>user@dev$ docker run -dt -v $PWD/ansible:/ansible \\\n               -v ~/.ssh:/root/win-ssh:ro \\\n               -e ANSIBLE_CONFIG=/ansible/ansible.cfg \\\n               --restart=unless-stopped \\\n               --name ansible ansible\n</code></pre> <p>Once the container is running run the playbook</p> <pre><code>user@dev$ docker exec -it ansible ansible-playbook /ansible/playbooks/ubuntu20.04/app/rancher-singlenode.yaml\n</code></pre> <p>Save the logs and kill off the ansible container</p> <pre><code>user@dev$ docker logs ansible &gt;&gt; $PWD/ansible_log.txt &amp;&amp; docker rm -f ansible\n</code></pre> <p>At this point, the server should be configured to use <code>https://rancher.&lt;server-ip&gt;.sslip.io</code>. By default the admin user is <code>admin</code> and the password is <code>chickensoup</code> but that can be changed in the <code>rancher-singlenode.yaml</code> file. Note self-signed TLS is enabled so there'll be browser notices, but TLS configuration is environment specific and not considered in the scope of this process definition.</p>"},{"location":"devops/infrastructure/rancher-k3s-single-node/#application-deployment-process","title":"Application Deployment Process","text":"<p>Now the project workload will need to be deployed to the server. This will use kubectl and can be done in various ways. If you wish to deploy from the local machine, the kubectl configuration for each node should be available in the <code>ansible/data</code> directory.</p> <p>For the purpose of demonstrating this process, it is assumed that this is done by using ssh to connect to the server as the root user, which should have the kubectl configuration already setup by the ansible deployment.</p> <pre><code>user@dev$ ssh root@&lt;server-ip&gt;\n</code></pre> <p>Clone the project repository</p> <pre><code>root@app-srv$ mkdir /manifests &amp;&amp; cd /manifests &amp;&amp; git clone https://gitlab.com/zacharlie/test-manifest.git\n</code></pre> <p>Replace the hostname field in the ingress with the fully qualified domain name of the server.</p> <pre><code>root@app-srv$ cp /manifests/test-manifest/ingress.yaml /manifests/test-manifest/ingress.yaml.bak &amp;&amp; awk '/nginx.test.localhost/ {sub(\"nginx.test.localhost\", \"123.456.7.89.sslip.io\", $0)} {print}' /manifests/test-manifest/ingress.yaml.bak &gt; /manifests/test-manifest/ingress.yaml\n</code></pre> <p>Apply the project resources using kustomize</p> <pre><code>root@app-srv$ cd /manifests/test-manifest &amp;&amp; kubectl apply -k .\n</code></pre> <p>Be patient and monitor the resources from the nifty rancher dashboard (Workload &gt;&gt; Pods is a good place to start). Once everything is running you may view the landing page at the hostname address that was configured within the ingress configuration.</p> <p></p>"},{"location":"devops/procedures/","title":"DevOps Procedures","text":"<p>DevOps tasks are handled by the DevOps board</p>"},{"location":"devops/security/","title":"Security","text":"<ul> <li>Cyber Hygiene Handbook</li> <li>Security Guidelines/ Rules</li> <li>Development Processes</li> <li>External Resources</li> </ul>"},{"location":"devops/security/#cyber-hygiene-handbook","title":"Cyber Hygiene Handbook","text":"<p>'twould be handy to collect resources and advice on training users in best practices for cyber-hygiene (applies in broad strokes to staff, trainees, community, and more). Compiled into a nice concise resource with pretty pictures and minimal complexity++</p> <p>Topics and structure could be outlined as follows:</p> <p>Users:</p> <ul> <li>Identifying phishing attacks</li> <li>Credential management</li> </ul> <p>Admins:</p> <ul> <li>Identifying whats on the network</li> <li>Account management</li> <li>Staying up to date</li> </ul> <p>Developers:</p> <ul> <li>Authentication management</li> <li>Dependency management</li> <li>Scanning</li> <li>SSH, SSL, SSO (lots of SS), Cryptography, Hashes, Salts</li> </ul>"},{"location":"devops/security/links/","title":"Security: Useful Links","text":"<p>A bunch of useful external resources related to security.</p>"},{"location":"devops/security/links/#articles","title":"Articles","text":"<ul> <li>https://security-list.js.org/#/</li> <li>https://www.sentinelone.com/blog/practice-these-10-basic-cyber-hygiene-tips-for-risk-mitigation/</li> </ul>"},{"location":"devops/security/links/#tools","title":"Tools","text":"<p>Projects which are good for security.</p> <ul> <li>https://keepass.info/</li> <li>https://bitwarden.com/</li> <li>https://pypi.org/project/xkcd-pass/</li> <li>https://snyk.io/</li> <li>https://docs.docker.com/engine/scan/</li> <li>https://github.com/trailofbits/pip-audit</li> <li>https://github.com/trufflesecurity/trufflehog</li> <li>https://github.com/danielmiessler/SecLists</li> </ul>"},{"location":"gis/","title":"GIS","text":"<p>Resources, information, and processes related to Geographic Information Systems</p> <p>Note that all users, regardless of role, should understand and review the security section.</p> <p>GIS is a core competency at Kartoza. All technical staff should endeavour to ensure that they have or develop some level of proficiency in GIS.</p> <ul> <li>Cartography</li> <li>Technologies</li> <li>Resources</li> </ul>"},{"location":"gis/cartography/","title":"Cartography","text":"<p>Guidelines on how to create effective cartography.</p> <p>Information on the development of cartographic style guides.</p>"},{"location":"gis/cartography/cartography-guidelines/","title":"Cartography Guidelines","text":"<p>This section is a collection of rules and mantras used by Kartoza to make beautiful and effective cartography.</p>"},{"location":"gis/cartography/cartography-guidelines/#tims-three-principles","title":"Tim's three principles","text":"<p>In the above image we can see a framework for you to use to develop your cartography skills. There are three principles you should understand in order to make great cartographic products. The principles are an adaptable construct designed to help you make maps which respect your user's intelligence in order to surprise, excite and delight them.</p> <ol> <li>Process: This is definition of a standardised, rigorous set of steps that you yourself should follow, along with your co-workers, and adhere to when developing your product. Process is the unexciting stuff that happens in the background. Your process should be defined as a nested list of statements. The best way to explain is by an example:<ol> <li>All map fonts should be at least or larger than 6pt.</li> <li>All map symbols should be at least or larger than 3mm.</li> <li>All third party map symbols should be available and published under an open license.</li> <li>.... etc. .... The purpose of the development process is to have the creation of your map repeatable from one end to another, and to be able to work collaboratively with colleages and clients in a consistent way.</li> </ol> </li> <li>Technical Skill: This is the process whereby you learn the capabilities of the software (e.g. QGIS) as thorough as possible in order to be able to articulate any idea you or others may have as a cartographic expression. Technical skill is a process of continual learning and acquiring it should commence with the development of a high level framework of the cartographic system and then progress with a deeper and deeper understanding of all of the constituent parts.</li> <li>Creativity: This is the hardest (and sometimes impossible) principle to imbue in a cartographer. When you lack personal creativity to come up with great map designs, you should (ethically) build on the work of others through emulation. By ethically we mean, crediting the inspiration of your designs to the correct person, asking permission when needed, and being sensitive to the livelihood and reputation for whoever you derive your work from. When you do have a creative side, you should be using it to inform the process and learning of technical skills so that you can articulate your vision in the cartography you produce.</li> </ol> <p>We should stress that all rules you or others create can be broken if they are broken with intention. That is to say, if you decide a 3 point font symbol is critical for the artistic or technical vision, you should not be afraid to make such a decision. What we want to avoid is unintentional breaking of your own rules as it will lead to a less professional looking product.</p> <p>The relative size of your circles for process versus creative versus technical abilities will likely differ widely. Your job is to strive in your personal journey as a cartographer to bring them into good proportion and to grow the areas which are under developed. There is a lot of theory out there on cartographic design, but I really recommend reading this wikipedia page: https://en.wikipedia.org/wiki/Cartographic_design and also reading the QGIS Map Design book by Anita Graser and Gretchen Peterson (available at https://locatepress.com/book/qmd2). .</p>"},{"location":"gis/cartography/cartography-guidelines/#cartographic-map-design","title":"Cartographic Map Design","text":""},{"location":"gis/cartography/cartography-guidelines/#reference--cartography-in-map-design","title":"Reference : Cartography in Map Design","text":"<p>Definition. Cartographic design or map design is the process of crafting the appearance of a map, applying the principles of design and knowledge of how maps are used to create a map that has both aesthetic appeal and practical function. It shares this dual goal with almost all forms of design; it also shares with other design, especially graphic design, the three skill sets of artistic talent, scientific reasoning, and technology. As a discipline, it integrates design, geography, and geographic information science.</p>"},{"location":"gis/cartography/cartography-guidelines/#types-of-maps","title":"Types of Maps","text":"<p>According to the ICSM (Intergovernmental Committee on Surveying and Mapping), there are five different types of maps namely: General Reference, Topographical, Thematic, Navigation Charts and Cadastral Maps and Plans.</p>"},{"location":"gis/cartography/cartography-guidelines/#the-cartographic-design-process","title":"The Cartographic Design Process","text":"<p>As map production and reproduction technology has advanced, the process of designing and producing maps has changed considerably. Most notably, GIS and graphics software not only makes it easier and faster to create a map, but it facilitates a non-linear editing process that is more flexible than in the days of manual cartography. There is still a general procedure that cartographers generally follow:</p> <ol> <li>Planning: Typically, this involves answering several questions:</li> </ol> Question Answer What is the purpose of the map? Maps serve a wide variety of purposes; they may be descriptive (showing the accurate location of geographic features to be used in a variety of ways, like a street map), exploratory (showing the distribution of phenomena and their properties, to look for underlying patterns and processes, like many thematic maps), explanatory (educating the audience about a specific topic), or even rhetorical (trying to convince the audience to believe or do something). Who is the audience? Maps will be more useful if they cater to the intended audience. This audience could range from the cartographer herself (desiring to learn about a topic by mapping it), to focused individuals or groups, to the general public. Several characteristics of the audience can aid this process, if they can be determined, such as: their level of knowledge about the subject matter and the region being covered; their skill in map reading and understanding of geographic principles (e.g., do they know what 1:100,000 means?); and their needs, motivations and biases. Is a map the best solution? There are times when a map could be made, but a chart, photograph, text, or other tool may better serve the purpose. What datasets are needed? The typical map will require data to serve several roles, including information about the primary purpose, as well as supporting background information. What medium should be used? Different mapping media, such as posters, brochures, folded maps, page maps, screen displays, and web maps have advantages and disadvantages for different purposes, audiences, and usage contexts. <ol> <li> <p>Data Collection: In the era of Geographic information systems, it seems like vast amounts of data are available for every conceivable topic, but they must be found and obtained. Frequently, available datasets are not perfect matches for the needs of the project at hand, and must be augmented or edited. Also, it is still common for there to be no available data on the specific topic, requiring the cartographer to create them, or derive them from existing data using GIS tools.</p> </li> <li> <p>Design and Implementation: This step involves making decisions about all of the aspects of map design, as listed below, and implementing them using computer software. In the manual drafting era, this was a very linear process of careful decision making, in which some aspects needed to be implemented before others (often, projection first). However, current GIS and graphics software enables interactive editing of all of these aspects interchangeably, leading to a non-linear, iterative process of experimentation, evaluation, and refinement.</p> </li> <li> <p>Production and Distribution: The last step is to produce the map in the chosen medium, and distribute it to the audience. This could be as simple as a desktop printer, or sending it to a press, or developing an interactive Web Mapping Site.</p> </li> </ol>"},{"location":"gis/cartography/cartography-guidelines/#design-goals","title":"Design Goals","text":"<p>While maps serve a variety of purposes, and come in a variety of styles, most designs share common goals. Some of the most commonly stated include:</p> Goal Description Accuracy, the degree to which the information on the map corresponds to the nature of the real world. Traditionally, this was the primary determinant of quality cartography. It is now accepted, due largely to studies in Critical Cartography, that no dataset or map is a perfect reproduction of reality, and that the subjective biases and motivations of the cartographer are virtually impossible to circumvent. That said, maps can still be crafted to be as accurate as possible, honest about their shortcomings, and leverage their subjectivity. Functionality, the usefulness of the map to achieve its purpose. During much of the latter 20th century, this was the primary goal of academic cartography, especially the Cartographic Communication school of thought: to determine how to make the most efficient maps as conduits of information. Clarity, the degree to which the map makes its purpose obvious and its information easy to access. Clarity can be achieved through removing all but the most important information, but this comes at the expense of other goals. Richness, the volume and diversity of information the reader can glean from the map. Even maps with a narrowly-defined purpose often require the reader to see patterns in large amounts of data. Aesthetic appeal, a positive emotional reaction to the overall appearance of the map. Maps may be appreciated as \"beautiful,\" but other positive affects include \"interesting,\" \"engaging,\" \"convincing,\" and \"motivating.\" Aesthetic reactions can be negative as well, such as \"ugly,\" \"cluttered,\" \"confusing,\" \"complicated,\" \"annoying,\" or \"off-putting.\" <p>These goals often seem to be in conflict, and it may be tempting to prioritize one over the others. However, quality design in cartography, as in any other design field, is about finding creative and innovative solutions to achieve multiple goals. According to Edward Tufte,</p> <p>What is to be sought in designs for the display of information is the clear portrayal of complexity. Not the complication of the simple; rather the task of the designer is to give visual access to the subtle and the difficult--that is, the revelation of the complex.</p> <p>In fact, good design can produce synergistic results. Even aesthetics can have practical value: potential map users are more likely to pick up, and more likely to spend time with, a beautiful map than one that is difficult to look at. In turn, the practical value of maps has gained aesthetic appeal, favoring those that exude a feeling of being \"professional,\" \"authoritative,\" \"well-crafted,\" \"clear,\" or \"informative.\" In 1942, cartographer John K. Wright said,</p> <p>An ugly map, with crude colors, careless line work, and disagreeable, poorly arranged lettering may be intrinsically as accurate as a beautiful map, but it is less likely to inspire confidence.</p> <p>Rudolf Arnheim, an art theorist, said this about the relationship between maps and aesthetics in 1976:</p> <p>The aesthetic or artistic qualities of maps are sometimes thought to be simply matters of so-called good taste, of harmonious color schemes and sensory appeal. In my opinion, those are secondary concerns. The principal task of the artist, be he a painter or a map designer, consists of translating the relevant aspects of the message into the expressive qualities of the medium in such a way that the information comes across as a direct impact of perceptual forces. This distinguishes the mere transmission of facts from the arousal of meaningful experience.</p> <p>More recently, cartographers have recognised the central role of aesthetics in cartographic design and called for greater focus on how this role functions over time and space. For example, in 2005, Dr Alex Kent (former President of the British Cartographic Society) recommended:</p> <p>It will thus be more useful to cartographers and the development of cartography in general to undertake further research towards understanding the role of aesthetics in cartography than to pursue universal principles. Some possible topics for investigation include:</p> <ol> <li>A history of the development of aesthetics in cartography;</li> <li>An exploration of geographical variations in cartographic aesthetics; and</li> <li>A critical examination of the factors influencing aesthetic decisions in contemporary mapmaking. </li> </ol>"},{"location":"gis/cartography/cartography-guidelines/#steps-to-preparing-a-map","title":"Steps to preparing a map","text":"<ol> <li>Choose your locality. Perhaps you have a client or an assignment which will determine the place. Or perhaps you have the freedom to choose the place yourself.</li> <li>Choose the scale for your map. Again this may be client driven based on a specification or you might have the freedom to choose yourself. But decide upfront what scale you will view the map at as it will influence all your decisions as you develop your map. In some cases you may be asked to make a variable scale product e.g. for a web map that can be zoomed in and out. If you are producing a variable scale map, determine which scale ranges and intervals will be used. For example, OpenStreetMap defines standard scale intervals which are used by many web mapping toolkits. If you are able to choose your own scale, try to choose a humanistic scale e.g. 1:500000, 1:5000 etc.</li> <li>Assuming you are preparing a fixed scale map, define the extent of your map. This may be determined by the print page size or by client factors. In some cases, the size of the print medium may influence the scale of your map. For example, you may be printing to an A4 map and need to choose a scale that allows you to see the whole town or area of interest.</li> <li>Determine which data layers to feature in your map. Some typical examples will be roads, buildings, rivers, water bodies, points of interest etc. Make a list and then go out and procure your data. For each dataset your find, keep note of the following details in a spreadsheet or notes file:<ol> <li>Date of download</li> <li>Source (name of person or organisation)</li> <li>Attribution (citation for the data)</li> <li>URL</li> <li>Notes</li> </ol> </li> <li>Determine the purpose of your map. Every map has a story to tell. Maybe you want to show a tourist around your city? Perhaps it is a narrative about crime or socio-economics. Try to create a short narrative like you were telling a friend about visiting the place in your map to create a sense of 'feel' about the place.</li> <li>Plan your colour palette. Overwhelming your reader with many, jarring colours won't help them to interpret your map well, nor will it make for an aesthetically pleasing experience. If you are not good at working with colours, find a photo of a key feature of the area and lift colours out of the image until you have about 8 to 10 colours of different hues and intensities. Or check out one of the many online sites like coolors.co .</li> <li>Choose a style. There are many amazing cartographic styles out there. Do you want something technical? Cartoony? Medieval looking? Find an example of a similar style to what you have in mind and look at what visual elements they use to cartography style.  - Medieval Mappa Mundi map from the 1300s (source - Wikipedia)</li> <li>Plan your layout. If you are making a print map, it can be useful to roughly plan where all the elements such as map, legend, photos or graphics e.t.c. will appear on your layout. That will influence some decisions in your cartography and will help you to organise your work plan.</li> <li>Create a set of guidelines. Create some rules for yourself to follow. Life gets a lot easier when you have a framework to construct your work on than when you are just 'winging it'.</li> <li>Use the QGIS QuickOSM Plugin to quickly establish a baseline set of layers for your map. .</li> </ol>"},{"location":"gis/cartography/cartography-guidelines/#map-layout","title":"Map Layout","text":"<p>A typical map, whether on paper or on a web page, consists of not only the map image, but also other elements that support the map:</p> <p>A title tells the reader what the map is about, including the purpose or theme, and perhaps the region covered. A legend or key explains the meaning of the symbols on the map A neatline may frame the entire map image, although many maps use negative space to set the map apart A compass rose or north arrow provides orientation Inset maps may serve several purposes, such as showing the context of the main map in a larger area, showing more detail for a subset of the main map, showing a separated but related area, or showing related themes for the same region. A bar scale or other indication of scale translates between map measurements and real distances. Illustrations may be included to help explain the map subject or add aesthetic appeal. Explanatory text may discuss the subject further Metadata declares the sources, date, authorship, projection, or other information about the construction of the map.</p> <p>Composing and arranging all of the elements on the page involves just as much design skill and knowledge of how readers will use the map as designing the map image itself. Page composition serves several purposes, including directing the reader's attention, establishing a particular aesthetic feel, clearly stating the purpose of the map, and making the map easier to understand and use. Therefore, Page layout follows many of the same principles of Composition above, including figure-ground and Visual hierarchy, as well as aesthetic principles adopted from Graphic design, such as balance and the use of White space (visual arts). In fact, this aspect of cartographic design has more in common with graphic design than any other part of the craft.</p>"},{"location":"gis/cartography/cartography-guidelines/#mantras","title":"Mantras","text":"<ol> <li>The user should not be exposed to the inner workings of the system. Which is guideline we try to avoid asking the user to perform activities that presume or require that they have inside knowledge of how the system works. Where breaking this guideline is unavoidable, such tasks should be well described so as to allow the user to carry out the task with the least possible friction.</li> <li>If you use somebody else's work, they should be attributed unless they do not require it.</li> <li>Get peer review of your work early and often.</li> <li>Keep to the topic of your creation.</li> <li>Speak to your audience. Think of your work from the user's point of view and create something they can relate to.</li> <li>Follow the principle of separation of concerns. Users should have a clear understanding of where they are in a workflow, being presented with information which is thematic to the task at hand, whilst non-relevant questions or information should be hidden.</li> <li>Use emphasis sparingly. If everything is emphasised, nothing is emphasised! Some things that provide emphasis are .</li> <li>Layout visual elements in a consistent way in terms of size, colour and spacing. Break this rule with intention. .</li> <li>Tell a story!</li> <li>Make sure the user understands why they are here.</li> <li>Explicit is better than implicit. Don't assume your users know about simple things like north direction etc.</li> <li>Design the system to be easy to maintain. In this case, the map should avoid introducing unneeded functionality, using unneeded infrastructure, include obfuscated or overly complex terms or visual imagery. To achieve this, we should strive for simplicity and clarity in the user experience.</li> <li>Don't present data, present insights.</li> <li>Pay extreme attention to detail to create a professional product.</li> <li>Reduce reduce reduce - prune the content until it has the minimum number of elements needed to convey your message.</li> <li>Make rules for yourself and follow them closely. Break your rules with intention. Create a consistent visual and interactive metaphor and using it throughout the system if possible.</li> <li>\"Obey the principle of least surprise\". Design to reduce cognitive friction.</li> <li>Surpise and delight your user.</li> <li>There is no such thing as cheating, only finding creative solutions to your problems.</li> <li>Whenever possible, do things for the user.</li> <li>Do not ever underestimate your capabilities to overcome any problem; if there is a will, there surely is a way! (credit: Namrata Karki)</li> </ol>"},{"location":"gis/cartography/cartography-guidelines/#choosing-a-coordinate-reference-system-for-your-map","title":"Choosing a Coordinate Reference System for your map","text":"<p>The order of preference for selection of a CRS for your map should be determined by the intended purpose and by client requirements:</p> Scale CRS Type CRS Notes Global Spherical IAU_2015:39965 Orthographic   Use for inset maps or contextual maps. Create a customised CRS based on this to move the LO origin. Global Cylindrical EPSG:8857 Equal Earth  Use for flat maps of the world. Represents country sizes fairly. See The Equal Earth Website. Any Spherical EPSG:3857 Spherical  Use for maps destined for the web or that incorporate data from online sources like MapTiler or OSM tiles. Regional Conic EPSG:102024 Lambert  Use for maps covering a large East-West area. For example spanning 3 or 4 UTM zones. Local Mercator EPSG:32XXXX UTM  Use for maps covering small, local areas. For example spanning  1 or less UTM zones."},{"location":"gis/cartography/cartography-guidelines/#cartography-resources","title":"Cartography Resources","text":"Name Description Oxford AI Textures Use for raster fill textures. License unknown. QGIS Styles A collection of re-usable QGIS styles you can use to level up your cartography and learn how to produce interesting cartography. QGIS Example Projects A collection of QGIS projects which provide reference examples of a number of different GIS workflows in QGIS. QGIS Example Processing Models A collection of processing models which solve different geospatial analysis problems. QGIS Example 3D Models A collection of 3d Objects you can use as point symbol markers for your 3D Scenes. QGIS .lyr Files A collection of QGIS .lyr files with useful datasets that you can add to your QGIS projects. Google Fonts A large collection of fonts you can use in your projects. We also deploy these fonts in OSGS too so if you make a web map they will be available there. GNU Fonts Another large set of free fonts you can use in your project."},{"location":"gis/cartography/cartography-guidelines/#common-issues-in-cartography","title":"Common issues in cartography","text":"No. Before Description After 1 Don't let labels overlap features. 2 Make sure that major roads cover minor roads. 3 1\ufe0f\u20e3 Set your canvas scale to the scale you plan to print at and 2\ufe0f\u20e3 lock it. When you 3\ufe0f\u20e3 zoom in and out with your scroll wheel it will zoom into pixels at the fixed scale. 4 Don't include roads on your map that end abruptly unless this indeed reflects reality. 5 Generally, you should use a round cap style for your roads to avoid issues of the road cap extending beyond intersecting roads. 6 As a general rule, points should be drawn above polygon and line features. 7 Use text that matches the language of your map. 8 Use symbol layers to place a solid background behind your symbols if they are transparent and are getting lost in the background of your map. Alternatively, make sure your symbols are darker than your background colours to make sure they are visible and not obstructed. 9 Do not use transparency inappropriately. Features like rivers should usually be rendered with a solid fill. 10 Avoid visual clutter on your map. If there are too many features visible with similar contrast your user will quickly become overwhelmed. 11 Avoid overlapping features that don't naturally overlap. 12 Use layer symbol levels for multilayer symbols to prevent elements of the style from overlapping. 13 Don't use legend items that are not meaningful. Make sure that your spelling is correct and that there are no underscores or dashes in your legend items. 14 Use call-out labels when they risk covering important features on the map. 15 Don't let line or polygon features obscure point symbols. 16 Make sure your north arrow is linked to your map and is correctly positioned and sized, and is also fully visible. 17 Avoid the side effects of using transparent colours. 18 Always run your text through a spelling and grammar checker. Make sure the font you choose to use is also readable. 19 Have someone else read your prose to ensure the phrasing is clear and easy to read. 20 Learn how to split legend columns and edit the symbol sizes to create a sense of balance in your composition. 21 Don't use light colours on a light background or dark colours on a dark background. Try to keep your background colours light enough so that all of the features you want to show can be as visible as possible. You want those features to be what your eyes are drawn towards. 22 Humanize all text shown to users (see mantra about exposing users to the inner workings of the system). 23 Set up your rendering options so that your symbols are not partially drawn/clipped - unless this is by design. 24 Make sure the sizes of the elements in your map layout are balanced. Ensure that the important elements stand out more than the less important elements. Make sure your elements don't overlap unless it's intentional. 25 Make sure all elements are aligned with pixel perfect precision. 26 Ensure the white space between elements is the same to create a sense of perfect symmetry. Using a layout grid can help a lot with this. 27 Avoid repeating labels on the same line segment unless it is by intention. 28 Where relevant, provide context at the edges of your map so the user understands where they will go if they leave the map or provide an inset map so users can orientate themselves within your map. 29 Make sure that all of your labels are legible and the colours are contrasting enough not to blend into the rest of your map. Make sure your labels also have the correct orientation. Road labels should go along the road. 30 Make sure your symbols are consistent unless intentionally not consistent. Look at the size, colour and transparency of the symbols. 31 When choosing colours for your map, and even your layout, first consider what the purpose of your map is and who is going to be looking at it. Does your map need to cater for colour-blind viewers or viewers with other visual impairments? What data are you representing? Are there colours associated with the data? Does your client have a style guide representing the colours they use? Consider all of these factors when styling your data as well as when you design your map layout. 32 Be careful when adding defined outlines to your data while styling. Make sure the stroke width is not too thick or that the colour is not too pronounced. You can often get away with not having a defined stroke on your features, depending on the feature. 33 Always use CRS that is suitable for the study area. Avoid CRS that distorts your study area. 34 Avoid placing labels such that the callout crosses several other boundaries. 35 Avoid repetition and unnecessary elements such as adding the word 'Legend' to the legend in situations where it is obvious. 36 Avoid using a sequential color scheme when you have many categories since it makes it difficult to distinguish between categories, and use diverging, converging, or other schemes. For example, instead of light red to dark red, use red to blue."},{"location":"gis/resources/","title":"GIS Resources","text":"<p>Where to find internal resource collections (projects, templates, data, assets etc)</p> <p>External resource links:</p> Resource About QGIS QGIS is a free, open-source software that allows users to create, edit,visualize, analyze, and publish geospatial information. PostgreSQL and PostGIS PostgreSQL is a powerful, open-source object-relational database system that uses and extends the SQL language many features. PostGIS is a spatial database extender for PostgreSQL. National Geospatial Institute catalog National Geospatial Institute catalog for South Africa. GeoSeer A spatial data search engine with datasets made available via standards compliant geospatial web services DIVA-GIS A popular free spatial data store with datasets for the whole world. Also a program for geographic data analysis and mapping."},{"location":"gis/resources/ngi/","title":"National Geospatial Intitute data","text":"<p>National Geospatial Institute (NGI)</p>"},{"location":"gis/resources/ngi/#ngi-portal","title":"NGI portal","text":"<p>The National Geospatial Institute of South Africa has a portal available which can be used to obtain South African remotely sensed data. The data is freely available and can be accessed from here: http://www.cdngiportal.co.za/cdngiportal/.</p> <p></p> <p>The catalog provides a lot of data and functionality, but for now this quick guide is to show a user can get access to the Aerial photography in QGIS and Geoserver.</p>"},{"location":"gis/resources/ngi/#aerial-photographs","title":"Aerial photographs","text":"<ul> <li>Click on the Browse ERDAS APOLLO Catalog tab to the left</li> </ul> <ul> <li>The Aerial imagery can be found at ROOT -&gt; IMAGERY -&gt; ORTHO_RECTIFIED -&gt; WGS84 -&gt; MOSAIC -&gt; DIGITAL</li> <li>Two folders are available:</li> <li>10K_TIF: This stores individual block tiles (e.g. 3319AA, 3319AB, etc.)</li> <li>MOSAICS: Here a user can find the mosaicked rasters for South Africa. 50cm covers the entire SA, whereas 25cm only covers some provinces/areas</li> </ul> <ul> <li>Click on the MOSAICS folder</li> <li>This should show the following on the catalog:</li> </ul> <ul> <li>Note that each dataset has a WMS link. This link can be used to add the data QGIS or Geoserver, or any other software which supports WMS</li> </ul>"},{"location":"gis/resources/ngi/#qgis","title":"QGIS","text":"<p>Short section on how to add the 50cm Aerials mosaic to QGIS.</p>"},{"location":"gis/resources/ngi/#50cm-mosaic","title":"50cm Mosaic","text":"<ul> <li>In QGIS, go to Layer -&gt; Data Source Manager</li> <li>Click on the WMS/WMTS tab</li> </ul> <ul> <li>Click New to add a new connection</li> <li>Provide a desired Name</li> <li>Here is the URL for the 50cm SA mosaic: http://apollo.cdngiportal.co.za/ApolloCatalogWMSPublic/service.svc/get?version=1.3.0&amp;layers=RSA_MOSAIC_50CM_201612</li> <li>Authentication should be set to No Authentication (data is freely available!)</li> </ul> <ul> <li>Click OK</li> <li>Click Connect</li> <li>After the connection succeeded, the user should see something similar to the following:</li> </ul> <ul> <li>Click Add. This will add the layer to the QGIS canvas</li> </ul>"},{"location":"gis/resources/ngi/#50cm-tiles","title":"50cm tiles","text":"<p>Individual tiles can also be added (e.g. 3319AA). This is especially useful if a user wants to focus on a particular area of interest.</p> <ul> <li>The Aerial tiles can be found at ROOT -&gt; IMAGERY -&gt; ORTHO_RECTIFIED -&gt; WGS84 -&gt; MOSAIC -&gt; DIGITAL -&gt; 10K_TIF</li> <li>Select the folder with the desired tileset</li> <li>The user can select an entire tileset (e.g. 3319), or a subset (e.g. 3319AA)</li> <li>Copy the WMS link as in the previous section. In this case 3319 will be used as an example: http://apollo.cdngiportal.co.za/ApolloCatalogWMSPublic/service.svc/get?service=WMS&amp;version=1.3.0&amp;REQUEST=GetLayer&amp;layers=3319_2</li> <li>Add the data to the canvas</li> <li>This would only now cover the selected tileset</li> </ul>"},{"location":"gis/resources/ngi/#geoserver","title":"Geoserver","text":"<p>Data from the NGI catalog can also be added to Geoserver as a store.</p> <ul> <li>In the left panel, click on Stores</li> <li>Click Add new Store</li> </ul> <p></p> <ul> <li>Select WMS. Its at the bottom, under the Other Data Sources section</li> <li>Set the parameters as follows:</li> <li>WMS Source Name: As desired</li> <li>Description is optional</li> <li>Enabled to selected</li> <li>Capabilities URL: http://apollo.cdngiportal.co.za/ApolloCatalogWMSPublic/service.svc/get?service=WMS&amp;version=1.3.0&amp;REQUEST=GetLayer&amp;layers=RSA_MOSAIC_50CM_201612</li> <li>User Name and Password should be left empty, as the data is freely available</li> <li>Leave the remaining parameters on default</li> </ul> <p></p> <ul> <li>Click Save</li> <li>Click on Layers on the left pane</li> <li>Click Add a new layer</li> <li>Select the store that's been added for the NGI data</li> <li>Select the RSA_MOSAIC_50CM_201612 layer and publish it</li> </ul> <p>The Geoserver will now have the 50cm Aerial mosaic available for South Africa. As with QGIS, this can also be done only for certain tileset (e.g. 3319) if desired.</p>"},{"location":"gis/resources/postgres/","title":"Postgres and PostGIS Resources","text":"<p>PostgreSQL is a powerful, open-source object-relational database system that uses and extends the SQL language many features (https://www.postgresql.org/). PostGIS is a spatial database extender for PostgreSQL (https://postgis.net/).</p> Resource focus Description Service file Connection parameters for a service name. Requirement for PostgreSQL. PostgreSQL, PostGIS, and QGIS Guide which will be useful when working with PostgreSQL, PostGIS and QGIS. [Issues and workarounds](./issues(index.md) Issues a user may encounter, and workarounds."},{"location":"gis/resources/postgres/issues/","title":"PostgreSQL issues a user may encounter","text":""},{"location":"gis/resources/postgres/issues/#console-commands-not-working","title":"Console commands not working","text":"<p>Sometimes the user may encounter issues with the commands not working in the console. Here is an example:</p> <p></p> <p>This can easily be resolved by opening the console in the folder which contains the Postgres console commands: - Go to the installation folder of the Postgres you want to work with (e.g. \u201cC:\\Program Files\\PostgreSQL\\14\\\u201d). Be sure to go to the correct version - Open the \u201cbin\u201d folder - The user will note that this folder contains a long list of executables. These executables are what we want to call from the console - Open the console in this folder - The user can now run the commands they desire from here</p> <p>Here is an example of the raster2pgsql command called from within the folder</p> <p></p> <p>Take note that this workaround will also be useful when a server has multiple versions of Postgres installed.</p>"},{"location":"gis/resources/postgres/postgres_qgis/","title":"Guidelines when working with PostgreSQL, PostGIS, and QGIS","text":"<p>This guide deals with restoration of a database (DB) which contains spatial data. In such cases the DB will be making use of the PostGIS extension.</p>"},{"location":"gis/resources/postgres/postgres_qgis/#database-restoration","title":"Database restoration","text":"<p>This section deals with the restoration for a DB from a dump into a newly created postgres database.</p>"},{"location":"gis/resources/postgres/postgres_qgis/#create-a-new-database","title":"Create a new database","text":"<p>This section deals with creating a new DB using pgAdmin, with focus on PostGIS. PostGIS allows the integration of spatial data (data which contains geometries), such as vector data and raster, into a Postgres DB. If the DB into which the restoration will be done already exists, this step can be skipped.</p> <ul> <li>Right-click on Databases, select Create, and then click on Database</li> </ul> <p></p> <ul> <li>In the Create-Database dialog, name the database as desired</li> </ul> <p>Be sure to use this name performing the restoration step as the target database.</p> <ul> <li>Select the desired user as the Owner. If the user does not exist, have a look at the User list section</li> </ul> <p></p> <ul> <li>Click Save</li> <li>The DB will now be created</li> <li>The user now need to add the extensions for PostGIS and PostGIS raster</li> <li>Right-click on Extensions, select Create and click on Extension</li> </ul> <p></p> <ul> <li>Under Name type \"postgis\"</li> <li>Select postgis</li> <li>Click Save</li> <li>If the user will also be importing rasters to the DB, the following needs to be done:</li> <li>Right-click on Extensions, select Create and click on Extension</li> <li>Under Name type \"postgis_raster\"</li> <li>Select the postgis_raster extension</li> <li>Click Save</li> <li> <p>In newer versions of PostGIS this might not be required, as PostGIS already includes the raster extension</p> </li> <li> <p>The end result should be similar to the following:</p> </li> </ul> <p></p>"},{"location":"gis/resources/postgres/postgres_qgis/#user-list","title":"User list","text":"<p>When restoring a DB dump all users needs to be added/created. This is required because the  restoration process will fail if the user is not present. This section can be skipped if the users are already present on the server.</p> <ul> <li>Open pgAdmin</li> <li>Right-click Login/Group Roles, select Create, and then click on Login/Group Role</li> <li>Provide a Name</li> <li>Click on the Privileges tab</li> <li>Set Can login, Superuser, Create roles, Create databases and Inherit rights from the parent roles as desired</li> <li>Click Save</li> </ul> <p></p> <ul> <li>The new user will now be created</li> <li>This can be a tedious process, especially if there are a lot of users to add</li> </ul> <p>If you are unsure on what users are required: An error will occur when attempting to perform the restore, pointing out that the missing user is the cause. This user can then be added.</p> <p>Best practice will be to provide the correct privileges for each user, but if you are unsure what the privileges need to be, set all to active. Only do this if you are sure the database is secure (e.g. only you will have access). Otherwise, avoid doing this.</p>"},{"location":"gis/resources/postgres/postgres_qgis/#restoring-the-database-dump","title":"Restoring the database dump","text":"<p>Once the above requirements has been met, the user can get to restoring the DB. The terminal/console will be made use of to perform the restoration.</p> <ul> <li>Open the terminal/console</li> <li>Change the directory to the folder which contains the DB dump</li> </ul> <p></p> <ul> <li>Run the pg_restore command in the terminal: \u201cpg_restore -h localhost -p 5432 -U postgres -d my_new_database my_old_database.dump\u201d</li> <li>-h: Host on the network, localhost in this case. If the DB is on a different computer on the network, use the IP of that computer</li> <li>-p: Port number, usually 5432 for Postgres. Make sure what port number your postgres is using</li> <li>-U: Use the \u201cpostgres\u201d username. The user might differ for your server</li> <li>my_new_database: The name of the database in the server, \u201cdominode_production\u201d in this case</li> <li>my_old_database: The DB dump file to restore.</li> <li>Press Enter to run the command</li> <li>When asked for a password, provide the password in the service file</li> </ul> <p>If the user experiences an error stating a user is not available/missing, add the user as described in the previous section.</p> <ul> <li>Once the restoration shows succeeded in the terminal, open pgAdmin;</li> <li>Refresh the DB into which the restoration were performed;</li> <li>Open it, and under Schemas, the user should see something similar to the following:</li> </ul> <p></p> <p>Take note that the imported tables might be stored in the public schema and the DB does not contain additional schemas</p> <p>Possible issues a users can encounter when restoring a DB: Restoration can fail for numerous reasons, and can be difficult to resolve. Here are some tips on solving possible issues: - Authentication issues:   - Be sure you are using the correct password associated with the correct user;   - If you are using the \"postgres\" user, the password will likely be the same as the password being used when opening pgAdmin4;   - If you are using user \u2018x\u2019 (the owner of the DB), be sure that you are using the password saved in the service file (Postgres Service file section); or   - Be sure that the service file has been set up correctly, especially the environmental variable, as this can also cause authentication issues. - User role error:   - If a user cannot be found when performing a DB restore, just add the username referred to in the console as explained in the User list section.</p>"},{"location":"gis/resources/postgres/postgres_qgis/#importing-spatial-data-into-a-database","title":"Importing spatial data into a database","text":""},{"location":"gis/resources/postgres/postgres_qgis/#raster-importing","title":"Raster importing","text":"<p>This section will deal with importing of rasters into a Postgres DB. As with other spatial data, the PostGIS extension needs to be installed for the DB. Also, this step can only be done using console commands. There are a lot of resources on how to use this command, but here is a short overview: https://spatial-dev.guru/2022/01/28/import-rasters-file-to-postgis-database-using-raster2pgsql/. Also be sure that the PostGIS raster extension has been added to the DB.</p> <p>The user first needs to get some information on the raster before getting started. - Open the raster in QGIS - Right-click on the raster in the layer list, and select Properties</p> <p></p> <ul> <li>Click on the Source tab</li> <li>The user needs to make a note of the Coordinate reference system (CRS). The importing requires the EPSG number. In this example its \"32620\"</li> </ul> <p></p> <ul> <li>If the user desires tiling on the raster when doing the import, click on the Information tab</li> <li>Make note of the Width and Height of the raster</li> </ul> <p></p> <p>These width and height values are the number of pixels. If the raster has a large number of pixels, it is recommended to make use of tiling when importing the raster. For a small raster, tiling will not be required</p> <ul> <li>Close the Layer Properties dialog</li> </ul> <p>Importing the raster will be done using the raster2pgsql command</p> <ul> <li>Open the terminal/console</li> <li>Run the following command: \"raster2pgsql -s 32620 -F -I -C directory-to-raster Schema.Table name | psql -h localhost -p 5432 -U Username database-name\"</li> <li>-s: This is CRS;</li> <li>-F: Creates a filename column;</li> <li>-I: Raster spatial index, which will optimize raster processing; and</li> <li>-t: (optional) Raster tile size (e.g. 2500x2500).</li> <li>directory-to-raster: Directory to the raster</li> <li>Schema: public, or any other desired schema</li> <li>Table name: \u2018elevation\u2019, or any name the user desires</li> <li>Username: Likely \u2018postgres\u2019, but might differ depending on the user\u2019s postgres</li> <li>database-name: Name of the database</li> <li>If the user gets memory errors while performing the raster import, not using tiling is likely the culprit</li> </ul> <p></p> <ul> <li>Do the following if you encounter such an error, otherwise this can be skipped:</li> <li>Make use of tiling</li> <li>-t: '2500x2500'</li> <li>This should suffice for most cases</li> <li>The user can make use of a tiling size which they feel will better fit their scenario</li> <li>Each tile will have a row in the imported raster table</li> <li>The importing might run a will, depending on the raster spatial resolution and number of pixels</li> <li>During importing the user should see something similar to the following:</li> </ul> <p></p> <ul> <li>Each of those inserts is a new tile being added to the table</li> <li>Once the processing is done, the user should see something similar to the following:</li> </ul> <p></p> <p>How to check if the raster imported successfully: - Open pgAdmin - Go to the DB, schema, and right-click on the schema which should contain the raster. Click Refresh - The table, which contains the raster tiles, should be there:</p> <p></p>"},{"location":"gis/resources/postgres/postgres_qgis/#connect-to-postgres-database-in-qgis","title":"Connect to Postgres database in QGIS","text":"<p>This section provides details on how to make a connection to a Postgres DB from within QGIS.</p> <ul> <li>Open **QGISv</li> <li>Click on the Layer menu, and select Data Source Manager</li> </ul> <p></p> <ul> <li>Click on the PostgreSQL tab</li> <li>Click on the New button</li> </ul> <p></p> <ul> <li>Provide the following information:</li> <li>Name: This can be what the user desire (e.g. \u201cDominode\u201d)</li> <li>Service: Leave empty</li> <li>Host: localhost (or the IP of the computer hosting the server)</li> <li>Port: 5432 (or the port number the user\u2019s server makes use of)</li> <li>Database: dominode_production</li> <li>User name: postgres</li> <li>Password: The password used when the user opens pgAdmin4</li> <li>Enable Store for the username and password if the user desired to save the authentication</li> <li>Enable Also list tables with no geometry</li> <li>Enable Allow saving/loading QGIS projects in the database</li> <li>Click Test Connection</li> <li>The user should see Connection to database was successful if there were no problems connecting to the DB</li> </ul> <p></p> <ul> <li>Click OK</li> <li>On the Connection dialog, click Connect. The user should see the DB schemas as shown below</li> </ul> <p></p> <ul> <li>Click Close</li> </ul>"},{"location":"gis/resources/postgres/postgres_qgis/#load-layers-from-a-database","title":"Load layers from a database","text":"<p>Once the user has established a connection the DB in QGIS, they can start loading data. - In QGIS, go to the Browser dialog. It should be below the Layers dialog - Click on PostGIS and then on database name (\u2018Dominode\u2019 in this example) - The user will now see the list of DB Schemas</p> <p></p> <ul> <li>Open the Schema where the data is stored, \"public\" in this example</li> <li>Right-click on the layer, and click on Add Layer to Project</li> </ul> <p>The user can also drag-and-drop the layer to accomplish the same result.</p> <p></p> <ul> <li>The layer will now be added to the QGIS canvas</li> </ul> <p></p>"},{"location":"gis/resources/postgres/postgres_qgis/#qgis-project-stored-in-a-database","title":"QGIS project stored in a database","text":"<p>Postgres allows the storage of QGIS projects, which can be opened directly in QGIS from the DB.</p> <p>Be sure that you selected the Allow saving/loading QGIS projects in the database when making the connection to the DB.</p> <ul> <li>In QGIS, go to the Browser (likely below the Layers section)</li> <li>Click on the PostGIS drop-down, and open the database the user desires (Dominode in this case);</li> <li>Open the Schema which contains the QGIS project (lsd_staging in this case);</li> </ul> <p></p> <ul> <li>Double-click on the project to open it</li> </ul> <p>If QGIS crashes when attempting to open the project, it might be a versioning issue (e.g. an older version were used to store the project in the DB).</p>"},{"location":"gis/resources/postgres/service_file/","title":"Service file","text":"<p>The connection service file stores connection parameters to be associated with a single service name. That service name can then be specified by a libpq connection, and the associated settings will be used. This allows connection parameters to be modified without requiring a recompile of the libpq application. The service name can also be specified using the PGSERVICE environment variable. More information on the service file can be found here: https://www.postgresql.org/docs/9.1/libpq-pgservice.html</p> <p>A service file is essential to work for the DB the user needs to set up. The service file can contain several DBs, and the credentials, host, DB name, etc. of each database.</p> <p>These steps are only required if the service file does not exist on the server: - Create a new file in a folder where your prefer to store it - Name the file \"pg_service.conf\"</p> <p>Add the DB which will be restored/created to the service file as follows: - Open the service file in a text editor - Add the following based on the requirements for the database which will be created/restored</p> <p></p> <ul> <li>Save the file</li> </ul> <p>If the user is making use of Windows, the user needs to create the environmental variable for the service file location. These steps can be ignored on Linux.</p> <ul> <li>In the Windows search, type \"environment\"</li> <li>Select Edit the system environmental variables</li> </ul> <p></p> <ul> <li>The System Properties dialog will open</li> <li>Click on Environmental Variables at the bottom</li> </ul> <p></p> <ul> <li>The Environmental Variables dialog will open</li> </ul> <p></p> <ul> <li>Under system variables, click New</li> <li>Provide the following information:</li> <li>Variable name: PGSYSCONFDIR</li> <li>Variable value: The directory which contains the service file</li> </ul> <p></p> <ul> <li>Click OK</li> <li>The user can now close the Environmental Variables dialog</li> </ul>"},{"location":"gis/resources/qgis/","title":"QGIS Resources","text":"<p>QGIS is a free, open-source software that allows users to create, edit,visualize, analyze, and publish geospatial information.</p> Resource focus Description Atlas Atlas allows you to create multiple maps using records in a shapefile or spatial data set. QGIS issues and workarounds Sections on issues a user may encounter with QGIS, and workarounds for issues."},{"location":"gis/resources/qgis/atlas/","title":"QGIS Atlas","text":""},{"location":"gis/resources/qgis/atlas/#view-topographic-maps","title":"View topographic maps","text":"<p>This section deals with viewing/opening the Atlas layouts. - In the QGIS project, go to the Project menu, and Layouts - All Topographic map options are available there, as can be seen here:</p> <p></p> <ul> <li>Click on the layout you want to open</li> <li>The user will see a window similar to the following:</li> </ul> <p></p> <ul> <li>Click on the Preview Atlas button. This will allow the user to have a look at the available topographic maps for this layout</li> </ul> <p></p> <ul> <li>The user can now use the arrows to traverse through all the maps, or use the drop-down list to select a particular map</li> </ul> <p></p> <p>A tile will automatically update when a user selects it.</p>"},{"location":"gis/resources/qgis/atlas/#export-topographic-maps","title":"Export topographic maps","text":"<p>A QGIS Atlas user can export a single tile, or batch export tiles. The first section deals with single exports, this is followed by a section on batch exporting.</p>"},{"location":"gis/resources/qgis/atlas/#single-export","title":"Single export","text":"<ul> <li>Click on the Layout tab;</li> <li>Have a look at the Export Settings group</li> <li>Set the Export resolution (dpi)</li> </ul> <p>Setting this value too high might result in QGIS taking very long to export the maps! So if you have exporting problems, lower this value.</p> <ul> <li>Activate Print as raster, as we will be exporting as PDF</li> <li>(Optional) Activate Save world file if desired</li> <li>Click on the Preview Atlas button</li> </ul> <p></p> <ul> <li>Select the tile you want to export</li> </ul> <p></p> <ul> <li>Click on the Export as PDF button</li> </ul> <p></p> <ul> <li>The Export to PDF dialog will now be open</li> <li>Provide your desired output name, or use the default output name</li> <li>Click Save</li> </ul> <p></p> <ul> <li>The PDF Export Options dialog will open;</li> <li>Set options as desired, but default options should suffice</li> <li>Click Save</li> </ul> <p></p> <ul> <li>It might take a few minutes, depending on the DPI you are using</li> </ul>"},{"location":"gis/resources/qgis/atlas/#batch-export","title":"Batch export","text":"<ul> <li>Click on the Layout tab;</li> <li>Have a look at the Export Settings group</li> <li>Set the Export resolution (dpi)</li> </ul> <p>Setting this value too high might result in QGIS taking very long to export the maps! So if you have exporting problems, lower this value.</p> <ul> <li>Activate Print as raster, as we will be exporting as PDF</li> <li>(Optional) Activate Save world file if desired</li> <li>Click on the Export drop-down menu</li> <li>Select Export Atlas as PDF</li> </ul> <p></p> <ul> <li>Select your desired output folder</li> <li>The PDF Export Options dialog will appear</li> <li>Options can be left as default here, or change as desired</li> </ul> <p></p> <ul> <li>Click Save;</li> <li>The exporting will start running, this can take very long, even on a fast computer. Exporting speed depends on the following:</li> <li>Number of topographic maps, which will differ depending on scale</li> <li>The higher the DPI, the longer this will take</li> </ul> <p></p> <ul> <li>QGIS might seem like its stalling, just leave it running</li> </ul> <p>Here is an example of a PDF topographic map:</p> <p></p>"},{"location":"gis/resources/qgis/issues/","title":"Possible issues with QGIS","text":"<p>A user can encounter numerous issues while working with QGIS (or any other software). This section provides a guide to a QGIS user on how to resolve some issues, or a workaround for the issues. These approaches will not always solve your problem.</p>"},{"location":"gis/resources/qgis/issues/#qgis-crashes-when-opening-a-project","title":"QGIS crashes when opening a project","text":"<p>The likely cause of this that the project were saved in a different version of QGIS. Do the following to resolve such cases: - Install an LTR version of QGIS (e.g. 3.16), and use it to open the project - Attempt to find out what version the project is stored in - If the user knows the version, install that version</p> <p>It might be easier to do this in Windows, as it's much easier to install older version in Windows than Linux</p> <ul> <li>If you are opening a project in a Postgres DB, be sure the DB is stored in the correct postgres version</li> <li>Be sure that the project has access to the data it needs to load</li> <li>Check if the data is not corrupted (e.g. try loading the data in another project)</li> </ul>"},{"location":"gis/resources/qgis/issues/#missing-layers","title":"Missing layers","text":"<p>The user might encounter a missing layer when opening a QGIS project. This usually happens if the layer source name or directory has changed. A quick solution will be to just remove the layer and readd it, but in some instances a user would like to keep the symbologies and other properties set for the layer. A better solution is therefore required for such cases.</p> <ul> <li>Right-click on the faulty layer, and click on Repair Data Source</li> <li>The Repair Data Source dialog will open</li> </ul> <p></p> <ul> <li>Browse and select the layer which should be used</li> </ul> <p></p> <ul> <li>Click OK</li> <li>The layer will now be repaired.</li> </ul>"},{"location":"gis/resources/qgis/issues/#loading-rasters","title":"Loading rasters","text":"<p>Rasters can sometimes take forever to load, or cause QGIS to crash while a user waits for it to finish. This can happen for several reasons, most likely that the raster has an extremely high spatial resolution and covers a large area. A computer with poor performance will also have more problems. Here are two possible approaches at remeding the issue.</p>"},{"location":"gis/resources/qgis/issues/#generate-pyrymid-layers","title":"Generate pyrymid layers","text":"<p>Pyrymid layers will drastically improve raster load times in spatial software. If a raster takes too long to load, the best will be to generate pyrimid layers if it has none.</p> <p>Be aware that for large raster the pyrimid layers can take up a lot of space</p> <ul> <li>Open the QGIS Processing toolbox</li> </ul> <p></p> <ul> <li>Type \"pyramid\" in the Search</li> <li>Open the Build overviews (pyramids) tool;</li> <li>Select the raster layer for which pyramids should be generated;</li> <li>Advanced Parameters can be changed as disired, but default should suffice for most cases;</li> <li>Click Run.</li> </ul> <p></p> <p>Depending on the raster size, this may run a while. Once done, loading the raster will be much faster in QGIS.</p>"},{"location":"gis/resources/qgis/issues/#raster-resolution","title":"Raster resolution","text":"<p>If pyramid layers did not solve the user's loading problems, reducing the spatial resolution can also help.</p> <p>Do not use this approach if you want to keep the accuracy/quality of the data, or if you need to perform analysis on the data.</p> <ul> <li>Right-click on the raster, go to Export and click on Save As</li> </ul> <p></p> <ul> <li>Select the output Format</li> <li>Set the output File name</li> <li>The CRS is best to be set to that of the raster</li> <li>Change the extent as desired</li> <li>Change the horizontal and vertical values to the desired spatial resolution</li> <li>Click OK</li> </ul> <p></p> <p>Reducing the spatial resolution, especially if the higher resolution is not required, will allow a user to spend less time waiting for the QGIS canvas to refresh, or needing to restart QGIS because it crashed.</p> <p>Here is an example of the number of pixels of a raster at a high spatial resolution:</p> <p></p> <p>Here is the same raster at a much lower spatial resolution:</p> <p></p>"},{"location":"gis/technologies/","title":"Technologies","text":"<p>Outline of Key GIS Technologies that all GIS practitioners should be familiar with.</p> <p>Outline GIS technologies and concepts for developers to get up to speed quickly.</p>"},{"location":"library/","title":"Resources","text":"<ul> <li>Cheatsheets</li> <li>External Resources</li> <li>Kartoza Media Center</li> <li>Tutorials</li> </ul>"},{"location":"library/cheatsheets/","title":"Cheatsheets","text":"<p>This article is under heavy development and is not considered production ready</p> <p>Cheatsheets and snippets for important and common operations.</p> <ul> <li>PostgreSQL</li> <li>BASH</li> <li>Kubectl</li> <li>OSGeo4W</li> </ul>"},{"location":"library/cheatsheets/bash/","title":"Bash","text":"<p>This article is under heavy development and is not considered production ready</p> <p>A shell is a computer program which exposes an operating system's services to a human user or other programs. The Bourne Again SHell (BASH) is a command-line interpreter that is commonly found in many UNIX based Operating Systems, and the term \"bash-scripting\" has become synonymous with the execution of UNIX commands.</p>"},{"location":"library/cheatsheets/bash/#busybox","title":"BusyBox","text":"<p>BusyBox is a software suite that provides several Unix utilities in a single executable file. Where bash provides a way for commands to be executed, BusyBox provides access to the commands themselves. BusyBox is probably the minimal toolset you can expect to find on UNIX systems and is commonly found on everything from workstations to servers to IoT devices. Many fan-favorite commands, such as <code>cat</code>, <code>ls</code>, <code>top</code>, <code>grep</code>, <code>awk</code>, <code>mount</code>, and more are actually made available by the BusyBox installation.</p> <pre><code>/bin/busybox --list-full\n</code></pre> <p>Other system commands vary from installation to installation, so the more you can achieve with the BusyBox tools and no external dependencies, the more robust your scripts will be.</p>"},{"location":"library/cheatsheets/bash/#executing-scripts","title":"Executing Scripts","text":"<p>A script is a file that contains a sequence of commands that are executed by the shell. To run those commands it must be made executable.</p> <pre><code>chmod +x /path/to/my-script.sh\n</code></pre> <p>For the system to know what software an executable file must be run with, a shebang is used at the top of the file</p> <pre><code>#!/bin/bash\n</code></pre>"},{"location":"library/cheatsheets/kubectl/","title":"Kubectl","text":"<p>This article is under heavy development and is not considered production ready</p> <p>Kubectl (Kubernetes Control) is a commandline utility for managing Kubernetes clusters.</p>"},{"location":"library/cheatsheets/kubectl/#kustomize","title":"Kustomize","text":"<p>Kustomize is a utility for building and \"patching\" complete resource definitions from a subset of information, allowing .</p> <p>There are a lot of declarative items within a set of Kubernetes resources that may be consistent and repeating these definitions would become rather redundant. An example would be all the resources in the project requiring the inclusion of the namespace element in the resource metadata.</p> <p>Not only is this redundancy inefficient to manage and control, but it also violates DRY principles that ensure our components are managed consistently and to prevent errors.</p>"},{"location":"library/cheatsheets/kubectl/#common-operations","title":"Common operations","text":"<p>To validate the current resource configurations, run <code>kubectl kustomize</code>, or pipe the output to a file with <code>kubectl kustomize &gt; output.yaml</code> to create a single file with a complete set of the resource definitions available within the current directory.</p> <p>Apply the current directories manifests using kustomize with <code>kubectl apply -k .</code>, with <code>.</code> being the current directory and using a <code>kustomize.yaml</code> file to define the resources to apply.</p> <p>Use <code>kubectl -n my-namespace get pods</code> to list the pods in the <code>my-namespace</code> namespace.</p> <p>This is useful when you want to find a pod by name, so that you can execute the command directly against a pod that forms a particular service. You would have to be careful with your deployments naming conventions to avoid collisions, but it can be super handy to enhance your automation capabilities.</p> <pre><code>$ nginx_pod=$(kubectl -n my-namespace get pods | awk '{print $1}' | grep -m 1 -e \"nginx\") &amp;&amp; \\\necho $nginx_pod\n</code></pre> <p>If you're a sucker for punishment, you can accomplish a similar result with powershell.</p> <pre><code>&gt; $nginx_pod = (kubectl -n my-namespace get pods |  Select-String -Pattern \"nginx\" -SimpleMatch | select -first 1 | %{ ($_ -split \"\\s+\")[0]})\n&gt; Write-Output $nginx_pod\n</code></pre> <p>This makes it trivial to copy data into a volume with kubectl</p> <pre><code>nginx_pod=$(kubectl -n my-namespace get pods | awk '{print $1}' | grep -m 1 -e \"nginx\") &amp;&amp; \\\n  kubectl -n my-namespace cp ./configs/web/index.html $nginx_pod:/web/index.html\n</code></pre> <p>Or execute commands such as a mapproxy cleanup</p> <pre><code>mapproxy_pod=$(kubectl -n my-namespace get pods | awk '{print $1}' | grep -m 1 -e \"mapproxy\") &amp;&amp; \\\n  kubectl -n my-namespace exec $mapproxy_pod -- /bin/bash -c \"mapproxy-seed -s /mapproxy/seed.yaml -f /mapproxy/mapproxy.yaml -c 4 --cleanup=remove_complete_levels\"\n</code></pre> <p>Or run the command in the background within the pod, such as a mapproxy seeding operation</p> <pre><code>mapproxy_pod=$(kubectl -n my-namespace get pods | awk '{print $1}' | grep -m 1 -e \"mapproxy\") &amp;&amp; \\\n  kubectl -n my-namespace exec $mapproxy_pod -- /bin/bash -c \"mapproxy-seed -s /mapproxy/seed.yaml -f /mapproxy/mapproxy.yaml -c 4 &gt; /dev/null 2&gt; /dev/null &amp;\"\n</code></pre>"},{"location":"library/cheatsheets/kubectl/#helm","title":"Helm","text":"<p>Helm is a Kubernetes Native package manager that provides more complex management capabilities for k8s resources. Helm is a separate application from kubectl that must be installed on a system with access to the cluster via the <code>~/.kube/config</code> file.</p>"},{"location":"library/cheatsheets/osgeo4w/","title":"OSGeo4W","text":"<p>This article is under heavy development and is not considered production ready</p> <p>OSGeo4W is an initiative from osgeo.org for packaging OpenSource Geospatial Systems, which are typically designed on and for Linux based systems, in an easy to use process that allows these systems (such as GRASS, QGIS, and others) to function well on the Windows Platform and manage each system and its dependencies in an easy to use way.</p>"},{"location":"library/cheatsheets/osgeo4w/#legacy-systems","title":"Legacy systems","text":"<p>Note that OSGeo4W used to be called <code>OSGeo4W</code> for the 32-bit installation and <code>OSGeo4W64</code> for the 64-bit installation. In 2021 a new OSGeo4W installer was created and support for legacy 32-bit systems was dropped. If you are using an older installation, it is highly recommended</p>"},{"location":"library/cheatsheets/osgeo4w/#cli","title":"CLI","text":"<p>OSGeo4W provides an isolated environment that keeps to the \"everything is a file\" philosophy of Linux systems. This means that all of the dependencies of the system are packaged within the OSGeo4W directory (typically \"C:\\OSGeo4W\"). This makes it easy to install and manage the system, however it also means that the windows environment doesn't know how to access these utilities all by itself.</p> <p>This means that using some commandline tools, like GDAL for example, will not be available by default.</p> <pre><code>PS C:\\Users\\Username&gt; Get-Command gdalinfo\nGet-Command: The term 'gdalinfo' is not recognized as a name of a cmdlet, function, script file, or executable program.\nCheck the spelling of the name, or if a path was included, verify that the path is correct and try again.\nPS C:\\Users\\Username&gt; cmd /k \"where gdalinfo\"\nINFO: Could not find files for the given pattern(s).\n</code></pre> <p>To access the OSGeo4W environment, we simply need to \"call\" the OSGeo4W batch script which handles the environment for us.</p> <pre><code>PS C:\\Users\\Username&gt; cmd /k \"call C:/OSGeo4W/OSGeo4W.bat\"\nrun o-help for a list of available commands\nC:\\OSGeo4W&gt;where gdalinfo\nC:\\OSGeo4W\\bin\\gdalinfo.exe\nC:\\OSGeo4W&gt;o-help\n</code></pre> <p>When you start an OSGeo4W application, like QGIS Desktop for example, it will handle this environment configuration for you. If, however, you are performing other operations, such as using the qgis_processing framework or GRASS commandline tools, you will need to configure this environment yourself.</p>"},{"location":"library/cheatsheets/postgresql/","title":"PostgreSQL","text":"<p>This article is under heavy development and is not considered production ready</p> <p>PostgreSQL best SQL -- Everyone who knows what's up</p> <p>PostgreSQL also known as Postgres, is a free and open-source relational database management system (RDBMS). The name comes from its succession of its predecessor, Ingres. You are now a database nerd... there's no turning back.</p>"},{"location":"library/cheatsheets/postgresql/#sql","title":"SQL","text":"<p>Structured Query Language (SQL) is a query language used to interact with databases. It is a \"standard\" language structure, although its implementation differs between implementations. It does more than just query as well (see the Language Structures section below). Each discrete action in SQL is called a \"statement\", which does something like retrieving the results of a query or creating a new record, and multiple statements can be bundled together into a transaction.</p> <p>Fortunately the key words used in SQL tend to be written in (mostly) plain english, so it's relatively easy to understand and pick up. Multiple statements, joins, and subqueries can also be used which is where things start to get complicated, and optimizing those operations (and the database) can get rather complex.</p>"},{"location":"library/cheatsheets/postgresql/#simple-query-structures","title":"Simple Query Structures","text":""},{"location":"library/cheatsheets/postgresql/#common-queries","title":"Common queries","text":"<p>Basic query</p> <pre><code>SELECT col1, col2 FROM schema.table WHERE col2 = 'value' ORDER BY col1;\n</code></pre> <p>Aggregated query</p> <pre><code>SELECT max(col1), col2 FROM schema.table\nGROUP BY col2\nHAVING col2 = 'value';\nLIMIT n\nOFFSET offset;\n</code></pre> <p>Basic joins</p> <pre><code>SELECT col1, col2\nFROM table1\nFULL OUTER JOIN table2 ON col1\n</code></pre>"},{"location":"library/cheatsheets/postgresql/#clone-table","title":"Clone table","text":"<p>Duplicate table with data</p> <pre><code>CREATE TABLE new_table AS SELECT * FROM old_table;\n</code></pre> <p>Duplicate table structure</p> <pre><code>CREATE TABLE new_table AS\nTABLE existing_table\nWITH NO DATA;\n</code></pre> <p>Replicate table using subset</p> <pre><code>CREATE TABLE new_table AS\nSELECT\n*\nFROM\n    existing_table\nWHERE\n    condition;\n</code></pre>"},{"location":"library/cheatsheets/postgresql/#psql","title":"psql","text":"<p>Using the psql interactive terminal for postgresql. Note that the version of psql on the local system is likely required to be an equal or higher version than for any databases with which you intend on interacting.</p>"},{"location":"library/cheatsheets/postgresql/#installing-psql","title":"Installing psql","text":"<p>psql is installed alongside postgresql on many systems, however if it is not expected that the local machine will run a database server, psql can be installed using postgresql client libraries.</p> <p>Debian</p> <pre><code>sudo apt install postgresql-client\n</code></pre> <p>Mac</p> <pre><code>brew link --force libpq\n</code></pre> <p>$ psql --help</p> <pre><code>psql is the PostgreSQL interactive terminal.\n\nUsage:\n  psql [OPTION]... [DBNAME [USERNAME]]\n\nGeneral options:\n  -c, --command=COMMAND    run only single command (SQL or internal) and exit\n  -d, --dbname=DBNAME      database name to connect to (default: \"Username\")\n  -f, --file=FILENAME      execute commands from file, then exit\n  -l, --list               list available databases, then exit\n  -v, --set=, --variable=NAME=VALUE\n                            set psql variable NAME to VALUE\n                            (e.g., -v ON_ERROR_STOP=1)\n  -V, --version            output version information, then exit\n  -X, --no-psqlrc          do not read startup file (~/.psqlrc)\n  -1 (\"one\"), --single-transaction\n                            execute as a single transaction (if non-interactive)\n  -?, --help[=options]     show this help, then exit\n      --help=commands      list backslash commands, then exit\n      --help=variables     list special variables, then exit\n\nInput and output options:\n  -a, --echo-all           echo all input from script\n  -b, --echo-errors        echo failed commands\n  -e, --echo-queries       echo commands sent to server\n  -E, --echo-hidden        display queries that internal commands generate\n  -L, --log-file=FILENAME  send session log to file\n  -n, --no-readline        disable enhanced command line editing (readline)\n  -o, --output=FILENAME    send query results to file (or |pipe)\n  -q, --quiet              run quietly (no messages, only query output)\n  -s, --single-step        single-step mode (confirm each query)\n  -S, --single-line        single-line mode (end of line terminates SQL command)\n\nOutput format options:\n  -A, --no-align           unaligned table output mode\n      --csv                CSV (Comma-Separated Values) table output mode\n  -F, --field-separator=STRING\n                            field separator for unaligned output (default: \"|\")\n  -H, --html               HTML table output mode\n  -P, --pset=VAR[=ARG]     set printing option VAR to ARG (see \\pset command)\n  -R, --record-separator=STRING\n                            record separator for unaligned output (default: newline)\n  -t, --tuples-only        print rows only\n  -T, --table-attr=TEXT    set HTML table tag attributes (e.g., width, border)\n  -x, --expanded           turn on expanded table output\n  -z, --field-separator-zero\n                            set field separator for unaligned output to zero byte\n  -0, --record-separator-zero\n                            set record separator for unaligned output to zero byte\n\nConnection options:\n  -h, --host=HOSTNAME      database server host or socket directory (default: \"local socket\")\n  -p, --port=PORT          database server port (default: \"5432\")\n  -U, --username=USERNAME  database user name (default: \"Username\")\n  -w, --no-password        never prompt for password\n  -W, --password           force password prompt (should happen automatically)\n</code></pre>"},{"location":"library/cheatsheets/postgresql/#abstract-basic-psql-commands","title":"abstract \"Basic psql commands\"","text":"<p>Example psql connection:</p> <pre><code>psql -U username -d database_name -h localhost -p 5432;\n</code></pre> <p>Quit psql:</p> <pre><code>\\q;\n</code></pre> <p>List databases:</p> <pre><code>\\l;\n</code></pre> <p>Connect to database:</p> <pre><code>\\c database_name;\n</code></pre> <p>List schemas:</p> <pre><code>\\dn;\n</code></pre> <p>List tables:</p> <pre><code>\\dt;\n</code></pre> <p>List table info:</p> <pre><code>\\d+ table_name;\n</code></pre> <p>List functions:</p> <pre><code>\\df;\n</code></pre> <p>List views:</p> <pre><code>\\dv;\n</code></pre>"},{"location":"library/cheatsheets/postgresql/#cli","title":"cli","text":"<p>Additional command line tools for interacting with postgreql databases. Note that the version of cli tools on the local system is likely required to be an equal or higher version than for any databases with which you intend on interacting. CLI tools are typically installed along with postgresql. Please see the \"installing psql\" section above for instructions on how to install command line tools.</p>"},{"location":"library/cheatsheets/postgresql/#pg_dump","title":"pg_dump","text":"<p>$ pg_dump --help</p> <pre><code>pg_dump dumps a database as a text file or to other formats.\n\nUsage:\n  pg_dump [OPTION]... [DBNAME]\n\nGeneral options:\n  -f, --file=FILENAME          output file or directory name\n  -F, --format=c|d|t|p         output file format (custom, directory, tar,\n                                plain text (default))\n  -j, --jobs=NUM               use this many parallel jobs to dump\n  -v, --verbose                verbose mode\n  -V, --version                output version information, then exit\n  -Z, --compress=0-9           compression level for compressed formats\n  --lock-wait-timeout=TIMEOUT  fail after waiting TIMEOUT for a table lock\n  --no-sync                    do not wait for changes to be written safely to disk\n  -?, --help                   show this help, then exit\n\nOptions controlling the output content:\n  -a, --data-only              dump only the data, not the schema\n  -b, --blobs                  include large objects in dump\n  -B, --no-blobs               exclude large objects in dump\n  -c, --clean                  clean (drop) database objects before recreating\n  -C, --create                 include commands to create database in dump\n  -E, --encoding=ENCODING      dump the data in encoding ENCODING\n  -n, --schema=PATTERN         dump the specified schema(s) only\n  -N, --exclude-schema=PATTERN do NOT dump the specified schema(s)\n  -O, --no-owner               skip restoration of object ownership in\n                                plain-text format\n  -s, --schema-only            dump only the schema, no data\n  -S, --superuser=NAME         superuser user name to use in plain-text format\n  -t, --table=PATTERN          dump the specified table(s) only\n  -T, --exclude-table=PATTERN  do NOT dump the specified table(s)\n  -x, --no-privileges          do not dump privileges (grant/revoke)\n  --binary-upgrade             for use by upgrade utilities only\n  --column-inserts             dump data as INSERT commands with column names\n  --disable-dollar-quoting     disable dollar quoting, use SQL standard quoting\n  --disable-triggers           disable triggers during data-only restore\n  --enable-row-security        enable row security (dump only content user has\n                                access to)\n  --exclude-table-data=PATTERN do NOT dump data for the specified table(s)\n  --extra-float-digits=NUM     override default setting for extra_float_digits\n  --if-exists                  use IF EXISTS when dropping objects\n  --inserts                    dump data as INSERT commands, rather than COPY\n  --load-via-partition-root    load partitions via the root table\n  --no-comments                do not dump comments\n  --no-publications            do not dump publications\n  --no-security-labels         do not dump security label assignments\n  --no-subscriptions           do not dump subscriptions\n  --no-synchronized-snapshots  do not use synchronized snapshots in parallel jobs\n  --no-tablespaces             do not dump tablespace assignments\n  --no-unlogged-table-data     do not dump unlogged table data\n  --on-conflict-do-nothing     add ON CONFLICT DO NOTHING to INSERT commands\n  --quote-all-identifiers      quote all identifiers, even if not key words\n  --rows-per-insert=NROWS      number of rows per INSERT; implies --inserts\n  --section=SECTION            dump named section (pre-data, data, or post-data)\n  --serializable-deferrable    wait until the dump can run without anomalies\n  --snapshot=SNAPSHOT          use given snapshot for the dump\n  --strict-names               require table and/or schema include patterns to\n                                match at least one entity each\n  --use-set-session-authorization\n                                use SET SESSION AUTHORIZATION commands instead of\n                                ALTER OWNER commands to set ownership\n\nConnection options:\n  -d, --dbname=DBNAME      database to dump\n  -h, --host=HOSTNAME      database server host or socket directory\n  -p, --port=PORT          database server port number\n  -U, --username=NAME      connect as specified database user\n  -w, --no-password        never prompt for password\n  -W, --password           force password prompt (should happen automatically)\n  --role=ROLENAME          do SET ROLE before dump\n\nIf no database name is supplied, then the PGDATABASE environment\nvariable value is used.\n\nReport bugs to &lt;pgsql-bugs@lists.postgresql.org&gt;.\n</code></pre>"},{"location":"library/cheatsheets/postgresql/#backup-with-pg_dump","title":"Backup with pg_dump","text":"<p>pg_dump is a command line tool for backing up postgresql databases.</p> <p>Simple example in bash:</p> <pre><code>pg_dump -U admin_user -x -n public -h localhost -p 5111 -d gis &gt; data.sql\n</code></pre> <p>Complex example in Windows:</p> <pre><code>\"C:\\Program Files\\PostgreSQL\\14\\bin\\pg_dump.exe\" --file \"C:\\\\backup\\\\data.sql\" --host \"127.0.0.1\" --port \"5234\" --username \"admin_user\" --no-password --verbose --format=p --no-owner --no-privileges --no-tablespaces --no-unlogged-table-data --encoding \"UTF8\" --schema \"public\" \"database_name\"\n</code></pre>"},{"location":"library/cheatsheets/postgresql/#postgis","title":"PostGIS","text":"<p>PostGIS is a spatial extension for PostgreSQL that provides spatial functions and datatypes.</p> <p>Be sure to check out the reference documentation at https://postgis.net/.</p>"},{"location":"library/cheatsheets/postgresql/#srids","title":"SRIDs","text":"<p>Spatial Reference Identifiers and Coordinate Reference System (CRS) management with PostGIS.</p> <p>Create Custom CRS Definitions</p> <p>Example using custom Albers EE Conic (Southern Africa)</p> <pre><code>INSERT INTO spatial_ref_sys (srid,proj4text) VALUES (40030,\n                            '+proj=aea +lat_0=0 +lon_0=25 +lat_1=-24 +lat_2=-33 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs +type=crs');\n</code></pre> <p>Use UpdateGeometrySRID to update geometry SRIDs.</p> <pre><code>select UpdateGeometrySRID('table_name', 'geom', 4326);\n</code></pre>"},{"location":"library/cheatsheets/postgresql/#language-structures","title":"Language Structures","text":"<p>There are multiple \"categories\" in SQL which apply different functionalities to different entities within the database. These are Data Definition Language (DDL), Data Query Language (DQL), Data Manipulation Language (DML), Transaction Control Language (TCL), and Data Control Language (DCL).</p> <p>Very basically, the nomenclature (and therefore the command used) may change between entities, and this makes it clear to the user and the database which entity or object the function should apply to. For example, you might use the DELETE command to delete a record from the database, but you would use the DROP command to delete a table, because tables and records are seen as different entities within the database. You can also do a ROLLBACK in a transaction, but you can't do so with a record INSERT command directly.</p> <p>These definitions are, of course, not arbitrarily assigned, and databases are a mature and complex field of computer science, which is where these formal structures and definitions come into effect.</p> <p>Read more on the topic at geeksforgeeks.org.</p>"},{"location":"library/links/","title":"Links","text":"<p>Links to interesting external resources</p>"},{"location":"library/media/","title":"Kartoza Media Center","text":"<p>Welcome to the Kartoza Media Center.</p> <p>Here you can find links to media releases, social media feeds, and notices from Kartoza (Pty) Ltd</p> <ul> <li>Kartoza.com</li> <li>GitHub</li> <li>Twitter</li> <li>DockerHub</li> <li>Newsletters</li> </ul>"},{"location":"library/media/newsletters/","title":"Resources","text":"<ul> <li>2021 Newsletter</li> </ul>"},{"location":"library/tutorials/","title":"Tutorials","text":"<p>Tutorials.</p> <ul> <li>General: General tutorials</li> <li>Links: Links to third party and external tutorials and learning resources</li> </ul>"},{"location":"library/tutorials/general/","title":"General Tutorials","text":"<p>General Tutorials.</p>"},{"location":"library/tutorials/general/#documentation","title":"Documentation","text":"<ul> <li>MkDocs PDF: Generating PDF Documentation from this site</li> </ul>"},{"location":"library/tutorials/general/#tools","title":"Tools","text":"<ul> <li>Git Primer: A light-but-functional introduction to the Git Version Control System</li> </ul>"},{"location":"library/tutorials/general/#procedures","title":"Procedures","text":"<ul> <li>Deployments with Docker-Ansible: How to set up, configure, and deploy to a new host with docker-ansible.</li> </ul>"},{"location":"library/tutorials/general/docker-ansible-deployments/","title":"Docker Ansible","text":"<p>Docker-Ansible is a project for running an ansible controller in a docker container. This allows the controller to be used on windows and avoids issues with environment specific configuration or possible collisions, making it possible to use ansible playbooks for consistent and reliable deployments on VPS and BareMetal instances.</p> <p>The Kartoza Playbooks is used for company maintained playbooks.</p>"},{"location":"library/tutorials/general/docker-ansible-deployments/#prerequisites","title":"Prerequisites","text":"<p>Ensure beforehand that you have ssh access to the relevant host, and replace the relevant server details in the host config etc. as outlined in the following commans. By default, Docker-Ansible will attempt to use the ssh configuration files from the host to perform operations.</p>"},{"location":"library/tutorials/general/docker-ansible-deployments/#procedure","title":"Procedure","text":"<p>The following example shows bash commands on how to create a new docker-based host. Ensure beforehand that you have ssh access to the host and replace the relevant server details in the host config etc.</p> <pre><code>git clone https://github.com/kartoza/docker-ansible.git\ncd docker-ansible/ansible\nrm -r playbooks\ngit clone https://github.com/kartoza/playbooks.git\necho 123.456.78.90 ansible_user=root ansible_private_key_file=/root/.ssh/id_ed25519 &gt; inventory/hosts.ini\ncd ..\ndocker build . -t ansible\ndocker run -dt -v $PWD/ansible:/ansible \\\n           -v ~/.ssh:/root/.ssh:ro \\\n           -e ANSIBLE_CONFIG=/ansible/ansible.cfg \\\n           --restart=unless-stopped \\\n           --name ansible ansible\ndocker exec -it ansible ansible-playbook /ansible/playbooks/ubuntu20.04/app/docker-ce.yaml -u iamgroot -k\n</code></pre>"},{"location":"library/tutorials/general/docker-ansible-deployments/#cleanup","title":"Cleanup","text":"<p>Once the task has run to completion you can always output the logs from the docker container to a file, e.g. <code>docker logs ansible &gt; ansible-output.log</code></p> <p>Remove the container <code>docker stop ansible &amp;&amp; docker rm ansible</code></p> <p>If desired, remove the docker-ansible directory. To purge the docker image use <code>docker image rm ansible</code>.</p>"},{"location":"library/tutorials/general/docker-ansible-deployments/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter permissions or ssh access errors, try setting <code>-v ~/.ssh:/root/.ssh:ro</code> to <code>-v ~/.ssh:/root/win-ssh:ro</code></p> <p>If ansible struggles to gather facts or stumbles into connection issues, using the ping command will allow you to verify whether the issue is connectivity related (e.g. ssh keys etc) or system specific.</p> <p>Simply ping your host to check the response <code>docker exec -it ansible ansible -m ping 123.456.78.90</code>.</p> <p>Otherwise you can always attach to the ansible controller shell with <code>docker exec -it ansible /bin/bash</code> and then troubleshoot from there, e.g. try ssh <code>ssh 123.456.78.90</code>, or check file permissions with <code>ls -alh /root/.ssh/</code></p>"},{"location":"library/tutorials/general/git-primer/","title":"Git Primer","text":"<p>This tutorial will show you how to use Git to safely implement new features or changes into a project, without any risk of adversely affecting the primary code base, or main branch. We will cover some concepts about git and version control systems to allow suers to get up to speed quickly and start contributing straight away.</p>"},{"location":"library/tutorials/general/git-primer/#basic-branching-in-git","title":"Basic branching in Git","text":"<p>A key function of Git is to function as a \"distributed version control system\", which means that it can be used to track changes in a project across multiple developers. Centralised version control systems typically require an individual to \"check out\" and \"lock\" a resource while it is under development, and then \"check in\" the changes when the resource is ready to be released. By contrast, distributed version control allows multiple  to \"fork\" an existing set of resources, and then track backwards through the commit history to see what changes have been made to the resource. A key element of git is \"forking\" and manipulation of \"branches\".</p> <p>Note that this tutorial assumes that users already have a git project setup and ready to use. If you need assistance getting setup, review other resources for Getting started with Git and GitHub, setting up projects on GitLab, migrating existing projects, or simply run <code>git init</code> in a project directory to get going quickly.</p>"},{"location":"library/tutorials/general/git-primer/#monitoring-change-in-git","title":"Monitoring Change in Git","text":"<p>Git is a version control system, but its mechanism of operation relies on the process of tracking changes. You could even think of Git primarily as a change monitoring tool, which has a whole host of additional features which make use of the metadata collected during the monitoring operation to provide sophisticated distributed version control.</p> <p>With every new commit, or collection of changes, git collects the differences between the previous commit and the current one, along with some additional metadata. Each commit has a signature, or hash, uniquely identifying it. Later on, this signature can be used to reference the commit and even check for differences between specified commits.</p>"},{"location":"library/tutorials/general/git-primer/#committing-code","title":"Committing Code","text":"<p>As highlighted above, Git is a change monitoring tool, but it also needs to be told what files to track the changes on.</p> <p>By default, git won't track changes to any files. Using <code>git add filename</code> will add the file to the \"staging\" area, and <code>git commit -m \"message\"</code> will commit the collection of staged changes to the currently active branch in the repository.</p> <p>In cases where more than one file needs to be added, using a space separated list of files, such as <code>git add file1.ext file2.ext</code> will add all listed files to the staging area.</p> <p>To add all files (not matched by .gitignore) to the staging area at once, simply use <code>git add -A</code>.</p>"},{"location":"library/tutorials/general/git-primer/#git-branches","title":"Git Branches","text":"<p>Commits track these changes in sequence to each other. This is what enables git as such a powerful method of distributed version control, and can also be effectively leveraged to provide a method of branching, whereby certain changes are developed in isolation to the rest of the project.</p> <p>Git uses branches by default, although new projects are simply created with a single default, or main, branch.</p> <p></p> <p>A chain of commits in a single branch Repository</p>"},{"location":"library/tutorials/general/git-primer/#creating-branches","title":"Creating Branches","text":"<p>Creating a new branch can be done with the command <code>git checkout -b branch-name</code>. The <code>-b</code>, or branch, flag indicates that git should create a new branch with the specified name. The base commit of the branch will simply reference the currently active HEAD commit. Future commits to the main branch will no longer affect the new branch.</p> <p></p> <p>A new git branch is simply creating a \"forked\" chain, using a specific commit record as the base</p> <p></p> <p>Performing a git commit with a new branch checked out creates a distinct history from parallel branches</p> <p>This should clearly illustrate how branches keep changes isolated between them, allowing changes to be made to the code without affecting the other branches.</p>"},{"location":"library/tutorials/general/git-primer/#viewing-branches","title":"Viewing Branches","text":"<p>Checking the available branches in a repository is as simple as <code>git branch</code>, which will print out a list of available branches and indicate the active branch with an asterisk.</p> <p></p> <p>GitHub provides a drop-down menu with the available branches</p> <p></p> <p>Branches and resources within branches can also be accessed via URL</p>"},{"location":"library/tutorials/general/git-primer/#changing-branches","title":"Changing Branches","text":"<p>Commits and other git commands are typically going to be executed on the active branch.</p> <p>To switch between branches, simply use <code>git checkout branch-name</code>.</p> <p>In some cases, you may have uncommitted changes which may get lost when switching between branches. To prevent data loss in these instances, git includes the stash feature.</p>"},{"location":"library/tutorials/general/git-primer/#and-then-some","title":"And then some","text":"<p>Git is incredibly powerful (and as a result, rather complex). A common use case, however, might be the merging of changes from a branch back into the main branch of the project. The <code>git merge</code> command is designed for exactly this, and it's typical usage follows a simple workflow.</p> <p>Simply switch the active branch to the branch it is desired that the changes are merged into, e.g. <code>git checkout main</code></p> <p>Then use <code>git merge branch-name</code> to merge the changes from branch-name into the main branch.</p> <p>Git supports multiple branches, and creating branches from other branches, so it's perfectly feasible that some long lived branches might be kept around for certain purposes, whilst others are merged into other branches, and then discarded.</p> <p></p> <p>A Git repository might include feature branches kept in isolation, or it may merge changes from a branch back into the main branch</p> <p>The \"HEAD\" reference in git is simply pointing to a specific commit, which will often be designated to the currently active branch.</p> <p>There's all sorts of advanced functionality available for managing the repository, such as remote repository management, cherry picking, and history modifications... but very often users simply need a way to manage simple changes in an isolated manner, and branches are a great way to do that.</p> <p>For the more advanced functionalities, additional tools can help simplify the management and deployment of Git repositories.</p>"},{"location":"library/tutorials/general/git-primer/#conflict-resolution-rebase-history-squashing-and-stashing","title":"Conflict resolution, Rebase, History, Squashing, and Stashing","text":"<p>These are terms for all that \"advanced functionality\" that was just mentioned.</p> <p>When merging changes from one branch into another, or switching between branches with committed and uncommitted changes, there are a number of scenarios which can occur. This introduces a plethora of git functionalities (and jargon) which give git it's superpowered reputation and a fair number of jokes about it's complexity.</p> <p>For the most part, however, the concepts are actually simple... it's the execution that can be complicated. Over the years, many tools and platforms have produced more efficient ways to handle these scenarios in efficient and reliable ways, so they're less of a pain point, but we'll cover them here at a high level so that when they are encountered, you'll know what to do with them \ud83d\ude09.</p> <p>If multiple individuals make changes to the same file, in the same place, a conflict can occur - that means that git cannot automatically resolve what the correct code is that should be injected into the resource, and manual conflict resolution must be done where a developer tells git which lines to keep, which to remove, and which to add to a new commit to patch things up to standard again.</p> <p>Sometimes a new set of features should be \"rebased\" - that is switching the \"base commit\" that it was forked from (often to the most recent main commit), and the developer can resolve any minor conflicts before asking for a code review from upstream developers etc. It's also capable of doing this in an interactive way, allowing developers to pick which commmits to keep when you have a git history full of noise, mistakes or other issues. There are also a great many ways to rewrite, or even migrate, git history... Remember that git tracks changes, which can, in fact, lead to bloat.</p> <p>Another way to clean up the history is to \"squash\" commits - so noisy or erroneous changes can be excluded from the history. This often makes things easier for reviewers to understand and cleans up bloat.</p> <p>Finally, git has a feature called \"stashing\" which allows developers to temporarily store changes in a \"stash\" before committing them to git. This is useful when a developer is working on changes that are not ready for committing, and needs to be able to revert to a previous state, change branches, or pull new commits from the upstream repository into their current branch. Essentially the \"stash\" is a kind of temporary or background commit that does not form a part of the git history, and allows users to avoid conflicts and prevent data loss.</p>"},{"location":"library/tutorials/general/git-primer/#releasing-changes","title":"Releasing Changes","text":"<p>Once you have your changes committed to your branch, you can create a \"featured checkpoint\" in your project using a git tag with the <code>-a</code> flag to add an annotation.</p> <pre><code>git tag -a v0.1-my-changes -m \"my changes for v0.1\"\n</code></pre> <p>On existing projects, you may want to review the available tags for a project using <code>git tag -l</code> before you decide on a tagging convention.</p> <p>Of course tags will only make the source code available to other developers, but a more sophisticated method of distributing your changes would be to create a release which might include compiled assets.</p> <p>If you're eager to get started with Git, be sure to check out what others are building, or contribute to ongoing open source initiatives.</p>"},{"location":"library/tutorials/general/git-primer/#prs-mrs-issues-releases-packages-cicd-and-gitoptions-galore","title":"PRs, MRs, Issues, Releases, Packages, CI/CD, and GitOp(tion)s Galore","text":"<p>Git itself is a version control utility... and a really great one at that. What it doesn't do are things like project management, issue tracking, bug reports, file distribution and more. There are, however, a lot of platforms that fill this gap and offer a great suite of tools for managing and deploying Git repositories alongside these other features, like GitHub, GitLab, BitBucket, or even self hosted systems like Gitea.</p> <p>The key elements are the \"Pull Requests\" and \"Merge Requests\". These are essentially the same thing conceptually - You are flagging to the \"upstream\" repository managers that you originally forked from that you have changes which you believe would be beneficial to integrate into another branch of the repository. Platforms like GitHub say \"Pull Request\", because the first action you would perform would be to get your changes pulled into the upstream repository. Platforms like GitLab say \"Merge Request\", because the last action you would perform would be to merge these changes into the upstream branch (typically the default branch of main or develop, but various workflows exist for feature branches and more).</p>"},{"location":"library/tutorials/general/git-primer/#tldr","title":"TL;DR","text":"<p>Git commandline seem a bit confusing for you? Good news is once you understand what it's doing under the hood, how you get it done should be less of an issue. I definitely recommend the super-duper-next-level-ultra-awesome GitLens for VSCode.</p>"},{"location":"library/tutorials/general/git-primer/#conclusion","title":"Conclusion","text":"<p>What? You made it to the end? I don't believe you I think you cheated and skipped the rest of the content. But if you really did make it all the way here from start to finish I am very proud. You deserve a present. Here, have a carrot... \ud83e\udd55</p>"},{"location":"library/tutorials/general/google-meet/","title":"Content","text":""},{"location":"library/tutorials/general/jitsi/","title":"Content","text":""},{"location":"library/tutorials/general/mkdocs-pdf/","title":"MkDocs PDF","text":"<p>Serving a local clone of this documentation with docker is as simple as <code>docker run --rm -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material</code>.</p> <p>This process walks through attempting to setup an environment with Docker that will produce PDF documentation for a MkDocs site with A MkDocs-Material Theme (Like this one).</p>"},{"location":"library/tutorials/general/mkdocs-pdf/#docker-environment","title":"Docker environment","text":"<p>Create a dockerfile with the relevant dependencies in line with the mkdocs-material docs and the mkdocs-pdf-export-plugin.</p> <pre><code>FROM squidfunk/mkdocs-material:8.2.15\n\n# RUN pip install mkdocs-pdf-export-plugin\n\n# Thanks to:\n# https://stackoverflow.com/questions/71372066/docker-fails-to-install-cffi-with-python3-9-alpine-in-dockerfile\n# https://github.com/Kozea/WeasyPrint/issues/699\n\nRUN apk add --update --no-cache --virtual .tmp-build-deps \\\n    gcc libc-dev linux-headers \\\n    &amp;&amp; apk add musl-dev jpeg-dev zlib-dev libffi-dev cairo-dev pango-dev gdk-pixbuf\n\nRUN python -m pip install --upgrade pip &amp;&amp; \\\n    python -m pip install mkdocs-pdf-export-plugin\n\n# Set working directory\nWORKDIR /docs\n\n# Expose MkDocs development server port\nEXPOSE 8000\n\n# Start development server by default\nENTRYPOINT [\"mkdocs\"]\nCMD [\"serve\", \"--dev-addr=0.0.0.0:8000\"]\n</code></pre> <p>Now create the custom mkdocs-material docker image with the command <code>docker build -t my-mkdocs .</code>.</p>"},{"location":"library/tutorials/general/mkdocs-pdf/#export-single-pdf","title":"Export Single PDF","text":"<p>To build the updated documentation and produce the output pdf, edit the <code>mkdocs.yml</code> file to include the plugin with the \"combined\" option set to true:</p> <pre><code>plugins:\n  - search\n  - pdf-export:\n      verbose: true\n      media_type: print\n      combined: true\n      combined_output_path: pdf/TheKartozaHandbook.pdf\n</code></pre> <p>Then use the following command to build the docs</p> <pre><code>docker run --rm -it -v ${PWD}:/docs my-mkdocs build\n</code></pre>"},{"location":"library/tutorials/general/mkdocs-pdf/#pdf-export-plugin","title":"PDF Export Plugin","text":"<p>The PDF export plugin can also be used to add a \"download pdf\" for each page. To try this out, edit the <code>mkdocs.yml</code> file to include the plugin:</p> <pre><code>plugins:\n  - search\n  - pdf-export\n</code></pre> <p>Now you can run the documentation with the command:</p> <pre><code>docker run --rm -p 8000:8000 -v ${PWD}:/docs my-mkdocs\n</code></pre> <p>Note that setting the \"combined\" option to true when service the documentation with the plugin will automatically point all download links to the collated file and individual page exports will not be available.</p> <p>This docker file takes rather a long time to start up...</p> <p>The docker logs will hang while the documentation is built. This is much slower with the PDF export plugin and using the default mkdocs-material image and configuration will be much faster for simple site builds. The expected docker logs output might be as follows:</p> <pre><code>warnings.warn(\nINFO     -  Cleaning site directory\nINFO     -  The following pages exist in the docs directory, but are not included in the \"nav\" configuration:\n  - development/conventions/dev_processes.md\n  - development/conventions/git.md\n  - development/conventions/ides.md\n  - development/conventions/project_processes.md\n  - development/environments/links.md\n  - development/environments/vscode/extension_install.md\n  - development/environments/vscode/links.md\n  - development/technologies/frameworks.md\n  - development/technologies/languages.md\n  - devops/infrastructure/personal_infrastructure.md\n  - devops/infrastructure/rancher-k3s-single-node.md\n  - devops/security/links.md\n  - library/cheatsheets/bash.md\n  - library/cheatsheets/postgresql.md\n  - library/media/newsletters.md\n  - library/tutorials/general/index.md\n  - library/tutorials/general/google-meet.md\n  - library/tutorials/general/jitsi.md\n  - library/tutorials/general/zoom.md\n  - library/tutorials/links/index.md\n  - library/tutorials/qgis/index.md\n</code></pre> <p>After a long wait period, the docker log should let you know the system is running and you can access the site on your local machine from the URL http://127.0.0.1:8000</p> <pre><code>INFO     -  Documentation built in 224.82 seconds\nINFO     -  [11:40:10] Watching paths for changes: 'docs', 'mkdocs.yml'\nINFO     -  [11:40:10] Serving on http://0.0.0.0:8000/\n</code></pre> <p>Once it's built it should include a pdf export button at the top of each page.</p>"},{"location":"library/tutorials/general/mkdocs-pdf/#todo","title":"TODO","text":"<ul> <li>Add theming/ customization</li> <li>Incorporate into GitHub Pages site and actions</li> <li>Add static collated pdf version (and download link) to published docs</li> </ul>"},{"location":"library/tutorials/general/zoom/","title":"Content","text":""},{"location":"library/tutorials/links/","title":"Tutorial Links","text":"<p>Links to third party and external tutorials and learning resources</p>"}]}